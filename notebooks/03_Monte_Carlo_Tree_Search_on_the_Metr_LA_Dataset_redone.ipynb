{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6kbAULJKy-wu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Set the main path in the root folder of the project.\n",
        "sys.path.append(os.path.join('..'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HGIZtEY6y-wv"
      },
      "outputs": [],
      "source": [
        "# Settings for autoreloading.\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "obBqbIuGy-wv"
      },
      "outputs": [],
      "source": [
        "from src.utils.seed import set_random_seed\n",
        "\n",
        "# Set the random seed for deterministic operations.\n",
        "SEED = 42\n",
        "set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHGuhgDmy-wv",
        "outputId": "7b633002-11e3-4114-dab9-e1f4216fcaf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The selected device is: \"cuda\"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set the device for training and querying the model.\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'The selected device is: \"{DEVICE}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhVgau7-y-ww"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HlTWno-1y-wx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BbiHBGWSy-wx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
        "    scaler = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nb6d7kVZy-wx"
      },
      "outputs": [],
      "source": [
        "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
        "from src.data.data_extraction import get_adjacency_matrix\n",
        "\n",
        "# Get the adjacency matrix\n",
        "adj_matrix_structure = get_adjacency_matrix(\n",
        "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
        "\n",
        "# Get the header of the adjacency matrix, the node indices and the\n",
        "# matrix itself.\n",
        "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
        "\n",
        "# Get the STGNN and load the checkpoints.\n",
        "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
        "\n",
        "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
        "                                      'st_gnn_metr_la.pth')\n",
        "\n",
        "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
        "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
        "\n",
        "# Set the STGNN in evaluation mode.\n",
        "spatial_temporal_gnn.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YIB5Kn57y-wx"
      },
      "outputs": [],
      "source": [
        "from src.data.data_extraction import get_locations_dataframe\n",
        "\n",
        "# Get the dataframe containing the latitude and longitude of each sensor.\n",
        "locations_df = get_locations_dataframe(\n",
        "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
        "    has_header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kwVU6H96y-wy"
      },
      "outputs": [],
      "source": [
        "# Get the node positions dictionary.\n",
        "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x-lj4muty-wy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Get the data scaler.\n",
        "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
        "    scaler = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kNItiWwGy-wy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Get the data and the values predicted by the STGNN.\n",
        "x_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train.npy'))\n",
        "y_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train.npy'))\n",
        "x_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train_time.npy'))\n",
        "y_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train_time.npy'))\n",
        "\n",
        "x_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val.npy'))\n",
        "y_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val.npy'))\n",
        "x_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val_time.npy'))\n",
        "y_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val_time.npy'))\n",
        "\n",
        "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test.npy'))\n",
        "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test.npy'))\n",
        "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test_time.npy'))\n",
        "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test_time.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCEWyqCvy-wz"
      },
      "outputs": [],
      "source": [
        "from src.data.data_processing import get_distance_matrix\n",
        "\n",
        "if not os.path.exists(\n",
        "    os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy')):\n",
        "    # Build the distance matrix between the nodes.\n",
        "    distance_matrix = get_distance_matrix(\n",
        "        adj_matrix,\n",
        "        locations_df,\n",
        "        node_pos_dict)\n",
        "\n",
        "    # Save the distance matrix.\n",
        "    np.save(os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'),\n",
        "            distance_matrix)\n",
        "\n",
        "else:\n",
        "    # Load the distance matrix.\n",
        "    distance_matrix = np.load(\n",
        "        os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU-tl1k9VJ7o"
      },
      "outputs": [],
      "source": [
        "i = 5\n",
        "x_, y_ = x_test[i], y_test[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NSba3NKU4ba",
        "outputId": "93f8263c-f96d-4696-9c31-7a4dc5fff446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution 1/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 2/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 3/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 4/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 5/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 6/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 7/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 8/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 9/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 10/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 11/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 12/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 13/50\n",
            "reward: 0.13933674139898833 , mae: 7.176857948303223\n",
            "Execution 14/50\n",
            "reward: 0.21191693215835242 , mae: 4.718830108642578\n",
            "Execution 15/50\n",
            "reward: 0.21191693215835242 , mae: 4.718830108642578\n",
            "Execution 16/50\n",
            "reward: 0.21191693215835242 , mae: 4.718830108642578\n",
            "Execution 17/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 18/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 19/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 20/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 21/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 22/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 23/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 24/50\n",
            "reward: 0.2444733444280671 , mae: 4.090425491333008\n",
            "Execution 25/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 26/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 27/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 28/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 29/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 30/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 31/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 32/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 33/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 34/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 35/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 36/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 37/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 38/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 39/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 40/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 41/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 42/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 43/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 44/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 45/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 46/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 47/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 48/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 49/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n",
            "Execution 50/50\n",
            "reward: 0.38009256767178184 , mae: 2.6309380531311035\n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_instance_explanations\n",
        "\n",
        "\n",
        "get_instance_explanations(\n",
        "    x= x_,\n",
        "    y = y_,\n",
        "    distance_matrix = distance_matrix,\n",
        "    spatial_temporal_gnn = spatial_temporal_gnn,\n",
        "    scaler = scaler,\n",
        "    n_rollouts= 50,\n",
        "    explanation_size_factor = 2,\n",
        "    cut_size_factor = 2,\n",
        "    exploration_weight = 10,\n",
        "    remove_value = 0.,\n",
        "    verbose= True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKZtVQc2y-wz",
        "outputId": "55f982f6-ef10-46e5-b647-624f6f42ed31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 559s - MAE: { severe_congestion 3.01 -congestion 1.35 -free_flow 0.814 - total: 1.69 } - RMSE: { severe_congestion 3.62 -congestion 1.69 -free_flow 1.02 - total: 2.07 } - MAPE: { severe_congestion 13.4% -congestion 2.77% -free_flow 1.25% - total: 5.77% } - Average time: 5.59s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 940s - MAE: { severe_congestion 3.12 -congestion 1.38 -free_flow 0.753 - total: 1.72 } - RMSE: { severe_congestion 3.78 -congestion 1.72 -free_flow 0.959 - total: 2.11 } - MAPE: { severe_congestion 14% -congestion 2.9% -free_flow 1.15% - total: 5.99% } - Average time: 9.4s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 576s - MAE: { severe_congestion 3.49 -congestion 1.46 -free_flow 0.828 - total: 1.88 } - RMSE: { severe_congestion 4.2 -congestion 1.83 -free_flow 1.02 - total: 2.3 } - MAPE: { severe_congestion 15.7% -congestion 3.05% -free_flow 1.27% - total: 6.59% } - Average time: 5.76s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 969s - MAE: { severe_congestion 3.04 -congestion 1.3 -free_flow 0.737 - total: 1.66 } - RMSE: { severe_congestion 3.65 -congestion 1.61 -free_flow 0.912 - total: 2.02 } - MAPE: { severe_congestion 14.3% -congestion 2.7% -free_flow 1.13% - total: 6% } - Average time: 9.69s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 585s - MAE: { severe_congestion 3.45 -congestion 1.46 -free_flow 0.776 - total: 1.87 } - RMSE: { severe_congestion 4.1 -congestion 1.81 -free_flow 0.985 - total: 2.26 } - MAPE: { severe_congestion 15.1% -congestion 3.04% -free_flow 1.19% - total: 6.42% } - Average time: 5.85s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 954s - MAE: { severe_congestion 2.89 -congestion 1.21 -free_flow 0.725 - total: 1.58 } - RMSE: { severe_congestion 3.52 -congestion 1.5 -free_flow 0.92 - total: 1.95 } - MAPE: { severe_congestion 13.6% -congestion 2.49% -free_flow 1.11% - total: 5.7% } - Average time: 9.54s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 713s - MAE: { severe_congestion 3.06 -congestion 1.41 -free_flow 0.803 - total: 1.72 } - RMSE: { severe_congestion 3.65 -congestion 1.75 -free_flow 1 - total: 2.09 } - MAPE: { severe_congestion 13.5% -congestion 2.9% -free_flow 1.24% - total: 5.82% } - Average time: 7.13s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1289s - MAE: { severe_congestion 3.1 -congestion 1.2 -free_flow 0.704 - total: 1.64 } - RMSE: { severe_congestion 3.72 -congestion 1.52 -free_flow 0.893 - total: 2.01 } - MAPE: { severe_congestion 14% -congestion 2.51% -free_flow 1.08% - total: 5.83% } - Average time: 12.9s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 811s - MAE: { severe_congestion 3.01 -congestion 1.41 -free_flow 0.79 - total: 1.71 } - RMSE: { severe_congestion 3.59 -congestion 1.74 -free_flow 0.987 - total: 2.07 } - MAPE: { severe_congestion 13.2% -congestion 2.96% -free_flow 1.21% - total: 5.74% } - Average time: 8.11s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1345s - MAE: { severe_congestion 2.93 -congestion 1.31 -free_flow 0.709 - total: 1.62 } - RMSE: { severe_congestion 3.6 -congestion 1.62 -free_flow 0.892 - total: 2 } - MAPE: { severe_congestion 13.1% -congestion 2.72% -free_flow 1.09% - total: 5.6% } - Average time: 13.5s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 806s - MAE: { severe_congestion 3.34 -congestion 1.69 -free_flow 0.809 - total: 1.91 } - RMSE: { severe_congestion 4.07 -congestion 2.09 -free_flow 1.04 - total: 2.36 } - MAPE: { severe_congestion 15.3% -congestion 3.59% -free_flow 1.24% - total: 6.67% } - Average time: 8.06s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1340s - MAE: { severe_congestion 2.91 -congestion 1.32 -free_flow 0.722 - total: 1.62 } - RMSE: { severe_congestion 3.53 -congestion 1.62 -free_flow 0.899 - total: 1.98 } - MAPE: { severe_congestion 12.9% -congestion 2.77% -free_flow 1.11% - total: 5.54% } - Average time: 13.4s \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import apply_grid_search\n",
        "\n",
        "apply_grid_search(\n",
        "    x_train[::10],\n",
        "    y_train[::10],\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts_list=[30, 50],\n",
        "    explanation_size_factor_list=[2, 3],\n",
        "    cut_size_factor_list=[2],\n",
        "    exploration_weight_list=[5, 10, 20],\n",
        "    remove_value_list=[0.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVtOLnQ68GDf"
      },
      "outputs": [],
      "source": [
        "CUT_SIZE_FACTOR = 2\n",
        "EXPLANATION_SIZE_FACTOR = 2\n",
        "EXPLORATION_WEIGHT = 20\n",
        "N_ROLLOUTS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FYN5mEu8GAl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "EXPLAINED_DATA_DIR = os.path.join(BASE_DATA_DIR, 'explained')\n",
        "os.makedirs(EXPLAINED_DATA_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdB__0mW8F-E",
        "outputId": "26b2c9fd-457f-4360-9be7-2cc98c677b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the training set...\n",
            "[999/999] - 10845s - MAE: { severe_congestion 3.32 -congestion 1.5 -free_flow 0.668 - total: 1.83 } - RMSE: { severe_congestion 4.07 -congestion 1.87 -free_flow 0.835 - total: 2.25 } - MAPE: { severe_congestion 14.9% -congestion 3.07% -free_flow 1.02% - total: 6.34% } - Average time: 10.9s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the training set...')\n",
        "x_train_explained, y_train_explained, train_scores = get_all_explanations(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train.npy'), x_train_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train.npy'), y_train_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'train_scores.npy'), train_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train_time.npy'), x_train_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train_time.npy'), y_train_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdIyprvn8F7d",
        "outputId": "1eae176b-aa88-47dd-e9a9-76f5380e49ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the validation set...\n",
            "[198/198] - 1871s - MAE: { severe_congestion 3.63 -congestion 1.39 -free_flow 0.668 - total: 1.86 } - RMSE: { severe_congestion 4.32 -congestion 1.73 -free_flow 0.848 - total: 2.26 } - MAPE: { severe_congestion 17.2% -congestion 2.75% -free_flow 1.02% - total: 6.88% } - Average time: 9.45s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the validation set...')\n",
        "x_val_explained, y_val_explained, val_scores = get_all_explanations(\n",
        "    x_val,\n",
        "    y_val,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val.npy'), x_val_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val.npy'), y_val_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'val_scores.npy'), val_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val_time.npy'), x_val_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val_time.npy'), y_val_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-U5tti38F4V",
        "outputId": "d1e55598-33e1-47a2-b411-aee576e9ab57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the test set...\n",
            "[300/300] - 2663s - MAE: { severe_congestion 3.21 -congestion 1.65 -free_flow 0.713 - total: 1.84 } - RMSE: { severe_congestion 3.82 -congestion 2.06 -free_flow 0.881 - total: 2.24 } - MAPE: { severe_congestion 14.1% -congestion 3.45% -free_flow 1.09% - total: 6.14% } - Average time: 8.88s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the test set...')\n",
        "x_test_explained, y_test_explained, test_scores = get_all_explanations(\n",
        "    x_test,\n",
        "    y_test,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test.npy'), x_test_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test.npy'), y_test_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'test_scores.npy'), test_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test_time.npy'), x_test_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test_time.npy'), y_test_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Get the data and the values predicted by the STGNN.\n",
        "x_train_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_train.npy'))\n",
        "\n",
        "x_val_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_val.npy'))\n",
        "\n",
        "x_test_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.explanation.events import get_largest_event_set\n",
        "from src.explanation.monte_carlo.explanation import evaluate\n",
        "from src.spatial_temporal_gnn.metrics import MAE, RMSE, MAPE\n",
        "\n",
        "\n",
        "def print_all_fidelity_plus(\n",
        "    x,\n",
        "    y,\n",
        "    x_explained,\n",
        "    spatial_temporal_gnn,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind: bool = True):\n",
        "    mae_criterion = MAE()\n",
        "    rmse_criterion = RMSE()\n",
        "    mape_criterion = MAPE()\n",
        "    \n",
        "    if divide_by_traffic_cluster_kind:\n",
        "        # Get the severe congestion sparsity, the firs third of the data.\n",
        "        severe_congestion_mae, severe_congestion_rmse, severe_congestion_mape = get_fidelity_plus(\n",
        "            x[:len(x) // 3], \n",
        "            y[:len(x) // 3],\n",
        "            x_explained[:len(x) // 3],  \n",
        "            spatial_temporal_gnn, \n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        # Get the congestion sparsity, the second third of the data.\n",
        "        congestion_mae, congestion_rmse, congestion_mape = get_fidelity_plus(\n",
        "            x[len(x) // 3:2 * len(x) // 3],\n",
        "            y[len(x) // 3:2 * len(x) // 3],\n",
        "            x_explained[len(x) // 3:2 * len(x) // 3],\n",
        "            spatial_temporal_gnn, \n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        # Get the free flow sparsity, the last third of the data.\n",
        "        free_flow_mae, free_flow_rmse, free_flow_mape = get_fidelity_plus(\n",
        "            x[2 * len(x) // 3:],\n",
        "            y[2 * len(x) // 3:],\n",
        "            x_explained[2 * len(x) // 3:],\n",
        "            spatial_temporal_gnn, \n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        # Compute the average sparsity.\n",
        "        mae = (severe_congestion_mae + congestion_mae +\n",
        "                    free_flow_mae) / 3\n",
        "        rmse = (severe_congestion_rmse + congestion_rmse +\n",
        "                    free_flow_rmse) / 3\n",
        "        mape = (severe_congestion_mape + congestion_mape +\n",
        "                    free_flow_mape) / 3\n",
        "        print(\n",
        "            f'MAE+: {{ severe_congestion {severe_congestion_mae:.3g} -'\n",
        "            f'congestion {congestion_mae:.3g} -'\n",
        "            f'free_flow {free_flow_mae:.3g} -',\n",
        "            f'total: {mae:.3g}% }} -'\n",
        "            f'RMSE+: {{ severe_congestion {severe_congestion_rmse:.3g} -'\n",
        "            f'congestion {congestion_rmse:.3g} -'\n",
        "            f'free_flow {free_flow_rmse:.3g} -',\n",
        "            f'total: {rmse:.3g}% }} -'\n",
        "            f'MAPE+: {{ severe_congestion {severe_congestion_mape * 100:.3g}% -'\n",
        "            f'congestion {congestion_mape * 100.:.3g}% -'\n",
        "            f'free_flow {free_flow_mape * 100.:.3g}% -',\n",
        "            f'total: {mape * 100.:.3g}% }}')\n",
        "    else:\n",
        "        print(\n",
        "            f'MAE: {mae:.3g} -',\n",
        "            f'RMSE: {rmse:.3g} -',\n",
        "            f'MAPE: {mape * 100.:.3g}%')\n",
        "        \n",
        "def get_fidelity_plus(\n",
        "    x, \n",
        "    y, \n",
        "    x_explained, \n",
        "    spatial_temporal_gnn, \n",
        "    mae_criterion,\n",
        "    rmse_criterion,\n",
        "    mape_criterion,\n",
        "    remove_value):\n",
        "    x_ = x.copy()\n",
        "    # Get the events of the complement of x_explained.\n",
        "    x_[x_explained != 0.] = 0.\n",
        "    running_mae = 0.\n",
        "    running_rmse = 0.\n",
        "    running_mape = 0.\n",
        "    for i in range(len(x_)):\n",
        "        input_events = get_largest_event_set(x_[i])\n",
        "    \n",
        "        # Evaluate the results.\n",
        "        mae, rmse, mape = evaluate(\n",
        "            x_[i],\n",
        "            y[i],\n",
        "            input_events,\n",
        "            spatial_temporal_gnn,\n",
        "            scaler,\n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        running_mae += mae\n",
        "        running_rmse += rmse\n",
        "        running_mape += mape\n",
        "\n",
        "    return running_mae / len(x_), running_rmse / len(x_), running_mape / len(x_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the fidelity+ for the training set...\n",
            "MAE+: { severe_congestion 35.4 -congestion 7.96 -free_flow 2 - total: 15.1% } -RMSE+: { severe_congestion 36 -congestion 8.32 -free_flow 2.26 - total: 15.5% } -MAPE+: { severe_congestion 162% -congestion 16.6% -free_flow 3.06% - total: 60.4% }\n",
            "\n",
            "Computing the fidelity+ for the validation set...\n",
            "MAE+: { severe_congestion 37.5 -congestion 6.44 -free_flow 2.24 - total: 15.4% } -RMSE+: { severe_congestion 38 -congestion 6.77 -free_flow 2.49 - total: 15.8% } -MAPE+: { severe_congestion 194% -congestion 12.9% -free_flow 3.4% - total: 70.1% }\n",
            "\n",
            "Computing the fidelity+ for the test set...\n",
            "MAE+: { severe_congestion 35.3 -congestion 9.24 -free_flow 2.07 - total: 15.5% } -RMSE+: { severe_congestion 36 -congestion 9.69 -free_flow 2.31 - total: 16% } -MAPE+: { severe_congestion 165% -congestion 19.9% -free_flow 3.17% - total: 62.5% }\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Computing the fidelity+ for the training set...')\n",
        "print_all_fidelity_plus(x_train, y_train, x_train_explained, spatial_temporal_gnn, remove_value=0.)\n",
        "print()\n",
        "\n",
        "print('Computing the fidelity+ for the validation set...')\n",
        "print_all_fidelity_plus(x_val, y_val, x_val_explained, spatial_temporal_gnn, remove_value=0.)\n",
        "print()\n",
        "\n",
        "print('Computing the fidelity+ for the test set...')\n",
        "print_all_fidelity_plus(x_test, y_test, x_test_explained, spatial_temporal_gnn, remove_value=0.)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_all_sparsity(\n",
        "    x,\n",
        "    x_explained,\n",
        "    divide_by_traffic_cluster_kind: bool = True):\n",
        "    if divide_by_traffic_cluster_kind:\n",
        "        # Get the severe congestion sparsity, the firs third of the data.\n",
        "        severe_congestion_sparsity = get_sparsity(\n",
        "            x[:len(x) // 3], x_explained[:len(x) // 3])\n",
        "        # Get the congestion sparsity, the second third of the data.\n",
        "        congestion_sparsity = get_sparsity(\n",
        "            x[len(x) // 3:2 * len(x) // 3],\n",
        "            x_explained[len(x) // 3:2 * len(x) // 3])\n",
        "        # Get the free flow sparsity, the last third of the data.\n",
        "        free_flow_sparsity = get_sparsity(\n",
        "            x[2 * len(x) // 3:], x_explained[2 * len(x) // 3:])\n",
        "        # Compute the average sparsity.\n",
        "        sparsity = (severe_congestion_sparsity + congestion_sparsity +\n",
        "                    free_flow_sparsity) / 3\n",
        "        print(\n",
        "            f'Sparsity: {{ severe_congestion {severe_congestion_sparsity:.3g} -'\n",
        "            f'congestion {congestion_sparsity:.3g} -'\n",
        "            f'free_flow {free_flow_sparsity:.3g} -',\n",
        "            f'total: {sparsity:.3g} }}')\n",
        "    else:\n",
        "        print(f'Sparsity: {sparsity:.3g}')\n",
        "\n",
        "\n",
        "def get_sparsity(x, x_explained):\n",
        "    # Count the number of non-zero values in the original data, by time step in the last axis.\n",
        "    x_non_zero = np.count_nonzero(x[..., 0], axis=0)\n",
        "    # Count the number of non-zero values in the explained data, by time step.\n",
        "    x_explained_non_zero = np.count_nonzero(x_explained[..., 0], axis=0)\n",
        "    # Compute the sparsity, by time step.\n",
        "    sparsity = 1 - x_explained_non_zero / x_non_zero\n",
        "    # Compute the average sparsity.\n",
        "    return np.mean(sparsity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the sparsity for the training set...\n",
            "Sparsity: { severe_congestion 0.984 -congestion 0.988 -free_flow 0.985 - total: 0.986 }\n",
            "\n",
            "Computing the sparsity for the validation set...\n",
            "Sparsity: { severe_congestion 0.985 -congestion 0.988 -free_flow 0.985 - total: 0.986 }\n",
            "\n",
            "Computing the sparsity for the test set...\n",
            "Sparsity: { severe_congestion 0.983 -congestion 0.987 -free_flow 0.984 - total: 0.985 }\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Computing the sparsity for the training set...')\n",
        "print_all_sparsity(x_train, x_train_explained)\n",
        "print()\n",
        "\n",
        "print('Computing the sparsity for the validation set...')\n",
        "print_all_sparsity(x_val, x_val_explained)\n",
        "print()\n",
        "\n",
        "print('Computing the sparsity for the test set...')\n",
        "print_all_sparsity(x_test, x_test_explained)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
