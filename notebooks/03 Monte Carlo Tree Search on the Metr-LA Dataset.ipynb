{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6kbAULJKy-wu",
        "outputId": "73e29314-6d4d-45d1-faa7-2d56722e744d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"import sys\\nimport os\\n\\n# Set the main path in the root folder of the project.\\nsys.path.append(os.path.join('..'))\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Set the main path in the root folder of the project.\n",
        "sys.path.append(os.path.join('..'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGIZtEY6y-wv",
        "outputId": "9c96f509-1730-42fc-9ea5-0f553509f1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Settings for autoreloading.\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "obBqbIuGy-wv"
      },
      "outputs": [],
      "source": [
        "from src.utils.seed import set_random_seed\n",
        "\n",
        "# Set the random seed for deterministic operations.\n",
        "SEED = 42\n",
        "set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHGuhgDmy-wv",
        "outputId": "908982ff-f94a-485a-c8e9-39f97fac303e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The selected device is: \"cuda\"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set the device for training and querying the model.\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'The selected device is: \"{DEVICE}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhVgau7-y-ww"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "HlTWno-1y-wx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "BbiHBGWSy-wx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
        "    scaler = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "nb6d7kVZy-wx"
      },
      "outputs": [],
      "source": [
        "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
        "from src.data.data_extraction import get_adjacency_matrix\n",
        "\n",
        "# Get the adjacency matrix\n",
        "adj_matrix_structure = get_adjacency_matrix(\n",
        "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
        "\n",
        "# Get the header of the adjacency matrix, the node indices and the\n",
        "# matrix itself.\n",
        "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
        "\n",
        "# Get the STGNN and load the checkpoints.\n",
        "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
        "\n",
        "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
        "                                      'st_gnn_metr_la.pth')\n",
        "\n",
        "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
        "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
        "\n",
        "# Set the STGNN in evaluation mode.\n",
        "spatial_temporal_gnn.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "YIB5Kn57y-wx"
      },
      "outputs": [],
      "source": [
        "from src.data.data_extraction import get_locations_dataframe\n",
        "\n",
        "# Get the dataframe containing the latitude and longitude of each sensor.\n",
        "locations_df = get_locations_dataframe(\n",
        "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
        "    has_header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "kwVU6H96y-wy"
      },
      "outputs": [],
      "source": [
        "# Get the node positions dictionary.\n",
        "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "x-lj4muty-wy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Get the data scaler.\n",
        "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
        "    scaler = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "kNItiWwGy-wy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Get the data and the values predicted by the STGNN.\n",
        "x_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train.npy'))\n",
        "y_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train.npy'))\n",
        "x_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train_time.npy'))\n",
        "y_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train_time.npy'))\n",
        "\n",
        "x_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val.npy'))\n",
        "y_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val.npy'))\n",
        "x_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val_time.npy'))\n",
        "y_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val_time.npy'))\n",
        "\n",
        "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test.npy'))\n",
        "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test.npy'))\n",
        "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test_time.npy'))\n",
        "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test_time.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "bCEWyqCvy-wz"
      },
      "outputs": [],
      "source": [
        "from src.data.data_processing import get_distance_matrix\n",
        "\n",
        "if not os.path.exists(\n",
        "    os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy')):\n",
        "    # Build the distance matrix between the nodes.\n",
        "    distance_matrix = get_distance_matrix(\n",
        "        adj_matrix,\n",
        "        locations_df,\n",
        "        node_pos_dict)\n",
        "\n",
        "    # Save the distance matrix.\n",
        "    np.save(os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'),\n",
        "            distance_matrix)\n",
        "\n",
        "else:\n",
        "    # Load the distance matrix.\n",
        "    distance_matrix = np.load(\n",
        "        os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKZtVQc2y-wz",
        "outputId": "040df60c-b0a3-493a-8eb9-0d0495151d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 547s - MAE: { severe_congestion 3.38 -congestion 1.48 -free_flow 0.587 - total: 1.68 } - RMSE: { severe_congestion 3.93 -congestion 1.81 -free_flow 0.756 - total: 2.01 } - MAPE: { severe_congestion 15.1% -congestion 2.93% -free_flow 0.895% - total: 5.79% } - Average time: 5.47s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 30 remove_value: -10.0\n",
            "[100/100] - 545s - MAE: { severe_congestion 3.35 -congestion 1.53 -free_flow 0.711 - total: 1.73 } - RMSE: { severe_congestion 4.01 -congestion 1.91 -free_flow 0.892 - total: 2.13 } - MAPE: { severe_congestion 15.4% -congestion 3.07% -free_flow 1.08% - total: 6.06% } - Average time: 5.45s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 890s - MAE: { severe_congestion 2.59 -congestion 1.36 -free_flow 0.557 - total: 1.46 } - RMSE: { severe_congestion 3.09 -congestion 1.68 -free_flow 0.721 - total: 1.78 } - MAPE: { severe_congestion 11.7% -congestion 2.72% -free_flow 0.849% - total: 4.96% } - Average time: 8.9s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 50 remove_value: -10.0\n",
            "[100/100] - 900s - MAE: { severe_congestion 4.03 -congestion 1.53 -free_flow 0.674 - total: 1.96 } - RMSE: { severe_congestion 5 -congestion 1.88 -free_flow 0.851 - total: 2.41 } - MAPE: { severe_congestion 24.2% -congestion 3.05% -free_flow 1.03% - total: 9% } - Average time: 9s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 540s - MAE: { severe_congestion 2.74 -congestion 1.64 -free_flow 0.592 - total: 1.6 } - RMSE: { severe_congestion 3.41 -congestion 2.12 -free_flow 0.758 - total: 2.02 } - MAPE: { severe_congestion 12.6% -congestion 3.26% -free_flow 0.903% - total: 5.43% } - Average time: 5.4s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 30 remove_value: -10.0\n",
            "[100/100] - 544s - MAE: { severe_congestion 4.29 -congestion 1.66 -free_flow 0.656 - total: 2.02 } - RMSE: { severe_congestion 5.37 -congestion 2.06 -free_flow 0.84 - total: 2.53 } - MAPE: { severe_congestion 22.2% -congestion 3.29% -free_flow 1% - total: 8.12% } - Average time: 5.44s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 888s - MAE: { severe_congestion 2.66 -congestion 1.57 -free_flow 0.561 - total: 1.56 } - RMSE: { severe_congestion 3.24 -congestion 1.89 -free_flow 0.742 - total: 1.91 } - MAPE: { severe_congestion 12.1% -congestion 3.18% -free_flow 0.858% - total: 5.27% } - Average time: 8.88s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 50 remove_value: -10.0\n",
            "[100/100] - 891s - MAE: { severe_congestion 3.42 -congestion 1.41 -free_flow 0.695 - total: 1.75 } - RMSE: { severe_congestion 4.05 -congestion 1.71 -free_flow 0.862 - total: 2.09 } - MAPE: { severe_congestion 19.3% -congestion 2.8% -free_flow 1.06% - total: 7.44% } - Average time: 8.91s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 541s - MAE: { severe_congestion 3.01 -congestion 1.41 -free_flow 0.622 - total: 1.61 } - RMSE: { severe_congestion 3.73 -congestion 1.74 -free_flow 0.798 - total: 1.99 } - MAPE: { severe_congestion 13.7% -congestion 2.76% -free_flow 0.951% - total: 5.57% } - Average time: 5.41s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 30 remove_value: -10.0\n",
            "[100/100] - 544s - MAE: { severe_congestion 3.81 -congestion 1.6 -free_flow 0.698 - total: 2 } - RMSE: { severe_congestion 4.59 -congestion 1.94 -free_flow 0.906 - total: 2.43 } - MAPE: { severe_congestion 22.1% -congestion 3.18% -free_flow 1.07% - total: 8.73% } - Average time: 5.44s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 888s - MAE: { severe_congestion 2.63 -congestion 1.31 -free_flow 0.546 - total: 1.45 } - RMSE: { severe_congestion 3.26 -congestion 1.63 -free_flow 0.707 - total: 1.81 } - MAPE: { severe_congestion 11.9% -congestion 2.6% -free_flow 0.834% - total: 4.99% } - Average time: 8.88s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 50 remove_value: -10.0\n",
            "[100/100] - 895s - MAE: { severe_congestion 3.22 -congestion 1.32 -free_flow 0.624 - total: 1.58 } - RMSE: { severe_congestion 4.3 -congestion 1.68 -free_flow 0.782 - total: 2.05 } - MAPE: { severe_congestion 15.4% -congestion 2.64% -free_flow 0.952% - total: 5.74% } - Average time: 8.95s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 669s - MAE: { severe_congestion 2.97 -congestion 1.36 -free_flow 0.574 - total: 1.57 } - RMSE: { severe_congestion 3.73 -congestion 1.71 -free_flow 0.718 - total: 1.96 } - MAPE: { severe_congestion 13.2% -congestion 2.69% -free_flow 0.876% - total: 5.33% } - Average time: 6.69s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 30 remove_value: -10.0\n",
            "[100/100] - 675s - MAE: { severe_congestion 3.53 -congestion 1.53 -free_flow 0.665 - total: 1.76 } - RMSE: { severe_congestion 4.36 -congestion 1.84 -free_flow 0.859 - total: 2.18 } - MAPE: { severe_congestion 16.4% -congestion 3.03% -free_flow 1.02% - total: 6.24% } - Average time: 6.75s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1110s - MAE: { severe_congestion 2.52 -congestion 1.44 -free_flow 0.57 - total: 1.46 } - RMSE: { severe_congestion 3.15 -congestion 1.77 -free_flow 0.736 - total: 1.83 } - MAPE: { severe_congestion 11.3% -congestion 2.91% -free_flow 0.87% - total: 4.87% } - Average time: 11.1s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 50 remove_value: -10.0\n",
            "[100/100] - 1111s - MAE: { severe_congestion 2.94 -congestion 1.49 -free_flow 0.667 - total: 1.56 } - RMSE: { severe_congestion 3.84 -congestion 1.8 -free_flow 0.829 - total: 1.96 } - MAPE: { severe_congestion 13.4% -congestion 2.97% -free_flow 1.02% - total: 5.21% } - Average time: 11.1s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 664s - MAE: { severe_congestion 2.86 -congestion 1.49 -free_flow 0.588 - total: 1.57 } - RMSE: { severe_congestion 3.51 -congestion 1.79 -free_flow 0.728 - total: 1.91 } - MAPE: { severe_congestion 12.7% -congestion 3.03% -free_flow 0.898% - total: 5.28% } - Average time: 6.64s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 30 remove_value: -10.0\n",
            "[100/100] - 666s - MAE: { severe_congestion 2.74 -congestion 1.69 -free_flow 0.714 - total: 1.66 } - RMSE: { severe_congestion 3.46 -congestion 2.03 -free_flow 0.868 - total: 2.04 } - MAPE: { severe_congestion 12.2% -congestion 3.41% -free_flow 1.09% - total: 5.41% } - Average time: 6.66s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1107s - MAE: { severe_congestion 2.37 -congestion 1.38 -free_flow 0.536 - total: 1.39 } - RMSE: { severe_congestion 2.87 -congestion 1.66 -free_flow 0.69 - total: 1.69 } - MAPE: { severe_congestion 10.5% -congestion 2.76% -free_flow 0.82% - total: 4.57% } - Average time: 11.1s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 50 remove_value: -10.0\n",
            "[100/100] - 1110s - MAE: { severe_congestion 2.53 -congestion 1.44 -free_flow 0.622 - total: 1.43 } - RMSE: { severe_congestion 3.17 -congestion 1.78 -free_flow 0.797 - total: 1.8 } - MAPE: { severe_congestion 11.4% -congestion 2.86% -free_flow 0.949% - total: 4.72% } - Average time: 11.1s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 662s - MAE: { severe_congestion 3.06 -congestion 1.45 -free_flow 0.622 - total: 1.62 } - RMSE: { severe_congestion 3.77 -congestion 1.77 -free_flow 0.82 - total: 2 } - MAPE: { severe_congestion 13.8% -congestion 2.87% -free_flow 0.949% - total: 5.57% } - Average time: 6.62s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 30 remove_value: -10.0\n",
            "[100/100] - 667s - MAE: { severe_congestion 3.59 -congestion 1.6 -free_flow 0.667 - total: 1.76 } - RMSE: { severe_congestion 4.53 -congestion 1.96 -free_flow 0.866 - total: 2.2 } - MAPE: { severe_congestion 15.7% -congestion 3.2% -free_flow 1.02% - total: 5.94% } - Average time: 6.67s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1102s - MAE: { severe_congestion 2.4 -congestion 1.42 -free_flow 0.55 - total: 1.43 } - RMSE: { severe_congestion 2.96 -congestion 1.78 -free_flow 0.716 - total: 1.78 } - MAPE: { severe_congestion 10.6% -congestion 2.85% -free_flow 0.839% - total: 4.69% } - Average time: 11s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 50 remove_value: -10.0\n",
            "[100/100] - 1114s - MAE: { severe_congestion 2.45 -congestion 1.52 -free_flow 0.652 - total: 1.48 } - RMSE: { severe_congestion 3.07 -congestion 1.92 -free_flow 0.853 - total: 1.87 } - MAPE: { severe_congestion 11.2% -congestion 3.02% -free_flow 0.994% - total: 4.89% } - Average time: 11.1s \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import apply_grid_search\n",
        "\n",
        "apply_grid_search(\n",
        "    x_train[::10],\n",
        "    y_train[::10],\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts_list=[30, 50],\n",
        "    explanation_size_factor_list=[2, 3],\n",
        "    cut_size_factor_list=[2],\n",
        "    exploration_weight_list=[5, 10, 20],\n",
        "    remove_value_list=[0., -10.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "qVtOLnQ68GDf"
      },
      "outputs": [],
      "source": [
        "CUT_SIZE_FACTOR = 2\n",
        "EXPLANATION_SIZE_FACTOR = 3\n",
        "EXPLORATION_WEIGHT = 10\n",
        "N_ROLLOUTS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "3FYN5mEu8GAl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "EXPLAINED_DATA_DIR = os.path.join(BASE_DATA_DIR, 'explained')\n",
        "os.makedirs(EXPLAINED_DATA_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdB__0mW8F-E",
        "outputId": "e2ac1c5c-0869-4033-f17f-6406838de2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the training set...\n",
            "[999/999] - 11263s - MAE: { severe_congestion 2.95 -congestion 1.23 -free_flow 0.561 - total: 1.57 } - RMSE: { severe_congestion 3.61 -congestion 1.54 -free_flow 0.719 - total: 1.94 } - MAPE: { severe_congestion 13.1% -congestion 2.5% -free_flow 0.861% - total: 5.44% } - Average time: 11.3s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the training set...')\n",
        "x_train_explained, y_train_explained, train_scores = get_all_explanations(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train.npy'), x_train_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train.npy'), y_train_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'train_scores.npy'), train_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train_time.npy'), x_train_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train_time.npy'), y_train_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdIyprvn8F7d",
        "outputId": "c5b318d7-ab59-48ef-d29f-64de570e857e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the validation set...\n",
            "[198/198] - 2155s - MAE: { severe_congestion 2.94 -congestion 1.39 -free_flow 0.548 - total: 1.58 } - RMSE: { severe_congestion 3.61 -congestion 1.73 -free_flow 0.693 - total: 1.95 } - MAPE: { severe_congestion 13% -congestion 2.83% -free_flow 0.845% - total: 5.41% } - Average time: 10.9s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the validation set...')\n",
        "x_val_explained, y_val_explained, val_scores = get_all_explanations(\n",
        "    x_val,\n",
        "    y_val,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val.npy'), x_val_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val.npy'), y_val_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'val_scores.npy'), val_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val_time.npy'), x_val_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val_time.npy'), y_val_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-U5tti38F4V",
        "outputId": "9bad1538-9bc9-4cc9-a524-0a42a3d8b371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the test set...\n",
            "[300/300] - 3428s - MAE: { severe_congestion 3.41 -congestion 1.29 -free_flow 0.556 - total: 1.73 } - RMSE: { severe_congestion 4.18 -congestion 1.62 -free_flow 0.704 - total: 2.14 } - MAPE: { severe_congestion 16.3% -congestion 2.58% -free_flow 0.854% - total: 6.51% } - Average time: 11.4s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the test set...')\n",
        "x_test_explained, y_test_explained, test_scores = get_all_explanations(\n",
        "    x_test,\n",
        "    y_test,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test.npy'), x_test_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test.npy'), y_test_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'test_scores.npy'), test_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test_time.npy'), x_test_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test_time.npy'), y_test_time)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
