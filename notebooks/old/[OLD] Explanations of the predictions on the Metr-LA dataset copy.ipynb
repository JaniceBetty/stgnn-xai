{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the event set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialTemporalGNN(\n",
       "  (encoder): Linear(in_features=9, out_features=64, bias=False)\n",
       "  (s_gnns): ModuleList(\n",
       "    (0-11): 12 x S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (hidden_s_gnns): ModuleList(\n",
       "    (0-10): 11 x S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (grus): ModuleList(\n",
       "    (0-11): 12 x GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (positional_encoder): PositionalEncoder()\n",
       "  (transformer): Transformer(\n",
       "    (queries_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (keys_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (values_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (normalization): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (normalization_out): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (prediction_head): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix and the matrix itself.\n",
    "header, _, adj_matrix = adj_matrix_structure\n",
    "\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "spatial_temporal_gnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from src.spatial_temporal_gnn.prediction import predict\n",
    "\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_train.npy'))\n",
    "y_train = predict(spatial_temporal_gnn, x_train, scaler, DEVICE)\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_val.npy'))\n",
    "y_val = predict(spatial_temporal_gnn, x_val, scaler, DEVICE)\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_test.npy'))\n",
    "y_test = predict(spatial_temporal_gnn, x_test, scaler, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from src.data.data_analysis import days_encoder\n",
    "\n",
    "def get_largest_event_set(data: np.ndarray\n",
    "                          ) -> List[Tuple[str, Optional[int], Optional[int]]]:\n",
    "    n_time_steps, n_nodes, _ = data.shape[-3:]\n",
    "    \n",
    "    # Get the largest event set related to the speed.\n",
    "    speed_events = [\n",
    "        (0, time_step, node) \n",
    "        for time_step in range(n_time_steps) \n",
    "        for node in range(n_nodes) \n",
    "        if data[..., time_step, node, 0] > 0]\n",
    "    # Get the largest event set related to the time of day.\n",
    "    time_of_day_events = [\n",
    "        (1, time_step, None) for time_step in range(n_time_steps)]\n",
    "    # Get the largest event set related to the day of week.\n",
    "    day_of_week_events = [(2, None, None)]\n",
    "    # Get the largest event set related to the kind of day.\n",
    "    #kind_of_day_events = [(3, None, None)]\n",
    "    \n",
    "    return speed_events + time_of_day_events + day_of_week_events #+\\\n",
    "    #    kind_of_day_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_largest_event_set(x_test[11])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map the event set to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "def remove_features_by_events(\n",
    "    data: Union[np.ndarray, torch.FloatTensor],\n",
    "    events: List[Tuple[int, Optional[int], Optional[int]]]\n",
    "    ) -> Union[np.ndarray, torch.FloatTensor]:\n",
    "    if isinstance(data, torch.FloatTensor):\n",
    "        filtered_data = data.clone()\n",
    "    else:\n",
    "        filtered_data = data.copy()\n",
    "    n_time_steps, n_nodes, n_features = filtered_data.shape[-3:]\n",
    "    \n",
    "    speed_events = [tuple(event) for event in events if event[0] == 0]\n",
    "    time_of_day_events = [tuple(event) for event in events if event[0] == 1]\n",
    "    day_of_week_events = [tuple(event) for event in events if event[0] == 2]\n",
    "    #kind_of_day_events = [event for event in events if event[0] == 3]\n",
    "    \n",
    "    if n_features > 1 and not len(day_of_week_events):\n",
    "        filtered_data[..., -7:] = 0\n",
    "    \n",
    "    for time_step in range(n_time_steps):\n",
    "        for node in range(n_nodes):\n",
    "            if (0, time_step, node) not in speed_events:\n",
    "                filtered_data[..., time_step, node, 0] = 0\n",
    "\n",
    "            '''if n_features > 1 and len(kind_of_day_events):\n",
    "                if 1 in data[..., -7:-2]:\n",
    "                    filtered_data[..., time_step, node, -7:-2] = 1\n",
    "                else:\n",
    "                    filtered_data[..., time_step, node, -2:] = 1'''\n",
    "\n",
    "        if n_features > 1 and (1, time_step, None) not in time_of_day_events:\n",
    "            filtered_data[..., time_step, :, 1] = -1\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_event_from_data(\n",
    "    data: Union[np.ndarray, torch.FloatTensor],\n",
    "    event: Union[np.ndarray, torch.FloatTensor]\n",
    "    ) -> Union[np.ndarray, torch.FloatTensor]:\n",
    "    # n_time_steps, n_nodes, _ = data.shape[-3:]\n",
    "    event_kind = event[0]\n",
    "    time_step = int(event[1].item())\n",
    "    node = int(event[2].item())\n",
    "    if event_kind == 0:\n",
    "        data[..., time_step, node, 0] = 0\n",
    "    elif event_kind == 1:\n",
    "        data[..., time_step, :, 1] = -1\n",
    "    elif event_kind == 2:\n",
    "        data[..., -7:] = 0\n",
    "    #elif event[0] == 3:\n",
    "    #    pass\n",
    "\n",
    "    '''if event_kind in [0, 1]: \n",
    "        for time_step in range(n_time_steps):\n",
    "            for node in range(n_nodes):\n",
    "                if 1 in data[..., -7:-2]:\n",
    "                    data[..., time_step, node, -7:-2] = 1\n",
    "                else:\n",
    "                    data[..., time_step, node, -2:] = 1'''\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = remove_features_by_events(x_test[0], [(1, 0, None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (event_type, speed, time_of_day, ...day, speed_target, time_of_day_target, ...day_target)\n",
    "\n",
    "# ->\n",
    "\n",
    "# -> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 207, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "def simulate_model(\n",
    "    instance: torch.FloatTensor, candidate_event: torch.FloatTensor,\n",
    "    candidate_event_score: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    # Get 1 or 0 with probability equal to the score.\n",
    "    result = torch.bernoulli(candidate_event_score)\n",
    "    for i in range(len(result)):\n",
    "        if result[i]:\n",
    "            instance[i] = remove_single_event_from_data(\n",
    "                instance[i], candidate_event[i])\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Navigator(nn.Module):\n",
    "    def __init__(self, device: str, hidden_features: int = 64) -> None:\n",
    "        super().__init__()\n",
    "        # Set the linear encoder.\n",
    "        self.linear_encoder = nn.LazyLinear(hidden_features)\n",
    "        # Set the linear decoder.\n",
    "        self.linear_decoder = nn.Linear(hidden_features, 1)\n",
    "        # Set the device that is used for training and querying the model.\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, candidate_event: torch.FloatTensor, target_events: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # Flatten the target events.\n",
    "        #target_events = target_events.flatten(start_dim=1)\n",
    "        # Concatenate the candidate event and the target events.\n",
    "        x = torch.cat((candidate_event, target_events), dim=1)\n",
    "        # Encode the input.\n",
    "        out = self.linear_encoder(x)\n",
    "        # Decode the output to get the logits prediction.\n",
    "        out = self.linear_decoder(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Navigator(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Optional, Tuple\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EventsDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"Initialize the dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            The input values of the dataset.\n",
    "        y : ndarray\n",
    "            The ground truth of the dataset's input data.\n",
    "        \"\"\"\n",
    "        events = []\n",
    "        for idx, (input, target) in enumerate(zip(x, y)):\n",
    "            input_events = get_largest_event_set(input)\n",
    "            target_events = get_largest_event_set(target)\n",
    "            permut = itertools.permutations(input_events, len(target_events))\n",
    "            for comb in permut:\n",
    "                zipped = zip(comb, target_events)\n",
    "                for input_event, target_event in zipped:\n",
    "                    events.append((idx, input_event, target_event))\n",
    "        \n",
    "        self.x = { i: input for i, input in enumerate(x) }\n",
    "        self.events = events\n",
    "        self.y = { i: target for i, target in enumerate(y) }\n",
    "        self.len = len(events)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get a dataset \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            The index from where to extract the dataset instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The input data at the given index.\n",
    "        ndarray\n",
    "            The ground truth with respect to the input data at the\n",
    "            given index.\n",
    "        \"\"\"\n",
    "        event = self.events[index]\n",
    "        idx = event[0]\n",
    "        input_event = event[1]\n",
    "        target_event = event[2]\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return x, y, torch.Tensor(input_event), torch.Tensor(target_event)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the length of the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "\n",
    "def get_dataloader(x: np.ndarray, y: np.ndarray, \n",
    "                   batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    dataset = EventsDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 25\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loader \u001b[39m=\u001b[39m get_dataloader(x_train, y_train, \u001b[39m64\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m val_loader \u001b[39m=\u001b[39m get_dataloader(x_val, y_val, \u001b[39m64\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 25\u001b[0m in \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataloader\u001b[39m(x: np\u001b[39m.\u001b[39mndarray, y: np\u001b[39m.\u001b[39mndarray, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m                    batch_size: \u001b[39mint\u001b[39m, shuffle: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataLoader:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     dataset \u001b[39m=\u001b[39m EventsDataset(x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39mshuffle)\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 25\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         zipped \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(comb, target_events)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39mfor\u001b[39;00m input_event, target_event \u001b[39min\u001b[39;00m zipped:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m             events\u001b[39m.\u001b[39;49mappend((idx, input_event, target_event))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m { i: \u001b[39minput\u001b[39m \u001b[39mfor\u001b[39;00m i, \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x) }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevents \u001b[39m=\u001b[39m events\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader(x_train, y_train, 64, True)\n",
    "val_loader = get_dataloader(x_val, y_val, 64, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x  = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spatial_temporal_gnn.training import Checkpoint\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=2e-6)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=.1, patience=10, verbose=False,\n",
    "    threshold=.001, threshold_mode='rel', cooldown=0, min_lr=1e-5, eps=1e-08)\n",
    "\n",
    "checkpoint_file_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                    'navigator_metr_la.pth')\n",
    "checkpoint = Checkpoint(checkpoint_file_path)\n",
    "\n",
    "EPOCHS = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.spatial_temporal_gnn.metrics import MAE, RMSE, MAPE\n",
    "from src.spatial_temporal_gnn.training import Checkpoint\n",
    "from src.data.data_processing import Scaler\n",
    "\n",
    "def train(\n",
    "    model: Navigator, optimizer: torch.optim.Optimizer,\n",
    "    train_dataloader: DataLoader, val_dataloader: DataLoader,\n",
    "    spatial_temporal_gnn: SpatialTemporalGNN, scaler: Scaler,\n",
    "    epochs: int, checkpoint: Optional[Checkpoint] = None,\n",
    "    lr_scheduler: Optional[object] = None,\n",
    "    reload_best_weights: bool = True) -> Dict[str, np.ndarray]:\n",
    "    # Get the device that is used for training and querying the model.\n",
    "    device = model.device\n",
    "\n",
    "    # Initialize the training criterions.\n",
    "    mae_criterion = MAE()\n",
    "    rmse_criterion = RMSE()\n",
    "    mape_criterion = MAPE()\n",
    "\n",
    "    # Initialize the histories.\n",
    "    metrics = ['train_mae', 'train_rmse', 'train_mape', 'val_mae', 'val_rmse',\n",
    "               'val_mape']\n",
    "    history = { m: [] for m in metrics }\n",
    "\n",
    "    # Set model in training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate across the epochs.\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        # Remove unused tensors from gpu memory.\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Initialize the running errors.\n",
    "        running_train_mae = 0.\n",
    "        running_train_rmse = 0.\n",
    "        running_train_mape = 0.\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        for batch_idx, (x, y, input_event, target_event) in enumerate(train_dataloader):\n",
    "            # Increment the number of batch steps.\n",
    "            batch_steps = batch_idx + 1\n",
    "\n",
    "            # Get the data.\n",
    "            x = x.type(torch.float32).to(device=device)\n",
    "            input_event = input_event.type(torch.float32).to(device=device)\n",
    "            target_event = target_event.type(torch.float32).to(device=device)\n",
    "            y = y.type(torch.float32).to(device=device)\n",
    "            \n",
    "            event_score = model(input_event, target_event)\n",
    "            \n",
    "            x = simulate_model(x, input_event, event_score.sigmoid())\n",
    "\n",
    "            x = scaler.scale(x)\n",
    "\n",
    "            # Compute the Spatial-Temporal GNN model predictions.\n",
    "            y_pred = spatial_temporal_gnn(x)\n",
    "\n",
    "            # Compute the loss on the scaled results and ground truth.\n",
    "            y_pred = scaler.un_scale(y_pred)\n",
    "\n",
    "            y = remove_features_by_events(y, [target_event])\n",
    "\n",
    "            loss = mae_criterion(y_pred, y)\n",
    "\n",
    "            # Compute errors and update running errors.\n",
    "            with torch.no_grad():\n",
    "                rmse = rmse_criterion(y_pred, y)\n",
    "                mape = mape_criterion(y_pred, y)\n",
    "\n",
    "            running_train_mae += loss.item()\n",
    "            running_train_rmse += rmse.item()\n",
    "            running_train_mape += mape.item()\n",
    "\n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use MAE as the loss function for backpropagation.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get the batch time.\n",
    "            epoch_time = time() - start_time\n",
    "            batch_time = epoch_time / batch_steps\n",
    "\n",
    "            # Print the batch results.\n",
    "            print(\n",
    "                f'[{batch_steps}/{len(train_dataloader)}] -',\n",
    "                f'{epoch_time:.0f}s {batch_time * 1e3:.0f}ms/step -',\n",
    "\n",
    "                f'train {{ MAE (loss): {running_train_mae / batch_steps:.3g} -',\n",
    "                f'RMSE: {running_train_rmse / batch_steps:.3g} -',\n",
    "                f'MAPE: {running_train_mape * 100. / batch_steps:.3g}% }} -',\n",
    "\n",
    "                f'lr: {optimizer.param_groups[0][\"lr\"]:.3g} -',\n",
    "                f'weight decay: {optimizer.param_groups[0][\"weight_decay\"]}',\n",
    "                '             ' if batch_steps < len(train_dataloader) else '',\n",
    "                end='\\r')\n",
    "\n",
    "        # Set the model in evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        # Get the average training errors and update the history.\n",
    "        train_mae = running_train_mae / len(train_dataloader)\n",
    "        train_rmse = running_train_rmse / len(train_dataloader)\n",
    "        train_mape = running_train_mape / len(train_dataloader)\n",
    "\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['train_mape'].append(train_mape)\n",
    "\n",
    "        # Get the validation results and update the history.\n",
    "        val_results = validate(model, val_dataloader, spatial_temporal_gnn,\n",
    "                               scaler)\n",
    "        val_mae, val_rmse, val_mape = val_results\n",
    "\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mape'].append(val_mape)\n",
    "\n",
    "        # Save the checkpoints if demanded.\n",
    "        if checkpoint is not None:\n",
    "            err_sum = val_mae + val_rmse + val_mape\n",
    "            checkpoint.save_best(model, optimizer, err_sum)\n",
    "\n",
    "        # Print the epoch results.\n",
    "        print(\n",
    "            f'[{len(train_dataloader)}/{len(train_dataloader)}] -',\n",
    "            f'{epoch_time:.0f}s -',\n",
    "\n",
    "            f'train: {{ MAE (loss): {train_mae:.3g} -',\n",
    "            f'RMSE: {train_rmse:.3g} -',\n",
    "            f'MAPE: {train_mape * 100.:.3g}% }} -',\n",
    "\n",
    "            f'val: {{ MAE: {val_mae:.3g} -',\n",
    "            f'RMSE: {val_rmse:.3g} -',\n",
    "            f'MAPE: {val_mape * 100.:.3g}% }} -',\n",
    "\n",
    "            f'lr: {optimizer.param_groups[0][\"lr\"]:.3g} -',\n",
    "            f'weight decay: {optimizer.param_groups[0][\"weight_decay\"]}')\n",
    "\n",
    "        # Update the learning rate scheduler.\n",
    "        lr_scheduler.step(train_mae)\n",
    "\n",
    "        # Set model in training mode.\n",
    "        model.train()\n",
    "\n",
    "    # Load the best weights of the model if demanded.\n",
    "    if checkpoint is not None and reload_best_weights:\n",
    "        checkpoint.load_best_weights(model)\n",
    "\n",
    "    # Set the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Remove unused tensors from gpu memory.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Turn the history to numpy arrays.\n",
    "    for k, v in history.items():\n",
    "        history[k] = np.array(v)\n",
    "\n",
    "    return history\n",
    "\n",
    "def validate(\n",
    "    model: Navigator, val_dataloader: DataLoader,\n",
    "    spatial_temporal_gnn: SpatialTemporalGNN, scaler: Scaler\n",
    "    ) -> Tuple[float, float, float]:\n",
    "    device = model.device\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize the validation criterions.\n",
    "    mae_criterion = MAE()\n",
    "    rmse_criterion = RMSE()\n",
    "    mape_criterion = MAPE()\n",
    "\n",
    "    # Inizialize running errors.\n",
    "    running_val_mae = 0.\n",
    "    running_val_rmse = 0.\n",
    "    running_val_mape = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (x, y, input_event, target_event) in enumerate(val_dataloader):\n",
    "            # Get the data.\n",
    "            x = x.type(torch.float32).to(device=device)\n",
    "            input_event = input_event.type(torch.float32).to(device=device)\n",
    "            target_event = target_event.type(torch.float32).to(device=device)\n",
    "            y = y.type(torch.float32).to(device=device)\n",
    "            \n",
    "            event_score = model(input_event, target_event)\n",
    "            \n",
    "            x = simulate_model(x, input_event, event_score.sigmoid())\n",
    "\n",
    "            x = scaler.scale(x)\n",
    "\n",
    "            # Compute the Spatial-Temporal GNN model predictions.\n",
    "            y_pred = spatial_temporal_gnn(x)\n",
    "\n",
    "            # Compute the loss on the scaled results and ground truth.\n",
    "            y_pred = scaler.un_scale(y_pred)\n",
    "\n",
    "            y = remove_features_by_events(y, [target_event])\n",
    "\n",
    "            # Get the prediction errors and update the running errors.\n",
    "            mae = mae_criterion(y_pred, y)\n",
    "            rmse = rmse_criterion(y_pred, y)\n",
    "            mape = mape_criterion(y_pred, y)\n",
    "\n",
    "            running_val_mae += mae.item()\n",
    "            running_val_rmse += rmse.item()\n",
    "            running_val_mape += mape.item()\n",
    "\n",
    "    # Remove unused tensors from gpu memory.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Get the average MAE, RMSE and MAPE scores.\n",
    "    val_mae = running_val_mae / len(val_dataloader)\n",
    "    val_rmse = running_val_rmse / len(val_dataloader)\n",
    "    val_mape = running_val_mape / len(val_dataloader)\n",
    "\n",
    "    return val_mae, val_rmse, val_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00286 - RMSE: 0.0332 - MAPE: 0.00488% } - val: { MAE: 0.0057 - RMSE: 0.0468 - MAPE: 0.017% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 2/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00274 - RMSE: 0.039 - MAPE: 0.0047% } - val: { MAE: 0.00505 - RMSE: 0.0434 - MAPE: 0.0141% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 3/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00261 - RMSE: 0.0367 - MAPE: 0.00447% } - val: { MAE: 0.0051 - RMSE: 0.0316 - MAPE: 0.0151% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 4/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.0023 - RMSE: 0.0297 - MAPE: 0.00393% } - val: { MAE: 0.00648 - RMSE: 0.0552 - MAPE: 0.0177% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 5/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00235 - RMSE: 0.0328 - MAPE: 0.00396% } - val: { MAE: 0.00732 - RMSE: 0.0561 - MAPE: 0.0224% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 6/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00228 - RMSE: 0.031 - MAPE: 0.00377% } - val: { MAE: 0.00733 - RMSE: 0.0798 - MAPE: 0.0216% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 7/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00299 - RMSE: 0.0337 - MAPE: 0.00519% } - val: { MAE: 0.00659 - RMSE: 0.0449 - MAPE: 0.021% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 8/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00244 - RMSE: 0.0305 - MAPE: 0.00412% } - val: { MAE: 0.00612 - RMSE: 0.0404 - MAPE: 0.0193% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 9/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00248 - RMSE: 0.039 - MAPE: 0.00423% } - val: { MAE: 0.00834 - RMSE: 0.0714 - MAPE: 0.0234% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 10/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00235 - RMSE: 0.0306 - MAPE: 0.00397% } - val: { MAE: 0.00473 - RMSE: 0.0318 - MAPE: 0.0141% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 11/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00265 - RMSE: 0.038 - MAPE: 0.00463% } - val: { MAE: 0.00633 - RMSE: 0.0418 - MAPE: 0.0206% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 12/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00255 - RMSE: 0.0313 - MAPE: 0.00429% } - val: { MAE: 0.00599 - RMSE: 0.0442 - MAPE: 0.0181% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 13/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00248 - RMSE: 0.0354 - MAPE: 0.00426% } - val: { MAE: 0.0065 - RMSE: 0.0425 - MAPE: 0.0197% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 14/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00241 - RMSE: 0.0345 - MAPE: 0.00412% } - val: { MAE: 0.00882 - RMSE: 0.0957 - MAPE: 0.027% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 15/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00233 - RMSE: 0.0335 - MAPE: 0.004% } - val: { MAE: 0.00533 - RMSE: 0.0864 - MAPE: 0.0155% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 16/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00255 - RMSE: 0.0307 - MAPE: 0.00427% } - val: { MAE: 0.00727 - RMSE: 0.0774 - MAPE: 0.021% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 17/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00249 - RMSE: 0.0368 - MAPE: 0.0043% } - val: { MAE: 0.00546 - RMSE: 0.0348 - MAPE: 0.0156% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 18/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00227 - RMSE: 0.0292 - MAPE: 0.00392% } - val: { MAE: 0.00531 - RMSE: 0.0427 - MAPE: 0.016% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 19/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00249 - RMSE: 0.0343 - MAPE: 0.00426% } - val: { MAE: 0.00682 - RMSE: 0.0637 - MAPE: 0.0218% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 20/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.0026 - RMSE: 0.0335 - MAPE: 0.00443% } - val: { MAE: 0.00776 - RMSE: 0.0661 - MAPE: 0.022% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 21/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00226 - RMSE: 0.0311 - MAPE: 0.00384% } - val: { MAE: 0.00535 - RMSE: 0.0394 - MAPE: 0.0172% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 22/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00224 - RMSE: 0.0334 - MAPE: 0.00389% } - val: { MAE: 0.00676 - RMSE: 0.0831 - MAPE: 0.0207% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 23/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00286 - RMSE: 0.0379 - MAPE: 0.00492% } - val: { MAE: 0.00482 - RMSE: 0.0288 - MAPE: 0.0148% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 24/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00285 - RMSE: 0.0357 - MAPE: 0.00485% } - val: { MAE: 0.00712 - RMSE: 0.0442 - MAPE: 0.0216% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 25/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00255 - RMSE: 0.035 - MAPE: 0.00443% } - val: { MAE: 0.00502 - RMSE: 0.0475 - MAPE: 0.0156% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 26/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00265 - RMSE: 0.031 - MAPE: 0.00442% } - val: { MAE: 0.00832 - RMSE: 0.0837 - MAPE: 0.0243% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 27/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00301 - RMSE: 0.0321 - MAPE: 0.00503% } - val: { MAE: 0.00624 - RMSE: 0.0484 - MAPE: 0.0199% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 28/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00249 - RMSE: 0.0349 - MAPE: 0.00429% } - val: { MAE: 0.00607 - RMSE: 0.0528 - MAPE: 0.0188% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 29/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00239 - RMSE: 0.0345 - MAPE: 0.00405% } - val: { MAE: 0.00711 - RMSE: 0.0766 - MAPE: 0.0211% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 30/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00254 - RMSE: 0.0328 - MAPE: 0.00436% } - val: { MAE: 0.00737 - RMSE: 0.0569 - MAPE: 0.0221% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 31/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00273 - RMSE: 0.0359 - MAPE: 0.00466% } - val: { MAE: 0.0071 - RMSE: 0.0462 - MAPE: 0.0222% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 32/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00267 - RMSE: 0.032 - MAPE: 0.00458% } - val: { MAE: 0.00671 - RMSE: 0.0491 - MAPE: 0.0215% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 33/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00339 - RMSE: 0.0413 - MAPE: 0.00598% } - val: { MAE: 0.00633 - RMSE: 0.0555 - MAPE: 0.0194% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 34/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00271 - RMSE: 0.0374 - MAPE: 0.0047% } - val: { MAE: 0.0063 - RMSE: 0.0424 - MAPE: 0.0192% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 35/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00251 - RMSE: 0.0288 - MAPE: 0.00432% } - val: { MAE: 0.00612 - RMSE: 0.0496 - MAPE: 0.0183% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 36/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.0025 - RMSE: 0.0319 - MAPE: 0.00425% } - val: { MAE: 0.00718 - RMSE: 0.0565 - MAPE: 0.022% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 37/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00238 - RMSE: 0.0295 - MAPE: 0.004% } - val: { MAE: 0.00701 - RMSE: 0.0481 - MAPE: 0.0222% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 38/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00281 - RMSE: 0.0392 - MAPE: 0.00483% } - val: { MAE: 0.00609 - RMSE: 0.0452 - MAPE: 0.0181% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 39/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00284 - RMSE: 0.0456 - MAPE: 0.00488% } - val: { MAE: 0.00626 - RMSE: 0.0493 - MAPE: 0.0194% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 40/1000\n",
      "[17/17] - 11s - train: { MAE (loss): 0.00255 - RMSE: 0.0341 - MAPE: 0.00433% } - val: { MAE: 0.00542 - RMSE: 0.0351 - MAPE: 0.0176% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 41/1000\n",
      "[5/17] - 3s 697ms/step - train { MAE (loss): 0.0021 - RMSE: 0.026 - MAPE: 0.0035% } - lr: 1e-05 - weight decay: 2e-06                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 30\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model, optimizer, train_loader, val_loader, spatial_temporal_gnn, scaler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     EPOCHS, checkpoint, lr_scheduler, reload_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 30\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m running_train_mape \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m start_time \u001b[39m=\u001b[39m time()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (x, y, event) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# Increment the number of batch steps.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     batch_steps \u001b[39m=\u001b[39m batch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m# Get the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 30\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39m201\u001b[39m) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39m201\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m probs \u001b[39m=\u001b[39m probs[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m [target_size] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m201\u001b[39;49m), \u001b[39m1\u001b[39;49m, p\u001b[39m=\u001b[39;49mprobs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m selected_events_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(target_events)), target_size, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X41sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m selected_events \u001b[39m=\u001b[39m target_events[selected_events_idx]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train(\n",
    "    model, optimizer, train_loader, val_loader, spatial_temporal_gnn, scaler,\n",
    "    EPOCHS, checkpoint, lr_scheduler, reload_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for(i, j) in zip(spatial_temporal_gnn.state_dict(), stgnn_checkpoints['model_state_dict']):\n",
    "#    if i != j: print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
