{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.explanation.navigator.model import Navigator\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the STGNN in evaluation mode.\n",
    "spatial_temporal_gnn.eval();\n",
    "\n",
    "# Get the Navigator and load the checkpoints.\n",
    "navigator = Navigator(DEVICE)\n",
    "\n",
    "navigator_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                          'navigator_metr_la.pth')\n",
    "\n",
    "navigator_checkpoints = torch.load(navigator_checkpoints_path)\n",
    "navigator.load_state_dict(navigator_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the Navigator in evaluation mode.\n",
    "navigator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the data scaler.\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the explained data.\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test.npy'))\n",
    "\n",
    "# Get the time information of the explained data.\n",
    "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test_time.npy'))\n",
    "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(BASE_DATA_DIR, 'structured', 'node_locations.pkl'), 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('773869', ('Ventura Freeway', 14.54715370560186)),\n",
       " ('767541', ('Glendale Freeway', 10.345751960044197)),\n",
       " ('767542', ('Glendale Freeway', 10.327808458197385)),\n",
       " ('717447', ('Hollywood Freeway', 13.112986186569106)),\n",
       " ('717446', ('Hollywood Freeway', 13.329055658812624))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(node_info.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y, sample_x_time, sample_y_time = x_test[0], y_test[0], x_test_time[0], y_test_time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n_timesteps, n_nodes, _ = y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_adjacency_distance_matrix)\n",
    "\n",
    "adj_distance_matrix = get_adjacency_distance_matrix(adj_matrix, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_temporal_distance_matrix)\n",
    "\n",
    "temporal_distance_matrix = get_temporal_distance_matrix(n_nodes, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best parameters based on the results of the grid search.\n",
    "\n",
    "EPS = .35\n",
    "MIN_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.  ],\n",
       "        [62.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[63.  ],\n",
       "        [63.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[63.75],\n",
       "        [59.25],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x[..., 0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2484, 1)\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.clustering.clustering import get_clusters\n",
    "\n",
    "clusters = get_clusters(\n",
    "    sample_x[..., :1],\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    eps=EPS,\n",
    "    min_samples=MIN_SAMPLES,\n",
    "    remove_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(sample_x != 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "\n",
    "print(np.unique(clusters))\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.unique(sample_x[clusters == -2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,\n",
       "         8,  9,  9, 10, 10, 11, 11], dtype=int64),\n",
       " array([ 16, 196,  16, 196,  16, 196,  16, 196,  16, 196,  16, 196,  16,\n",
       "        196,  16, 196,  16, 196,  16, 196,  16, 196,  16, 196], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0], dtype=int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _get_time(date: np.datetime64) -> Tuple[str, str, str]:\n",
    "    days = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',\n",
    "            4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "    Y, M, D, h, m = [date.astype('datetime64[%s]' % kind) for kind in 'YMDhm']\n",
    "\n",
    "    year = Y.astype(int) + 1970\n",
    "    month = M.astype(int) % 12 + 1\n",
    "    day = (D - M).astype(int) + 1\n",
    "    day_of_week = days[((D - M).astype(int) - 1) % 7]\n",
    "    hour = (h - D).astype(int)\n",
    "    minute = (m - h).astype(int)\n",
    "\n",
    "    return day_of_week, f'{day:02d}/{month:02d}/{year}', f'{hour:02d}:{minute:02d}'\n",
    "\n",
    "def _get_cluster_type(\n",
    "    values: np.ndarray,\n",
    "    congestion_max_speed: float = 60,\n",
    "    free_flow_min_speed: float = 110\n",
    "    ) -> str:\n",
    "    if np.all(values) <= congestion_max_speed:\n",
    "        return 'congestion'\n",
    "    elif np.all(values) >= free_flow_min_speed:\n",
    "        return 'free flow'\n",
    "    else:\n",
    "        return 'group of nodes'\n",
    "\n",
    "def _set_cluster_location_info(\n",
    "    knowledge_graph: List[Tuple[str, str, str]],\n",
    "    reference_cluster_name: str,\n",
    "    node_info: Dict[str, Tuple[str, int]],\n",
    "    node_indices: np.ndarray\n",
    "    ) -> None:\n",
    "    # Get the unique node indices.\n",
    "    node_indices = np.unique(node_indices)\n",
    "    # Get the IDs of the nodes by their indices.\n",
    "    node_ids = [ node_pos_dict[idx] for idx in node_indices ]\n",
    "\n",
    "    # Get a dictionary containing the street and kilometrage of each node.\n",
    "    streets = {}\n",
    "    for node_id in node_ids:\n",
    "        # Get the street and kilometrage of the node.\n",
    "        street, km = node_info[node_id]\n",
    "        # Add the street and kilometrage to the dictionary.\n",
    "        if not street in streets.keys():\n",
    "            streets[street] = [km]\n",
    "        else:\n",
    "            streets[street].append(km)\n",
    "\n",
    "    for street, kms in streets.items():\n",
    "        # Add the street and its kilometrages to the knowledge graph.\n",
    "        knowledge_graph.append((reference_cluster_name, 'in highway', street))\n",
    "        for km in kms:\n",
    "            knowledge_graph.append((street, 'at km', f'{km:.2g}'))\n",
    "\n",
    "def _set_cluster_time_info(\n",
    "    knowledge_graph: List[Tuple[str, str, str]],\n",
    "    reference_cluster_name: str,\n",
    "    time_info: np.ndarray,\n",
    "    time_indices: np.ndarray\n",
    "    ) -> None:\n",
    "    # Get the minimum and maximum timestep of the target nodes.\n",
    "    min_timestep, max_timestep = np.min(time_indices), np.max(time_indices)\n",
    "    y_min_time, y_max_time = time_info[min_timestep][0], time_info[max_timestep][0]\n",
    "    beginning_day, beginning_date, beginning_hour = _get_time(y_min_time)\n",
    "    end_day, end_date, end_hour = _get_time(y_max_time)\n",
    "\n",
    "\n",
    "    # Put the date and day information of the target nodes in the\n",
    "    # knowledge graph.\n",
    "    if beginning_date == end_date:\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'on date', beginning_date))\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'on day', beginning_day))\n",
    "    else:\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'from date', beginning_date))\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'to date', end_date))\n",
    "\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'from day', beginning_day))\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'to day', end_day))\n",
    "\n",
    "    # Put the time information of the target nodes in the knowledge graph.\n",
    "    knowledge_graph.append(\n",
    "        (reference_cluster_name, 'from time', beginning_hour))\n",
    "    knowledge_graph.append(\n",
    "        (reference_cluster_name, 'to time', end_hour))\n",
    "\n",
    "def get_knowledge_graph(\n",
    "    x: np.ndarray,\n",
    "    x_times: np.ndarray,\n",
    "    x_clusters: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    y_times: np.ndarray) -> List[Tuple[str, str, str]]:\n",
    "    knowledge_graph = []\n",
    "\n",
    "    # Get the values of the selected target nodes.\n",
    "    target_node_values = y[y > 0]\n",
    "\n",
    "    # Get the type of the target nodes cluster (eg.: congestion, free flow).\n",
    "    target_type = _get_cluster_type(target_node_values)\n",
    "\n",
    "    # Get the name of the target nodes cluster.\n",
    "    target_name = f'target {target_type}' \n",
    "    # Put the average speed of the target nodes cluster in the knowledge graph.\n",
    "    knowledge_graph.append(\n",
    "        (target_name, 'has average speed',\n",
    "         f'{target_node_values.mean():.3g} km/h'))\n",
    "\n",
    "    # Get the indices of the non-null values of the target nodes.\n",
    "    y_indices = np.nonzero(y)\n",
    "\n",
    "    _set_cluster_time_info(knowledge_graph, target_name, y_times, y_indices[0])\n",
    "\n",
    "    _set_cluster_location_info(\n",
    "        knowledge_graph, target_name, node_info, y_indices[1])\n",
    "\n",
    "    for i, c in enumerate([c for c in np.unique(x_clusters) if c != -1]):\n",
    "        # Get the values of the nodes of the cluster.\n",
    "        cluster_node_values = x[x_clusters == c]\n",
    "\n",
    "        # Get the type of the target nodes cluster (eg.: congestion, free flow).\n",
    "        if c == -2:\n",
    "            cluster_type = 'group of nodes'\n",
    "        else:\n",
    "            cluster_type = _get_cluster_type(cluster_node_values)\n",
    "\n",
    "        # Get the name of the target nodes cluster.\n",
    "        cluster_name = f'{cluster_type}{{{i}}}'\n",
    "\n",
    "        # Add the causation information to the knowledge graph.\n",
    "        knowledge_graph.append((target_name, 'caused by', cluster_name))\n",
    "\n",
    "        # Put the average speed of the target nodes cluster in the knowledge graph.\n",
    "        if c != -2:\n",
    "            knowledge_graph.append(\n",
    "                (cluster_name, \n",
    "                'has average speed', \n",
    "                f'{cluster_node_values.mean():.3g} km/h'))\n",
    "\n",
    "        x_indices = np.where(x_clusters == c)\n",
    "        _set_cluster_time_info(knowledge_graph, cluster_name, x_times, x_indices[0])\n",
    "        _set_cluster_location_info(knowledge_graph, cluster_name, node_info, x_indices[1])\n",
    "\n",
    "    print(knowledge_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.352768, 35.112137, 34.866005, 34.5799  , 34.926838, 34.584217,\n",
       "       35.600292, 35.142715, 34.550823, 34.035618, 32.767227, 32.132698,\n",
       "       33.677856, 33.246357, 33.76597 , 33.834396, 33.604694, 34.004375,\n",
       "       33.706665, 33.565376, 34.097023, 33.589268, 33.505253, 33.066093],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_y[sample_y != 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('target congestion', 'has average speed', '34.1 km/h'), ('target congestion', 'on date', '04/06/2012'), ('target congestion', 'on day', 'Wednesday'), ('target congestion', 'from time', '05:45'), ('target congestion', 'to time', '06:40'), ('target congestion', 'in highway', 'Arroyo Seco Parkway'), ('Arroyo Seco Parkway', 'at km', '1.8'), ('Arroyo Seco Parkway', 'at km', '2.5'), ('target congestion', 'caused by', 'group of nodes{0}'), ('group of nodes{0}', 'on date', '04/06/2012'), ('group of nodes{0}', 'on day', 'Wednesday'), ('group of nodes{0}', 'from time', '04:45'), ('group of nodes{0}', 'to time', '05:40'), ('group of nodes{0}', 'in highway', 'Ventura Freeway'), ('Ventura Freeway', 'at km', '15'), ('group of nodes{0}', 'in highway', 'Glendale Freeway'), ('Glendale Freeway', 'at km', '10'), ('Glendale Freeway', 'at km', '8.2'), ('group of nodes{0}', 'in highway', 'Hollywood Freeway'), ('Hollywood Freeway', 'at km', '14'), ('Hollywood Freeway', 'at km', '1.6'), ('Hollywood Freeway', 'at km', '3.8'), ('group of nodes{0}', 'in highway', 'San Diego Freeway'), ('San Diego Freeway', 'at km', '10'), ('San Diego Freeway', 'at km', '12'), ('group of nodes{0}', 'in highway', 'Foothill Freeway'), ('Foothill Freeway', 'at km', '29'), ('group of nodes{0}', 'in highway', 'Golden State Freeway'), ('Golden State Freeway', 'at km', '8.6'), ('target congestion', 'caused by', 'congestion{1}'), ('congestion{1}', 'has average speed', '56.1 km/h'), ('congestion{1}', 'on date', '04/06/2012'), ('congestion{1}', 'on day', 'Wednesday'), ('congestion{1}', 'from time', '05:10'), ('congestion{1}', 'to time', '05:40'), ('congestion{1}', 'in highway', 'Hollywood Freeway'), ('Hollywood Freeway', 'at km', '13'), ('Hollywood Freeway', 'at km', '14'), ('target congestion', 'caused by', 'congestion{2}'), ('congestion{2}', 'has average speed', '48.8 km/h'), ('congestion{2}', 'on date', '04/06/2012'), ('congestion{2}', 'on day', 'Wednesday'), ('congestion{2}', 'from time', '05:20'), ('congestion{2}', 'to time', '05:40'), ('congestion{2}', 'in highway', 'Hollywood Freeway'), ('Hollywood Freeway', 'at km', '1.6'), ('target congestion', 'caused by', 'congestion{3}'), ('congestion{3}', 'has average speed', '47.4 km/h'), ('congestion{3}', 'on date', '04/06/2012'), ('congestion{3}', 'on day', 'Wednesday'), ('congestion{3}', 'from time', '05:20'), ('congestion{3}', 'to time', '05:40'), ('congestion{3}', 'in highway', 'Arroyo Seco Parkway'), ('Arroyo Seco Parkway', 'at km', '3.5')]\n"
     ]
    }
   ],
   "source": [
    "get_knowledge_graph(sample_x, sample_x_time, clusters, sample_y, sample_y_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec54067b2497434eb6441a2e8ec4641c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\riccardo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9017e190d80e4dadb6013c389c2aa09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fe40d183254cae9dea142e4e6cc42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb932c00c8e942d49b15c0bcc6d4cf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7147e7bc0d47e1a7a2ead0aad4dde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72825bf9fe654be7ba6d6bc5c3438dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)main/modelling_RW.py:   0%|          | 0.00/47.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- modelling_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ca69d8047c4d7aa67b381db06e3800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e86b99240604e91af94426b9ebafeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca0de4a1604438abec8f46bd93a50fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da9eddbb25a45b4ab153e830a1c48e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\riccardo\\AppData\\Local\\Temp\\ipykernel_25200\\4134692782.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\riccardo\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_25200\\\\4134692782.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">es\\__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">788</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pipeline</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">785 │   # Forced if framework already defined, inferred if it's None</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">786 │   # Will load the correct model if possible</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">787 │   </span>model_classes = {<span style=\"color: #808000; text-decoration-color: #808000\">\"tf\"</span>: targeted_task[<span style=\"color: #808000; text-decoration-color: #808000\">\"tf\"</span>], <span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>: targeted_task[<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>]}                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>788 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>framework, model = infer_framework_load_model(                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">789 │   │   </span>model,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">790 │   │   </span>model_classes=model_classes,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">791 │   │   </span>config=config,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">es\\base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">279</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">infer_framework_load_model</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 276 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">continue</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 277 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 278 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(model, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 279 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Could not load model {</span>model<span style=\"color: #808000; text-decoration-color: #808000\">} with any of the following cl</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 280 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 281 │   </span>framework = <span style=\"color: #808000; text-decoration-color: #808000\">\"tf\"</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"keras.engine.training.Model\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(inspect.getmro(model.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__clas</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 282 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> framework, model                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Could not load model tiiuae/falcon-7b with any of the following classes: <span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.auto.modeling_auto.AutoModelForCausalLM'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'</span><span style=\"font-weight: bold\">&gt;)</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\riccardo\\AppData\\Local\\Temp\\ipykernel_25200\\4134692782.py\u001b[0m:\u001b[94m8\u001b[0m in \u001b[92m<module>\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\riccardo\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_25200\\\\4134692782.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mes\\__init__.py\u001b[0m:\u001b[94m788\u001b[0m in \u001b[92mpipeline\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m785 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Forced if framework already defined, inferred if it's None\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m786 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Will load the correct model if possible\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m787 \u001b[0m\u001b[2m│   \u001b[0mmodel_classes = {\u001b[33m\"\u001b[0m\u001b[33mtf\u001b[0m\u001b[33m\"\u001b[0m: targeted_task[\u001b[33m\"\u001b[0m\u001b[33mtf\u001b[0m\u001b[33m\"\u001b[0m], \u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m: targeted_task[\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m]}                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m788 \u001b[2m│   \u001b[0mframework, model = infer_framework_load_model(                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m789 \u001b[0m\u001b[2m│   │   \u001b[0mmodel,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m790 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_classes=model_classes,                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m791 \u001b[0m\u001b[2m│   │   \u001b[0mconfig=config,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mes\\base.py\u001b[0m:\u001b[94m279\u001b[0m in \u001b[92minfer_framework_load_model\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 276 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mcontinue\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 277 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 278 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(model, \u001b[96mstr\u001b[0m):                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 279 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCould not load model \u001b[0m\u001b[33m{\u001b[0mmodel\u001b[33m}\u001b[0m\u001b[33m with any of the following cl\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 280 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 281 \u001b[0m\u001b[2m│   \u001b[0mframework = \u001b[33m\"\u001b[0m\u001b[33mtf\u001b[0m\u001b[33m\"\u001b[0m \u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mkeras.engine.training.Model\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m \u001b[96mstr\u001b[0m(inspect.getmro(model.\u001b[91m__clas\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 282 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m framework, model                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mCould not load model tiiuae/falcon-7b with any of the following classes: \u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'transformers.models.auto.modeling_auto.AutoModelForCausalLM'\u001b[0m\u001b[1m>\u001b[0m, \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   '''\n",
    "   Translate this tuple set representing a knowledge graph into a textual output. Note, numbers between curly brackets are used to aid you to identify the entities of the graph, although they shouldn\\'t be considered in the textual output:\n",
    "   \"\n",
    "   (target congestion, from, 10:00)\n",
    "   (target_congestion, to, 11:00)\n",
    "   (target_congestion, caused by, congestion{0})\n",
    "   (congestion{0}, from, 09:00)\n",
    "   (congestion{0}, to, 10:00)\n",
    "   (target_congestion, caused by, congestion{1})\n",
    "   (congestion{1}, from, 07:00)\n",
    "   (congestion{1}, to, 08:00)\n",
    "   \"\n",
    "   ''',\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print({seq['generated_text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (1000) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = '''Translate this tuple set representing a knowledge graph into a textual output. Note, numbers between curly brackets are used to aid you to identify the entities of the graph, although they shouldn\\'t be considered in the textual output:\n",
    "\"\n",
    "(target congestion, from, 10:00)\n",
    "(target_congestion, to, 11:00)\n",
    "(target_congestion, caused by, congestion{0})\n",
    "(congestion{0}, from, 09:00)\n",
    "(congestion{0}, to, 10:00)\n",
    "(target_congestion, caused by, congestion{1})\n",
    "(congestion{1}, from, 07:00)\n",
    "(congestion{1}, to, 08:00)\n",
    "\"'''\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model.generate(**encoded_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate this tuple set representing a knowledge graph into a textual output. Note, numbers between curly brackets are used to aid you to identify the entities of the graph, although they shouldn't be considered in the textual output:\n",
    "\n",
    "(target congestion, from, 10:00)\n",
    "(target_congestion, to, 11:00)\n",
    "(target_congestion, caused by, congestion{0})\n",
    "(congestion{0}, from, 09:00)\n",
    "(congestion{0}, to, 10:00)\n",
    "(target_congestion, caused by, congestion{1})\n",
    "(congestion{1}, from, 07:00)\n",
    "(congestion{1}, to, 08:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate this tuple set representing a knowledge graph into a textual output. Note, numbers between curly brackets are used to aid you to identify the entities of the graph, although they shouldn\\'t be considered in the textual output:\\n\"\\n(target congestion, from, 10:00)\\n(target_congestion, to, 11:00)\\n(target_congestion, caused by, congestion{0})\\n(congestion{0}, from, 09:00)\\n(congestion{0}, to, 10:00)\\n(target_congestion, caused by, congestion{1})\\n(congestion{1}, from, 07:00)\\n(congestion{1}, to, 08:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{1}, caused by, congestion{2})\\n(congestion{2}, from, 09:00)\\n(congestion{2}, caused by, congestion{3})\\n(congestion{3}, from, 07:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{3}, caused by, congestion{4})\\n(congestion{4}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{4}, caused by, congestion{5})\\n(congestion{5}, from, 07:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{5}, caused by, congestion{6})\\n(congestion{6}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{6}, caused by, congestion{7})\\n(congestion{7}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{7}, caused by, congestion{8})\\n(congestion{8}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{8}, caused by, congestion{9})\\n(congestion{9}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{9}, caused by, congestion{10})\\n(congestion{10}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{10}, caused by, congestion{11})\\n(congestion{11}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{11}, caused by, congestion{12})\\n(congestion{12}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{12}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{12}, caused by, congestion{13})\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n\\n(target_congestion, from, 10:00)\\n\\n(congestion{13}, from, 09:00)\\n\\n\"\\n\\n(target_congestion, from, 10:00)\\n\\n(congestion{13}, from, 09:00)\\n\\n\"\\n\\n(target_congestion, from,'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
