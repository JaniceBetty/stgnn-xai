{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6kbAULJKy-wu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Set the main path in the root folder of the project.\n",
        "sys.path.append(os.path.join('..'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HGIZtEY6y-wv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Settings for autoreloading.\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "obBqbIuGy-wv"
      },
      "outputs": [],
      "source": [
        "from src.utils.seed import set_random_seed\n",
        "\n",
        "# Set the random seed for deterministic operations.\n",
        "SEED = 42\n",
        "set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHGuhgDmy-wv",
        "outputId": "f72a962b-a8e4-4741-c313-28fb904159be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The selected device is: \"cuda\"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set the device for training and querying the model.\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'The selected device is: \"{DEVICE}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhVgau7-y-ww"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HlTWno-1y-wx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE_DATA_DIR = os.path.join('..', 'data', 'pems-bay')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BbiHBGWSy-wx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
        "    scaler = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nb6d7kVZy-wx"
      },
      "outputs": [],
      "source": [
        "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
        "from src.data.data_extraction import get_adjacency_matrix\n",
        "\n",
        "# Get the adjacency matrix\n",
        "adj_matrix_structure = get_adjacency_matrix(\n",
        "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_pems_bay.pkl'))\n",
        "\n",
        "# Get the header of the adjacency matrix, the node indices and the\n",
        "# matrix itself.\n",
        "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
        "\n",
        "# Get the STGNN and load the checkpoints.\n",
        "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
        "\n",
        "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
        "                                      'st_gnn_pems_bay.pth')\n",
        "\n",
        "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
        "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
        "\n",
        "# Set the STGNN in evaluation mode.\n",
        "spatial_temporal_gnn.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YIB5Kn57y-wx"
      },
      "outputs": [],
      "source": [
        "from src.data.data_extraction import get_locations_dataframe\n",
        "\n",
        "# Get the dataframe containing the latitude and longitude of each sensor.\n",
        "locations_df = get_locations_dataframe(\n",
        "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_pems_bay.csv'),\n",
        "    has_header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kwVU6H96y-wy"
      },
      "outputs": [],
      "source": [
        "# Get the node positions dictionary.\n",
        "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x-lj4muty-wy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Get the data scaler.\n",
        "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
        "    scaler = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kNItiWwGy-wy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Get the data and the values predicted by the STGNN.\n",
        "x_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train.npy'))\n",
        "y_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train.npy'))\n",
        "x_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train_time.npy'))\n",
        "y_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train_time.npy'))\n",
        "\n",
        "x_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val.npy'))\n",
        "y_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val.npy'))\n",
        "x_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val_time.npy'))\n",
        "y_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val_time.npy'))\n",
        "\n",
        "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test.npy'))\n",
        "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test.npy'))\n",
        "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test_time.npy'))\n",
        "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test_time.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bCEWyqCvy-wz"
      },
      "outputs": [],
      "source": [
        "from src.data.data_processing import get_distance_matrix\n",
        "\n",
        "if not os.path.exists(\n",
        "    os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy')):\n",
        "    # Build the distance matrix between the nodes.\n",
        "    distance_matrix = get_distance_matrix(\n",
        "        adj_matrix,\n",
        "        locations_df,\n",
        "        node_pos_dict)\n",
        "\n",
        "    # Save the distance matrix.\n",
        "    np.save(os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'),\n",
        "            distance_matrix)\n",
        "\n",
        "else:\n",
        "    # Load the distance matrix.\n",
        "    distance_matrix = np.load(\n",
        "        os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y70iKYfv2P9r",
        "outputId": "22baca3d-8c29-4999-9e6b-1d722b37c7dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 30 remove_value: perturb\n",
            "[100/100] - 586s - MAE: { severe_congestion 3.98 -congestion 2.02 -free_flow 0.849 - total: 2.05 } - RMSE: { severe_congestion 4.45 -congestion 2.28 -free_flow 0.968 - total: 2.33 } - MAPE: { severe_congestion 16.9% -congestion 4.03% -free_flow 1.27% - total: 6.81% } - Average time: 5.86s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 582s - MAE: { severe_congestion 11.3 -congestion 2.7 -free_flow 1.69 - total: 5.24 } - RMSE: { severe_congestion 11.6 -congestion 3.17 -free_flow 1.91 - total: 5.57 } - MAPE: { severe_congestion 56.5% -congestion 5.53% -free_flow 2.51% - total: 21.7% } - Average time: 5.82s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 50 remove_value: perturb\n",
            "[100/100] - 937s - MAE: { severe_congestion 3.31 -congestion 1.78 -free_flow 0.836 - total: 1.94 } - RMSE: { severe_congestion 3.66 -congestion 2.13 -free_flow 1.02 - total: 2.24 } - MAPE: { severe_congestion 14.3% -congestion 3.73% -free_flow 1.25% - total: 6.41% } - Average time: 9.37s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 975s - MAE: { severe_congestion 10.8 -congestion 2.53 -free_flow 1.61 - total: 4.97 } - RMSE: { severe_congestion 11.1 -congestion 2.93 -free_flow 1.8 - total: 5.27 } - MAPE: { severe_congestion 54.2% -congestion 5.17% -free_flow 2.38% - total: 20.8% } - Average time: 9.75s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 30 remove_value: perturb\n",
            "[100/100] - 592s - MAE: { severe_congestion 3.69 -congestion 1.63 -free_flow 0.69 - total: 1.87 } - RMSE: { severe_congestion 4.11 -congestion 1.76 -free_flow 0.795 - total: 2.09 } - MAPE: { severe_congestion 15.3% -congestion 3.36% -free_flow 1.04% - total: 6.21% } - Average time: 5.92s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 583s - MAE: { severe_congestion 11.2 -congestion 2.73 -free_flow 1.71 - total: 5.2 } - RMSE: { severe_congestion 11.5 -congestion 3.14 -free_flow 1.94 - total: 5.5 } - MAPE: { severe_congestion 55.8% -congestion 5.55% -free_flow 2.53% - total: 21.5% } - Average time: 5.83s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 50 remove_value: perturb\n",
            "[100/100] - 950s - MAE: { severe_congestion 4.1 -congestion 1.91 -free_flow 0.768 - total: 2.13 } - RMSE: { severe_congestion 4.58 -congestion 2.23 -free_flow 0.863 - total: 2.43 } - MAPE: { severe_congestion 17.9% -congestion 4.04% -free_flow 1.16% - total: 7.37% } - Average time: 9.5s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 967s - MAE: { severe_congestion 10.8 -congestion 2.23 -free_flow 1.6 - total: 4.89 } - RMSE: { severe_congestion 11.1 -congestion 2.67 -free_flow 1.81 - total: 5.19 } - MAPE: { severe_congestion 54.3% -congestion 4.5% -free_flow 2.36% - total: 20.6% } - Average time: 9.67s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 30 remove_value: perturb\n",
            "[100/100] - 579s - MAE: { severe_congestion 3.31 -congestion 1.79 -free_flow 0.796 - total: 1.86 } - RMSE: { severe_congestion 3.71 -congestion 2.08 -free_flow 0.953 - total: 2.13 } - MAPE: { severe_congestion 14.1% -congestion 3.54% -free_flow 1.2% - total: 6.07% } - Average time: 5.79s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 577s - MAE: { severe_congestion 11 -congestion 2.67 -free_flow 1.81 - total: 5.15 } - RMSE: { severe_congestion 11.3 -congestion 3.17 -free_flow 2.07 - total: 5.48 } - MAPE: { severe_congestion 55.1% -congestion 5.46% -free_flow 2.67% - total: 21.3% } - Average time: 5.77s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 50 remove_value: perturb\n",
            "[100/100] - 949s - MAE: { severe_congestion 4.09 -congestion 1.34 -free_flow 0.897 - total: 2.08 } - RMSE: { severe_congestion 4.67 -congestion 1.55 -free_flow 1 - total: 2.38 } - MAPE: { severe_congestion 18.4% -congestion 2.72% -free_flow 1.35% - total: 7.49% } - Average time: 9.49s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 959s - MAE: { severe_congestion 10.7 -congestion 2.39 -free_flow 1.74 - total: 4.95 } - RMSE: { severe_congestion 11 -congestion 2.88 -free_flow 1.95 - total: 5.26 } - MAPE: { severe_congestion 54.1% -congestion 4.89% -free_flow 2.58% - total: 20.7% } - Average time: 9.59s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 30 remove_value: perturb\n",
            "[100/100] - 770s - MAE: { severe_congestion 3.87 -congestion 1.59 -free_flow 0.837 - total: 2.03 } - RMSE: { severe_congestion 4.47 -congestion 1.84 -free_flow 0.963 - total: 2.36 } - MAPE: { severe_congestion 18% -congestion 3.12% -free_flow 1.26% - total: 7.36% } - Average time: 7.7s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 753s - MAE: { severe_congestion 11.4 -congestion 2.61 -free_flow 1.83 - total: 5.27 } - RMSE: { severe_congestion 11.7 -congestion 3.14 -free_flow 2.08 - total: 5.62 } - MAPE: { severe_congestion 56.7% -congestion 5.31% -free_flow 2.72% - total: 21.7% } - Average time: 7.53s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 50 remove_value: perturb\n",
            "[100/100] - 1274s - MAE: { severe_congestion 4.12 -congestion 1.77 -free_flow 0.933 - total: 2.21 } - RMSE: { severe_congestion 4.61 -congestion 2.01 -free_flow 1.07 - total: 2.5 } - MAPE: { severe_congestion 17.2% -congestion 3.6% -free_flow 1.41% - total: 7.27% } - Average time: 12.7s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1264s - MAE: { severe_congestion 10.9 -congestion 2.08 -free_flow 1.65 - total: 4.87 } - RMSE: { severe_congestion 11.2 -congestion 2.55 -free_flow 1.87 - total: 5.19 } - MAPE: { severe_congestion 54.3% -congestion 4.19% -free_flow 2.44% - total: 20.5% } - Average time: 12.6s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 30 remove_value: perturb\n",
            "[100/100] - 795s - MAE: { severe_congestion 4.46 -congestion 1.98 -free_flow 1.1 - total: 2.47 } - RMSE: { severe_congestion 5.09 -congestion 2.27 -free_flow 1.25 - total: 2.82 } - MAPE: { severe_congestion 20.8% -congestion 4.13% -free_flow 1.67% - total: 8.8% } - Average time: 7.95s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 801s - MAE: { severe_congestion 10.7 -congestion 2.81 -free_flow 1.75 - total: 5.06 } - RMSE: { severe_congestion 11 -congestion 3.39 -free_flow 2 - total: 5.42 } - MAPE: { severe_congestion 53.4% -congestion 5.76% -free_flow 2.58% - total: 20.7% } - Average time: 8.01s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 50 remove_value: perturb\n",
            "[100/100] - 1304s - MAE: { severe_congestion 3.83 -congestion 2.41 -free_flow 0.89 - total: 2.35 } - RMSE: { severe_congestion 4.32 -congestion 2.69 -free_flow 1.02 - total: 2.65 } - MAPE: { severe_congestion 17.1% -congestion 5% -free_flow 1.35% - total: 7.8% } - Average time: 13s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1289s - MAE: { severe_congestion 10.7 -congestion 2.42 -free_flow 1.66 - total: 4.94 } - RMSE: { severe_congestion 11.1 -congestion 2.84 -free_flow 1.87 - total: 5.24 } - MAPE: { severe_congestion 53.7% -congestion 4.93% -free_flow 2.46% - total: 20.6% } - Average time: 12.9s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 30 remove_value: perturb\n",
            "[100/100] - 769s - MAE: { severe_congestion 4.11 -congestion 1.82 -free_flow 0.699 - total: 2.19 } - RMSE: { severe_congestion 4.62 -congestion 2.14 -free_flow 0.804 - total: 2.5 } - MAPE: { severe_congestion 18.6% -congestion 3.59% -free_flow 1.04% - total: 7.78% } - Average time: 7.69s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 30 remove_value: 0.0\n",
            "[100/100] - 742s - MAE: { severe_congestion 11.3 -congestion 2.62 -free_flow 1.75 - total: 5.21 } - RMSE: { severe_congestion 11.6 -congestion 3.07 -free_flow 2.02 - total: 5.55 } - MAPE: { severe_congestion 55.9% -congestion 5.39% -free_flow 2.59% - total: 21.5% } - Average time: 7.42s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 50 remove_value: perturb\n",
            "[100/100] - 1234s - MAE: { severe_congestion 5.17 -congestion 1.84 -free_flow 0.806 - total: 2.53 } - RMSE: { severe_congestion 5.69 -congestion 2.04 -free_flow 0.933 - total: 2.81 } - MAPE: { severe_congestion 23.1% -congestion 3.63% -free_flow 1.21% - total: 9.19% } - Average time: 12.3s \n",
            "\n",
            "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 50 remove_value: 0.0\n",
            "[100/100] - 1247s - MAE: { severe_congestion 10.7 -congestion 2.22 -free_flow 1.61 - total: 4.84 } - RMSE: { severe_congestion 10.9 -congestion 2.66 -free_flow 1.83 - total: 5.14 } - MAPE: { severe_congestion 53.7% -congestion 4.51% -free_flow 2.39% - total: 20.4% } - Average time: 12.5s \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import apply_grid_search\n",
        "\n",
        "apply_grid_search(\n",
        "    x_train[::10],\n",
        "    y_train[::10],\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts_list=[30, 50],\n",
        "    explanation_size_factor_list=[2, 3],\n",
        "    cut_size_factor_list=[2],\n",
        "    exploration_weight_list=[5, 10, 20],\n",
        "    remove_value_list=['perturb', 0.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qVtOLnQ68GDf"
      },
      "outputs": [],
      "source": [
        "CUT_SIZE_FACTOR = 2\n",
        "EXPLANATION_SIZE_FACTOR = 2\n",
        "EXPLORATION_WEIGHT = 20\n",
        "N_ROLLOUTS = 30\n",
        "REMOVE_VALUE = 'perturb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3FYN5mEu8GAl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "EXPLAINED_DATA_DIR = os.path.join(BASE_DATA_DIR, 'explained')\n",
        "os.makedirs(EXPLAINED_DATA_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdB__0mW8F-E",
        "outputId": "0fa3bb15-0240-407c-98f2-f8d51ec5e908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the training set...\n",
            "[999/999] - 6378s - MAE: { severe_congestion 3.83 -congestion 1.92 -free_flow 0.804 - total: 2.18 } - RMSE: { severe_congestion 4.34 -congestion 2.14 -free_flow 0.93 - total: 2.46 } - MAPE: { severe_congestion 16.7% -congestion 3.88% -free_flow 1.22% - total: 7.24% } - Average time: 6.38s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the training set...')\n",
        "x_train_explained, y_train_explained, train_scores = get_all_explanations(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=REMOVE_VALUE,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train.npy'), x_train_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train.npy'), y_train_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'train_scores.npy'), train_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train_time.npy'), x_train_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train_time.npy'), y_train_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdIyprvn8F7d",
        "outputId": "63cee00d-cb6e-4714-ad78-6c0660bce7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the validation set...\n",
            "[198/198] - 1314s - MAE: { severe_congestion 2.95 -congestion 2.21 -free_flow 0.77 - total: 1.93 } - RMSE: { severe_congestion 3.32 -congestion 2.5 -free_flow 0.888 - total: 2.18 } - MAPE: { severe_congestion 12.4% -congestion 4.83% -free_flow 1.18% - total: 5.99% } - Average time: 6.64s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the validation set...')\n",
        "x_val_explained, y_val_explained, val_scores = get_all_explanations(\n",
        "    x_val,\n",
        "    y_val,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=REMOVE_VALUE,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val.npy'), x_val_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val.npy'), y_val_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'val_scores.npy'), val_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val_time.npy'), x_val_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val_time.npy'), y_val_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-U5tti38F4V",
        "outputId": "268c15e4-a8ed-40fe-d5d7-d2275beefa11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the explanations for the test set...\n",
            "[300/300] - 1920s - MAE: { severe_congestion 4.05 -congestion 2.22 -free_flow 0.725 - total: 2.32 } - RMSE: { severe_congestion 4.41 -congestion 2.45 -free_flow 0.837 - total: 2.55 } - MAPE: { severe_congestion 16.8% -congestion 4.64% -free_flow 1.1% - total: 7.47% } - Average time: 6.4s \n"
          ]
        }
      ],
      "source": [
        "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
        "\n",
        "\n",
        "print('Computing the explanations for the test set...')\n",
        "x_test_explained, y_test_explained, test_scores = get_all_explanations(\n",
        "    x_test,\n",
        "    y_test,\n",
        "    distance_matrix,\n",
        "    spatial_temporal_gnn,\n",
        "    scaler,\n",
        "    n_rollouts=N_ROLLOUTS,\n",
        "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
        "    cut_size_factor=CUT_SIZE_FACTOR,\n",
        "    exploration_weight=EXPLORATION_WEIGHT,\n",
        "    remove_value=REMOVE_VALUE,\n",
        "    divide_by_traffic_cluster_kind=True)\n",
        "\n",
        "# Save the explained data.\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test.npy'), x_test_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test.npy'), y_test_explained)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'test_scores.npy'), test_scores)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test_time.npy'), x_test_time)\n",
        "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test_time.npy'), y_test_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTkhNMiXH7_n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Get the data and the values predicted by the STGNN.\n",
        "x_train_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_train.npy'))\n",
        "\n",
        "x_val_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_val.npy'))\n",
        "\n",
        "x_test_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.explanation.events import get_largest_event_set\n",
        "from src.explanation.monte_carlo.explanation import evaluate\n",
        "from src.spatial_temporal_gnn.metrics import MAE, RMSE, MAPE\n",
        "\n",
        "\n",
        "def print_all_fidelity_plus(\n",
        "    x,\n",
        "    y,\n",
        "    x_explained,\n",
        "    spatial_temporal_gnn,\n",
        "    remove_value=0.,\n",
        "    divide_by_traffic_cluster_kind: bool = True):\n",
        "    mae_criterion = MAE()\n",
        "    rmse_criterion = RMSE()\n",
        "    mape_criterion = MAPE()\n",
        "    \n",
        "    if divide_by_traffic_cluster_kind:\n",
        "        # Get the severe congestion sparsity, the firs third of the data.\n",
        "        severe_congestion_mae, severe_congestion_rmse, severe_congestion_mape = get_fidelity_plus(\n",
        "            x[:len(x) // 3], \n",
        "            y[:len(x) // 3],\n",
        "            x_explained[:len(x) // 3],  \n",
        "            spatial_temporal_gnn, \n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        # Get the congestion sparsity, the second third of the data.\n",
        "        congestion_mae, congestion_rmse, congestion_mape = get_fidelity_plus(\n",
        "            x[len(x) // 3:2 * len(x) // 3],\n",
        "            y[len(x) // 3:2 * len(x) // 3],\n",
        "            x_explained[len(x) // 3:2 * len(x) // 3],\n",
        "            spatial_temporal_gnn, \n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        # Get the free flow sparsity, the last third of the data.\n",
        "        free_flow_mae, free_flow_rmse, free_flow_mape = get_fidelity_plus(\n",
        "            x[2 * len(x) // 3:],\n",
        "            y[2 * len(x) // 3:],\n",
        "            x_explained[2 * len(x) // 3:],\n",
        "            spatial_temporal_gnn, \n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        # Compute the average sparsity.\n",
        "        mae = (severe_congestion_mae + congestion_mae +\n",
        "                    free_flow_mae) / 3\n",
        "        rmse = (severe_congestion_rmse + congestion_rmse +\n",
        "                    free_flow_rmse) / 3\n",
        "        mape = (severe_congestion_mape + congestion_mape +\n",
        "                    free_flow_mape) / 3\n",
        "        print(\n",
        "            f'MAE+: {{ severe_congestion {severe_congestion_mae:.3g} -'\n",
        "            f'congestion {congestion_mae:.3g} -'\n",
        "            f'free_flow {free_flow_mae:.3g} -',\n",
        "            f'total: {mae:.3g}% }} -'\n",
        "            f'RMSE+: {{ severe_congestion {severe_congestion_rmse:.3g} -'\n",
        "            f'congestion {congestion_rmse:.3g} -'\n",
        "            f'free_flow {free_flow_rmse:.3g} -',\n",
        "            f'total: {rmse:.3g}% }} -'\n",
        "            f'MAPE+: {{ severe_congestion {severe_congestion_mape * 100:.3g}% -'\n",
        "            f'congestion {congestion_mape * 100.:.3g}% -'\n",
        "            f'free_flow {free_flow_mape * 100.:.3g}% -',\n",
        "            f'total: {mape * 100.:.3g}% }}')\n",
        "    else:\n",
        "        print(\n",
        "            f'MAE: {mae:.3g} -',\n",
        "            f'RMSE: {rmse:.3g} -',\n",
        "            f'MAPE: {mape * 100.:.3g}%')\n",
        "        \n",
        "def get_fidelity_plus(\n",
        "    x, \n",
        "    y, \n",
        "    x_explained, \n",
        "    spatial_temporal_gnn, \n",
        "    mae_criterion,\n",
        "    rmse_criterion,\n",
        "    mape_criterion,\n",
        "    remove_value):\n",
        "    x_ = x.copy()\n",
        "    # Get the events of the complement of x_explained.\n",
        "    x_[x_explained != 0.] = 0.\n",
        "    running_mae = 0.\n",
        "    running_rmse = 0.\n",
        "    running_mape = 0.\n",
        "    for i in range(len(x_)):\n",
        "        input_events = get_largest_event_set(x_[i])\n",
        "    \n",
        "        # Evaluate the results.\n",
        "        mae, rmse, mape = evaluate(\n",
        "            x_[i],\n",
        "            y[i],\n",
        "            input_events,\n",
        "            spatial_temporal_gnn,\n",
        "            scaler,\n",
        "            mae_criterion,\n",
        "            rmse_criterion,\n",
        "            mape_criterion,\n",
        "            remove_value)\n",
        "        running_mae += mae\n",
        "        running_rmse += rmse\n",
        "        running_mape += mape\n",
        "\n",
        "    return running_mae / len(x_), running_rmse / len(x_), running_mape / len(x_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the fidelity+ for the training set...\n",
            "MAE+: { severe_congestion 9.94 -congestion 28.2 -free_flow 26.3 - total: 21.5% } -RMSE+: { severe_congestion 11 -congestion 29.5 -free_flow 28.9 - total: 23.1% } -MAPE+: { severe_congestion 39.4% -congestion 53% -free_flow 40.3% - total: 44.2% }\n",
            "\n",
            "Computing the fidelity+ for the validation set...\n",
            "MAE+: { severe_congestion 9.65 -congestion 25.7 -free_flow 24.8 - total: 20% } -RMSE+: { severe_congestion 10.6 -congestion 26.9 -free_flow 28 - total: 21.8% } -MAPE+: { severe_congestion 37.8% -congestion 48.8% -free_flow 38% - total: 41.5% }\n",
            "\n",
            "Computing the fidelity+ for the test set...\n",
            "MAE+: { severe_congestion 10.7 -congestion 27.3 -free_flow 27.9 - total: 22% } -RMSE+: { severe_congestion 11.7 -congestion 28.5 -free_flow 30.8 - total: 23.7% } -MAPE+: { severe_congestion 41.2% -congestion 51.6% -free_flow 42.6% - total: 45.1% }\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Computing the fidelity+ for the training set...')\n",
        "print_all_fidelity_plus(x_train, y_train, x_train_explained, spatial_temporal_gnn, remove_value=REMOVE_VALUE)\n",
        "print()\n",
        "\n",
        "print('Computing the fidelity+ for the validation set...')\n",
        "print_all_fidelity_plus(x_val, y_val, x_val_explained, spatial_temporal_gnn, remove_value=REMOVE_VALUE)\n",
        "print()\n",
        "\n",
        "print('Computing the fidelity+ for the test set...')\n",
        "print_all_fidelity_plus(x_test, y_test, x_test_explained, spatial_temporal_gnn, remove_value=REMOVE_VALUE)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_all_sparsity(\n",
        "    x,\n",
        "    x_explained,\n",
        "    divide_by_traffic_cluster_kind: bool = True):\n",
        "    if divide_by_traffic_cluster_kind:\n",
        "        # Get the severe congestion sparsity, the firs third of the data.\n",
        "        severe_congestion_sparsity = get_sparsity(\n",
        "            x[:len(x) // 3], x_explained[:len(x) // 3])\n",
        "        # Get the congestion sparsity, the second third of the data.\n",
        "        congestion_sparsity = get_sparsity(\n",
        "            x[len(x) // 3:2 * len(x) // 3],\n",
        "            x_explained[len(x) // 3:2 * len(x) // 3])\n",
        "        # Get the free flow sparsity, the last third of the data.\n",
        "        free_flow_sparsity = get_sparsity(\n",
        "            x[2 * len(x) // 3:], x_explained[2 * len(x) // 3:])\n",
        "        # Compute the average sparsity.\n",
        "        sparsity = (severe_congestion_sparsity + congestion_sparsity +\n",
        "                    free_flow_sparsity) / 3\n",
        "        print(\n",
        "            f'Sparsity: {{ severe_congestion {severe_congestion_sparsity:.3g} -'\n",
        "            f'congestion {congestion_sparsity:.3g} -'\n",
        "            f'free_flow {free_flow_sparsity:.3g} -',\n",
        "            f'total: {sparsity:.3g} }}')\n",
        "    else:\n",
        "        print(f'Sparsity: {sparsity:.3g}')\n",
        "\n",
        "\n",
        "def get_sparsity(x, x_explained):\n",
        "    # Count the number of non-zero values in the original data, by time step in the last axis.\n",
        "    x_non_zero = np.count_nonzero(x[..., 0], axis=0)\n",
        "    # Count the number of non-zero values in the explained data, by time step.\n",
        "    x_explained_non_zero = np.count_nonzero(x_explained[..., 0], axis=0)\n",
        "    # Compute the sparsity, by time step.\n",
        "    sparsity = 1 - x_explained_non_zero / x_non_zero\n",
        "    # Compute the average sparsity.\n",
        "    return np.mean(sparsity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the sparsity for the training set...\n",
            "Sparsity: { severe_congestion 0.989 -congestion 0.992 -free_flow 0.989 - total: 0.99 }\n",
            "\n",
            "Computing the sparsity for the validation set...\n",
            "Sparsity: { severe_congestion 0.99 -congestion 0.991 -free_flow 0.987 - total: 0.989 }\n",
            "\n",
            "Computing the sparsity for the test set...\n",
            "Sparsity: { severe_congestion 0.99 -congestion 0.992 -free_flow 0.989 - total: 0.99 }\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Computing the sparsity for the training set...')\n",
        "print_all_sparsity(x_train, x_train_explained)\n",
        "print()\n",
        "\n",
        "print('Computing the sparsity for the validation set...')\n",
        "print_all_sparsity(x_val, x_val_explained)\n",
        "print()\n",
        "\n",
        "print('Computing the sparsity for the test set...')\n",
        "print_all_sparsity(x_test, x_test_explained)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
