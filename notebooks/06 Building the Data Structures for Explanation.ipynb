{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.explanation.navigator.model import Navigator\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the STGNN in evaluation mode.\n",
    "spatial_temporal_gnn.eval();\n",
    "\n",
    "# Get the Navigator and load the checkpoints.\n",
    "navigator = Navigator(DEVICE)\n",
    "\n",
    "navigator_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                          'navigator_metr_la.pth')\n",
    "\n",
    "navigator_checkpoints = torch.load(navigator_checkpoints_path)\n",
    "navigator.load_state_dict(navigator_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the Navigator in evaluation mode.\n",
    "navigator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the data scaler.\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the explained data.\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test.npy'))\n",
    "\n",
    "# Get the time information of the explained data.\n",
    "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test_time.npy'))\n",
    "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(BASE_DATA_DIR, 'structured', 'node_locations.pkl'), 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('773869', ('Ventura Freeway', 14.54715370560186)),\n",
       " ('767541', ('Glendale Freeway', 10.345751960044197)),\n",
       " ('767542', ('Glendale Freeway', 10.327808458197385)),\n",
       " ('717447', ('Hollywood Freeway', 13.112986186569106)),\n",
       " ('717446', ('Hollywood Freeway', 13.329055658812624))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(node_info.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y, sample_x_time, sample_y_time = x_test[0], y_test[0], x_test_time[0], y_test_time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n_timesteps, n_nodes, _ = y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_adjacency_distance_matrix)\n",
    "\n",
    "adj_distance_matrix = get_adjacency_distance_matrix(adj_matrix, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_temporal_distance_matrix)\n",
    "\n",
    "temporal_distance_matrix = get_temporal_distance_matrix(n_nodes, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best parameters based on the results of the grid search.\n",
    "\n",
    "EPS = .35\n",
    "MIN_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.  ],\n",
       "        [62.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[63.  ],\n",
       "        [63.  ],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]],\n",
       "\n",
       "       [[63.75],\n",
       "        [59.25],\n",
       "        [ 0.  ],\n",
       "        ...,\n",
       "        [ 0.  ],\n",
       "        [ 0.  ],\n",
       "        [ 0.  ]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x[..., 0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2484, 1)\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.clustering.clustering import get_clusters\n",
    "\n",
    "clusters = get_clusters(\n",
    "    sample_x[..., :1],\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    eps=EPS,\n",
    "    min_samples=MIN_SAMPLES,\n",
    "    remove_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(sample_x != 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "\n",
    "print(np.unique(clusters))\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.unique(sample_x[clusters == -2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,\n",
       "         8,  9,  9, 10, 10, 11, 11], dtype=int64),\n",
       " array([ 16, 196,  16, 196,  16, 196,  16, 196,  16, 196,  16, 196,  16,\n",
       "        196,  16, 196,  16, 196,  16, 196,  16, 196,  16, 196], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0], dtype=int64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _get_time(date: np.datetime64) -> Tuple[str, str, str]:\n",
    "    days = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',\n",
    "            4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "    Y, M, D, h, m = [date.astype('datetime64[%s]' % kind) for kind in 'YMDhm']\n",
    "\n",
    "    year = Y.astype(int) + 1970\n",
    "    month = M.astype(int) % 12 + 1\n",
    "    day = (D - M).astype(int) + 1\n",
    "    day_of_week = days[((D - M).astype(int) - 1) % 7]\n",
    "    hour = (h - D).astype(int)\n",
    "    minute = (m - h).astype(int)\n",
    "\n",
    "    return day_of_week, f'{day:02d}/{month:02d}/{year}', f'{hour:02d}:{minute:02d}'\n",
    "\n",
    "def _get_cluster_type(\n",
    "    values: np.ndarray,\n",
    "    congestion_max_speed: float = 60,\n",
    "    free_flow_min_speed: float = 110\n",
    "    ) -> str:\n",
    "    if np.all(values) <= congestion_max_speed:\n",
    "        return 'congestion'\n",
    "    elif np.all(values) >= free_flow_min_speed:\n",
    "        return 'free flow'\n",
    "    else:\n",
    "        return 'group of nodes'\n",
    "\n",
    "def _set_cluster_location_info(\n",
    "    knowledge_graph: List[Tuple[str, str, str]],\n",
    "    reference_cluster_name: str,\n",
    "    node_info: Dict[str, Tuple[str, int]],\n",
    "    node_indices: np.ndarray\n",
    "    ) -> None:\n",
    "    # Get the unique node indices.\n",
    "    node_indices = np.unique(node_indices)\n",
    "    # Get the IDs of the nodes by their indices.\n",
    "    node_ids = [ node_pos_dict[idx] for idx in node_indices ]\n",
    "\n",
    "    # Get a dictionary containing the street and kilometrage of each node.\n",
    "    streets = {}\n",
    "    for node_id in node_ids:\n",
    "        # Get the street and kilometrage of the node.\n",
    "        street, km = node_info[node_id]\n",
    "        # Add the street and kilometrage to the dictionary.\n",
    "        if not street in streets.keys():\n",
    "            streets[street] = [km]\n",
    "        else:\n",
    "            streets[street].append(km)\n",
    "\n",
    "    for street, kms in streets.items():\n",
    "        # Add the street and its kilometrages to the knowledge graph.\n",
    "        knowledge_graph.append((reference_cluster_name, 'in highway', street))\n",
    "        for km in kms:\n",
    "            knowledge_graph.append((street, 'at km', f'{km:.2g}'))\n",
    "\n",
    "def _set_cluster_time_info(\n",
    "    knowledge_graph: List[Tuple[str, str, str]],\n",
    "    reference_cluster_name: str,\n",
    "    time_info: np.ndarray,\n",
    "    time_indices: np.ndarray\n",
    "    ) -> None:\n",
    "    # Get the minimum and maximum timestep of the target nodes.\n",
    "    min_timestep, max_timestep = np.min(time_indices), np.max(time_indices)\n",
    "    y_min_time, y_max_time = time_info[min_timestep][0], time_info[max_timestep][0]\n",
    "    beginning_day, beginning_date, beginning_hour = _get_time(y_min_time)\n",
    "    end_day, end_date, end_hour = _get_time(y_max_time)\n",
    "\n",
    "\n",
    "    # Put the date and day information of the target nodes in the\n",
    "    # knowledge graph.\n",
    "    if beginning_date == end_date:\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'on date', beginning_date))\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'on day', beginning_day))\n",
    "    else:\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'from date', beginning_date))\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'to date', end_date))\n",
    "\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'from day', beginning_day))\n",
    "        knowledge_graph.append(\n",
    "            (reference_cluster_name, 'to day', end_day))\n",
    "\n",
    "    # Put the time information of the target nodes in the knowledge graph.\n",
    "    knowledge_graph.append(\n",
    "        (reference_cluster_name, 'from time', beginning_hour))\n",
    "    knowledge_graph.append(\n",
    "        (reference_cluster_name, 'to time', end_hour))\n",
    "\n",
    "def get_knowledge_graph(\n",
    "    x: np.ndarray,\n",
    "    x_times: np.ndarray,\n",
    "    x_clusters: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    y_times: np.ndarray) -> List[Tuple[str, str, str]]:\n",
    "    knowledge_graph = []\n",
    "\n",
    "    # Get the values of the selected target nodes.\n",
    "    target_node_values = y[y > 0]\n",
    "\n",
    "    # Get the type of the target nodes cluster (eg.: congestion, free flow).\n",
    "    target_type = _get_cluster_type(target_node_values)\n",
    "\n",
    "    # Get the name of the target nodes cluster.\n",
    "    target_name = f'target {target_type}' \n",
    "    # Put the average speed of the target nodes cluster in the knowledge graph.\n",
    "    knowledge_graph.append(\n",
    "        (target_name, 'has average speed',\n",
    "         f'{target_node_values.mean():.3g} km/h'))\n",
    "\n",
    "    # Get the indices of the non-null values of the target nodes.\n",
    "    y_indices = np.nonzero(y)\n",
    "\n",
    "    _set_cluster_time_info(knowledge_graph, target_name, y_times, y_indices[0])\n",
    "\n",
    "    _set_cluster_location_info(\n",
    "        knowledge_graph, target_name, node_info, y_indices[1])\n",
    "\n",
    "    for i, c in enumerate([c for c in np.unique(x_clusters) if c != -1]):\n",
    "        # Get the values of the nodes of the cluster.\n",
    "        cluster_node_values = x[x_clusters == c]\n",
    "\n",
    "        # Get the type of the target nodes cluster (eg.: congestion, free flow).\n",
    "        if c == -2:\n",
    "            cluster_type = 'group of nodes'\n",
    "        else:\n",
    "            cluster_type = _get_cluster_type(cluster_node_values)\n",
    "\n",
    "        # Get the name of the target nodes cluster.\n",
    "        cluster_name = f'{cluster_type}{{{i}}}'\n",
    "        \n",
    "        # Add the causation information to the knowledge graph.\n",
    "        knowledge_graph.append((target_name, 'caused by', cluster_name))\n",
    "        \n",
    "        # Put the average speed of the target nodes cluster in the knowledge graph.\n",
    "        if c != -2:\n",
    "            knowledge_graph.append(\n",
    "                (cluster_name, \n",
    "                'has average speed', \n",
    "                f'{cluster_node_values.mean():.3g} km/h'))\n",
    "\n",
    "        x_indices = np.where(x_clusters == c)\n",
    "        _set_cluster_time_info(knowledge_graph, cluster_name, x_times, x_indices[0])\n",
    "        _set_cluster_location_info(knowledge_graph, cluster_name, node_info, x_indices[1])\n",
    "\n",
    "    print(knowledge_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.352768, 35.112137, 34.866005, 34.5799  , 34.926838, 34.584217,\n",
       "       35.600292, 35.142715, 34.550823, 34.035618, 32.767227, 32.132698,\n",
       "       33.677856, 33.246357, 33.76597 , 33.834396, 33.604694, 34.004375,\n",
       "       33.706665, 33.565376, 34.097023, 33.589268, 33.505253, 33.066093],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_y[sample_y != 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('target congestion', 'has average speed', '34.1 km/h'), ('target congestion', 'on date', '04/06/2012'), ('target congestion', 'on day', 'Wednesday'), ('target congestion', 'from time', '05:45'), ('target congestion', 'to time', '06:40'), ('target congestion', 'in highway', 'Arroyo Seco Parkway'), ('Arroyo Seco Parkway', 'at km', '1.8'), ('Arroyo Seco Parkway', 'at km', '2.5'), ('target congestion', 'caused by', 'group of nodes{0}'), ('group of nodes{0}', 'on date', '04/06/2012'), ('group of nodes{0}', 'on day', 'Wednesday'), ('group of nodes{0}', 'from time', '04:45'), ('group of nodes{0}', 'to time', '05:40'), ('group of nodes{0}', 'in highway', 'Ventura Freeway'), ('Ventura Freeway', 'at km', '15'), ('group of nodes{0}', 'in highway', 'Glendale Freeway'), ('Glendale Freeway', 'at km', '10'), ('Glendale Freeway', 'at km', '8.2'), ('group of nodes{0}', 'in highway', 'Hollywood Freeway'), ('Hollywood Freeway', 'at km', '14'), ('Hollywood Freeway', 'at km', '1.6'), ('Hollywood Freeway', 'at km', '3.8'), ('group of nodes{0}', 'in highway', 'San Diego Freeway'), ('San Diego Freeway', 'at km', '10'), ('San Diego Freeway', 'at km', '12'), ('group of nodes{0}', 'in highway', 'Foothill Freeway'), ('Foothill Freeway', 'at km', '29'), ('group of nodes{0}', 'in highway', 'Golden State Freeway'), ('Golden State Freeway', 'at km', '8.6'), ('target congestion', 'caused by', 'congestion{1}'), ('congestion{1}', 'has average speed', '56.1 km/h'), ('congestion{1}', 'on date', '04/06/2012'), ('congestion{1}', 'on day', 'Wednesday'), ('congestion{1}', 'from time', '05:10'), ('congestion{1}', 'to time', '05:40'), ('congestion{1}', 'in highway', 'Hollywood Freeway'), ('Hollywood Freeway', 'at km', '13'), ('Hollywood Freeway', 'at km', '14'), ('target congestion', 'caused by', 'congestion{2}'), ('congestion{2}', 'has average speed', '48.8 km/h'), ('congestion{2}', 'on date', '04/06/2012'), ('congestion{2}', 'on day', 'Wednesday'), ('congestion{2}', 'from time', '05:20'), ('congestion{2}', 'to time', '05:40'), ('congestion{2}', 'in highway', 'Hollywood Freeway'), ('Hollywood Freeway', 'at km', '1.6'), ('target congestion', 'caused by', 'congestion{3}'), ('congestion{3}', 'has average speed', '47.4 km/h'), ('congestion{3}', 'on date', '04/06/2012'), ('congestion{3}', 'on day', 'Wednesday'), ('congestion{3}', 'from time', '05:20'), ('congestion{3}', 'to time', '05:40'), ('congestion{3}', 'in highway', 'Arroyo Seco Parkway'), ('Arroyo Seco Parkway', 'at km', '3.5')]\n"
     ]
    }
   ],
   "source": [
    "get_knowledge_graph(sample_x, sample_x_time, clusters, sample_y, sample_y_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (1000) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = '''Translate this tuple set representing a knowledge graph into a textual output. Note, numbers between curly brackets are used to aid you to identify the entities of the graph, although they shouldn\\'t be considered in the textual output:\n",
    "\"\n",
    "(target congestion, from, 10:00)\n",
    "(target_congestion, to, 11:00)\n",
    "(target_congestion, caused by, congestion{0})\n",
    "(congestion{0}, from, 09:00)\n",
    "(congestion{0}, to, 10:00)\n",
    "(target_congestion, caused by, congestion{1})\n",
    "(congestion{1}, from, 07:00)\n",
    "(congestion{1}, to, 08:00)\n",
    "\"'''\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model.generate(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate this tuple set representing a knowledge graph into a textual output. Note, numbers between curly brackets are used to aid you to identify the entities of the graph, although they shouldn\\'t be considered in the textual output:\\n\"\\n(target congestion, from, 10:00)\\n(target_congestion, to, 11:00)\\n(target_congestion, caused by, congestion{0})\\n(congestion{0}, from, 09:00)\\n(congestion{0}, to, 10:00)\\n(target_congestion, caused by, congestion{1})\\n(congestion{1}, from, 07:00)\\n(congestion{1}, to, 08:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{1}, caused by, congestion{2})\\n(congestion{2}, from, 09:00)\\n(congestion{2}, caused by, congestion{3})\\n(congestion{3}, from, 07:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{3}, caused by, congestion{4})\\n(congestion{4}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{4}, caused by, congestion{5})\\n(congestion{5}, from, 07:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{5}, caused by, congestion{6})\\n(congestion{6}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{6}, caused by, congestion{7})\\n(congestion{7}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{7}, caused by, congestion{8})\\n(congestion{8}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{8}, caused by, congestion{9})\\n(congestion{9}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{9}, caused by, congestion{10})\\n(congestion{10}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{10}, caused by, congestion{11})\\n(congestion{11}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{11}, caused by, congestion{12})\\n(congestion{12}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{12}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{12}, caused by, congestion{13})\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n(target_congestion, from, 10:00)\\n(congestion{13}, from, 09:00)\\n\"\\n\\n(target_congestion, from, 10:00)\\n\\n(congestion{13}, from, 09:00)\\n\\n\"\\n\\n(target_congestion, from, 10:00)\\n\\n(congestion{13}, from, 09:00)\\n\\n\"\\n\\n(target_congestion, from,'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
