{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'pems-bay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.explanation.navigator.model import Navigator\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_pems_bay.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_pems_bay.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the STGNN in evaluation mode.\n",
    "spatial_temporal_gnn.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_pems_bay.csv'),\n",
    "    has_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the explained data.\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))[..., :1]\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test.npy'))[..., :1]\n",
    "\n",
    "# Get the time information of the explained data.\n",
    "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test_time.npy'))\n",
    "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config import MPH_TO_KMH_FACTOR, SEVERE_CONGESTION_THRESHOLD_MPH, CONGESTION_THRESHOLD_MPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test * MPH_TO_KMH_FACTOR\n",
    "y_test = y_test * MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(BASE_DATA_DIR, 'structured', 'node_locations.pkl'), 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y, sample_x_time, sample_y_time = x_test[0], y_test[0], x_test_time[0], y_test_time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n_timesteps, n_nodes, _ = y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_adjacency_distance_matrix)\n",
    "\n",
    "adj_distance_matrix = get_adjacency_distance_matrix(adj_matrix, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_temporal_distance_matrix)\n",
    "\n",
    "temporal_distance_matrix = get_temporal_distance_matrix(n_nodes, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best parameters based on the results of the grid search.\n",
    "\n",
    "SPEED_DISTANCE_WEIGHT = 2\n",
    "N_CLUSTERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_verbs = [\n",
    "    'predicted',\n",
    "    'anticipated',\n",
    "    'forecasted',\n",
    "    'expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph_sentences = [\n",
    "    'A {c} was {prediction} {w:} {d}, with an average speed of {s} km/h {t}.',\n",
    "    '{D}, {w} was {prediction} to experience {c}, averaging {s} km/h {t}.',\n",
    "    '{W} was {prediction} to {c} {d}, with an average speed of {s} km/h {t}.',\n",
    "    'A {c} was {prediction} to hit {w} {d}, {t}, with an average speed of {s} km/h.',\n",
    "    'A {c} was {prediction} on {w:on} {d}, with an average speed of {s} km/h {t}.',\n",
    "    '{W} was {prediction} to see {c} {d}, with an average speed of {s} km/h {t}.',\n",
    "    'A {c} was {prediction} {w:} {d}, {t}, with an average speed of {s} km/h.',\n",
    "    'A {c} was {prediction} to occur {w:} {d}, {t}, with an average speed of {s} km/h.',\n",
    "    '{D}, a {prediction} {c} affected {w}, maintaining an average speed of {s} km/h {t}.',\n",
    "    '{W} {prediction} {c} {d}, {t}, with an average speed of {s} km/h.']\n",
    "\n",
    "extra_involved_street_sentences = [\n",
    "    'The {c} also affected{w}.',\n",
    "    'The {c} also impacted{w}.',\n",
    "    'The {c} also hit{w}.',\n",
    "    'The {c} also took place {w:}.',\n",
    "    'The {c} also happened {w:}.',\n",
    "    'The {c} extended {w:}.']\n",
    "\n",
    "first_paragraph_end_sentences = [\n",
    "    'This was caused by a',\n",
    "    'This resulted from a',\n",
    "    'This was induced by a',\n",
    "    'This happened because of a',\n",
    "    'This was driven by a',\n",
    "    'This was a result of a',\n",
    "    'The motivation was a',\n",
    "    'This was triggered by a',\n",
    "    'This occurred because of a',\n",
    "    'The reason behind it was a']\n",
    "\n",
    "second_paragraph_connectors = [\n",
    "    'An initial {c}',\n",
    "    'A first {c}',\n",
    "    'Firstly, a {c}',\n",
    "    'Initially, a {c}',\n",
    "    'To begin, a {c}',\n",
    "    'To start, a {c}',\n",
    "    'To commence, a {c}',\n",
    "]\n",
    "\n",
    "second_paragraph_verbs = [\n",
    "    'occurred',\n",
    "    'happened',\n",
    "    'manifested',\n",
    "    'materialized',\n",
    "    'took place'\n",
    "]\n",
    "\n",
    "other_paragraphs_connectors = [\n",
    "    'Following this, {c}',\n",
    "    'Subsequently, {c}',\n",
    "    'Next, {c}',\n",
    "    'Then, {c}',\n",
    "    'Afterwards, {c}',\n",
    "    'After this, {c}',\n",
    "    'After that, {c}',\n",
    "]\n",
    "\n",
    "final_paragraph_connectors = [\n",
    "    'Finally, {c}',\n",
    "    'Lastly, {c}',\n",
    "    'Eventually, {c}',\n",
    "    'To conclude, {c}',\n",
    "    'In the end, {c}',\n",
    "    'Ultimately, {c}',\n",
    "    'At last, {c}']\n",
    "\n",
    "second_paragraph_sentences = [\n",
    "    ' {w:} {t} {d}, with an average speed of {s} km/h.',\n",
    "    ' {w:}, occurring {t} {d}, with an average speed of {s} km/h.',\n",
    "    ', averaging at a speed of {s} km/h, {w:} {d}, {t}.',\n",
    "    ', at {s} km/h, {w:} {d}, {t}.',\n",
    "    ', at {s} km/h, {w:} {t} {d}.',\n",
    "    ', with an average speed of {s} km/h, {w:} occurring {t} {d}.',\n",
    "    ' {w:}, with an average speed of {s} km/h, {d}, {t}.',\n",
    "    ' {w:}, occurring {t} {d}, with an average speed of {s} km/h.',\n",
    "    ' {w:}, with an average speed of {s} km/h, {d}, {t}.',\n",
    "    ' {d}, {t} {w:} with an average speed of {s} km/h.']\n",
    "\n",
    "another_connectors = [\n",
    "    'another',\n",
    "    'a new',\n",
    "    'a further',\n",
    "    'an additional',\n",
    "    'an extra']\n",
    "\n",
    "again_connectors= [\n",
    "    'again',\n",
    "    'once more',\n",
    "    'another time']\n",
    "\n",
    "yet_again_connectors = [\n",
    "    'yet again',\n",
    "    'once again',\n",
    "    'yet another time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import get_explanation_clusters\n",
    "\n",
    "clusters = get_explanation_clusters(\n",
    "    sample_x,\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    speed_distance_weight=SPEED_DISTANCE_WEIGHT,\n",
    "    n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _get_cluster_type(\n",
    "    values: np.ndarray\n",
    "    ) -> str:\n",
    "    if values.mean() <= SEVERE_CONGESTION_THRESHOLD_MPH * MPH_TO_KMH_FACTOR:\n",
    "        return 'severe congestion'\n",
    "\n",
    "    elif values.mean() <= CONGESTION_THRESHOLD_MPH * MPH_TO_KMH_FACTOR:\n",
    "        return 'congestion'\n",
    "\n",
    "    else:\n",
    "        return 'free flow'\n",
    "\n",
    "def _get_time(date: np.datetime64) -> Tuple[str, str, str]:\n",
    "    days = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',\n",
    "            4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "    Y, M, D, h, m = [date.astype('datetime64[%s]' % kind) for kind in 'YMDhm']\n",
    "\n",
    "    year = Y.astype(int) + 1970\n",
    "    month = M.astype(int) % 12 + 1\n",
    "    day = (D - M).astype(int) + 1\n",
    "    day_of_week = days[((D - M).astype(int) - 1) % 7]\n",
    "    hour = (h - D).astype(int)\n",
    "    minute = (m - h).astype(int)\n",
    "\n",
    "    return day_of_week, f'{day:02d}/{month:02d}/{year}', f'{hour:02d}:{minute:02d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def _get_cluster_location_info(\n",
    "    node_info: Dict[str, Tuple[str, int]],\n",
    "    node_indices: np.ndarray\n",
    "    ) -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Get a dictionary containing the street involved in the cluster along\n",
    "    with their involved kms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_info : { str: (str, int) }\n",
    "        The dictionary containing for each node id the street and kilometrage.\n",
    "    node_indices : ndarray\n",
    "        The indices of the nodes involved in the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    { str: list of int }\n",
    "        The dictionary containing the streets involved in the cluster along\n",
    "        with their involved kms.\n",
    "    \"\"\"\n",
    "    # Get the unique node indices.\n",
    "    node_indices = np.unique(node_indices)\n",
    "    # Get the IDs of the nodes by their indices.\n",
    "    node_ids = [ node_pos_dict[idx] for idx in node_indices ]\n",
    "\n",
    "    # Get a dictionary containing the street and kilometrage of each node.\n",
    "    streets = {}\n",
    "    for node_id in node_ids:\n",
    "        # Get the street and kilometrage of the node.\n",
    "        street, km = node_info[node_id]\n",
    "        # Add the street and kilometrage to the dictionary.\n",
    "        if not street in streets.keys():\n",
    "            streets[street] = [km]\n",
    "        else:\n",
    "            streets[street].append(km)\n",
    "    # Sort the kms of each street and round them to the nearest integer.\n",
    "    for street, kms in streets.items():\n",
    "        streets[street] = sorted(set([int(km) for km in kms]))\n",
    "    # Sort the streets by the number of involved kms.\n",
    "    streets = dict(sorted(streets.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "    return streets\n",
    "\n",
    "def _get_repetition_of_location_information(\n",
    "    location_information: Dict[str, List[int]],\n",
    "    previous_location_information: List[Dict[str, List[int]]] = None,\n",
    "    ) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    For each street, count the times it has been involved in the previous\n",
    "    location information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    location_information : { str: list of int }\n",
    "        The dictionary containing the location information.\n",
    "    previous_location_information : list of { str: list of int }, optional\n",
    "        The list of the previous location information, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    { str: int }\n",
    "        The dictionary containing the times each street has been involved in\n",
    "        the previous location information.\n",
    "    \"\"\"\n",
    "    # Set the dictionary containing the times each street has been involved in\n",
    "    # the previous location information to count 0 for each street.\n",
    "    equal_times_counts = { k: 0 for k in location_information.keys() }\n",
    "    \n",
    "    if previous_location_information is not None:\n",
    "        # For each street, count the times it has been involved in the previous\n",
    "        # location information.\n",
    "        for k in location_information.keys():\n",
    "            equal_times_count = 0\n",
    "            for previous_location_info in previous_location_information:\n",
    "                if k in previous_location_info.keys():\n",
    "                    equal_times_count += 1\n",
    "            equal_times_counts[k] = equal_times_count\n",
    "\n",
    "    return equal_times_counts\n",
    "\n",
    "def _get_target_location_sentence(\n",
    "    location_information: Dict[str, List[int]],\n",
    "    previous_location_information: List[Dict[str, List[int]]] = None\n",
    "    ) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Get a list of tuple of sentences which for each involved street contains the\n",
    "    information about the involved kms. The first sentence of the tuple is the\n",
    "    sentence without the adverb, the second one is the sentence with the adverb\n",
    "    \"on\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    location_information : { str: list of int }\n",
    "        The dictionary containing the location information.\n",
    "        Keys are the streets and values are the involved kms.\n",
    "    previous_location_information : list of { str: list of int }, optional\n",
    "        The list of the previous location information, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of (str, str)\n",
    "        The list of the sentences containing the location information.\n",
    "        The first sentence of the tuple is the sentence without the adverb,\n",
    "        the second one is the sentence with the adverb \"on\".\n",
    "    \"\"\"\n",
    "    previous_locations_information_count = _get_repetition_of_location_information(\n",
    "        location_information,\n",
    "        previous_location_information)\n",
    "\n",
    "    # Set the list of the location sentences.\n",
    "    location_sentences = []\n",
    "    for street, kms in location_information.items():\n",
    "        if previous_locations_information_count[street] == 0:\n",
    "            connector = ''\n",
    "        elif previous_locations_information_count[street] == 1:\n",
    "            connector = f', {random.choice(again_connectors)}, '\n",
    "        else:\n",
    "            connector = f', {random.choice(yet_again_connectors)}, '\n",
    "        \n",
    "        location_sentence = f'{connector}{street} at '\n",
    "        \n",
    "        \n",
    "        if len(kms) == 1:\n",
    "            location_sentence += f'km {kms[0]}'\n",
    "        else:\n",
    "            kms_sentence = ', '.join([f'{km}' for km in kms[:-1]])\n",
    "            kms_sentence += f' and {kms[-1]}'\n",
    "            location_sentence += f'kms {kms_sentence}'\n",
    "        location_sentences.append((\n",
    "            location_sentence, \n",
    "            'on ' + location_sentence if connector == '' else 'on' + location_sentence))\n",
    "    return location_sentences\n",
    "\n",
    "def _link_other_locations_sentences(\n",
    "    location_sentences: List[str],\n",
    "    ) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Link the sentences containing the location information in a single\n",
    "    sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    location_sentences : list of str\n",
    "        The list of the sentences containing the location information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The linked sentence containing the location information.\n",
    "    str\n",
    "        The linked sentence containing the location information with the\n",
    "        adverb \"on\". \n",
    "    \"\"\"\n",
    "    if len(location_sentences) == 1:\n",
    "        return location_sentences[0]\n",
    "    else:\n",
    "        formatted_location_sentences = [\n",
    "            l[0][2:] if l[0][0] == ',' else l[0] for l in location_sentences]\n",
    "        \n",
    "        location_sentence = ', '.join(formatted_location_sentences[:-1])\n",
    "        location_sentence += f' and {formatted_location_sentences[-1]}'\n",
    "\n",
    "        formatted_location_sentences = [\n",
    "            l[1][2:] if l[1][0] == ',' else l[1] for l in location_sentences]\n",
    "        location_sentence_adv = ', '.join(formatted_location_sentences[:-1])\n",
    "        location_sentence_adv += f' and {formatted_location_sentences[-1]}'\n",
    "\n",
    "        return location_sentence, location_sentence_adv\n",
    "\n",
    "def _get_cluster_time_info(\n",
    "    time_info: np.ndarray,\n",
    "    time_indices: np.ndarray\n",
    "    ) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get the time information of the cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_info : ndarray\n",
    "        The array containing the time information of the nodes.\n",
    "    time_indices : ndarray\n",
    "        The indices of the nodes involved in the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    { str: str }\n",
    "        The dictionary containing the time information of the cluster.\n",
    "    \"\"\"\n",
    "    # Get the minimum and maximum timestep of the target nodes.\n",
    "    min_timestep, max_timestep = np.min(time_indices), np.max(time_indices)\n",
    "    y_min_time, y_max_time = time_info[min_timestep][0], time_info[max_timestep][0]\n",
    "    beginning_day, beginning_date, beginning_hour = _get_time(y_min_time)\n",
    "    end_day, end_date, end_hour = _get_time(y_max_time)\n",
    "\n",
    "    cluster_time_info = {}\n",
    "\n",
    "    # Put the date and day information of the target nodes in the\n",
    "    # knowledge graph.\n",
    "    if beginning_date == end_date:\n",
    "        cluster_time_info['on date'] = beginning_date\n",
    "        cluster_time_info['on day'] = beginning_day\n",
    "        if beginning_hour == end_hour:\n",
    "            cluster_time_info['on time'] = beginning_hour\n",
    "        else:\n",
    "            cluster_time_info['from time'] = beginning_hour\n",
    "            cluster_time_info['to time'] = end_hour\n",
    "    else:\n",
    "        cluster_time_info['from date'] = beginning_date\n",
    "        cluster_time_info['to date'] = end_date\n",
    "\n",
    "        cluster_time_info['from day'] = beginning_day\n",
    "        cluster_time_info['to day'] = end_day\n",
    "        \n",
    "        # Put the time information of the target nodes in the knowledge graph.\n",
    "        cluster_time_info['from time'] = beginning_hour\n",
    "        cluster_time_info['to time'] = end_hour\n",
    "        \n",
    "    return cluster_time_info\n",
    "\n",
    "def _get_time_sentence(\n",
    "    temporal_information: Dict[str, str]\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Get the sentence containing the time information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    temporal_information : { str: str }\n",
    "        The dictionary containing the time information of the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The sentence containing the time information.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'from time' in temporal_information:\n",
    "        from_time = temporal_information['from time']\n",
    "        to_time = temporal_information['to time']\n",
    "        return f'from {from_time} to {to_time}'\n",
    "    else:\n",
    "        on_time = temporal_information['on time']\n",
    "        return f'at {on_time}'\n",
    "\n",
    "def _get_target_day_sentence(\n",
    "    target_temporal_information: Dict[str, Any],\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Get the sentence containing the day information of the cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_temporal_information : { str: Any }\n",
    "        The dictionary containing the time information of the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The sentence containing the day information of the cluster.\n",
    "    \"\"\"\n",
    "    if 'from day' in target_temporal_information:\n",
    "        from_day = target_temporal_information['from day']\n",
    "        to_day = target_temporal_information['to day']\n",
    "        from_date = target_temporal_information['from date']\n",
    "        to_date = target_temporal_information['to date']\n",
    "        day_sentence = f'from {from_day}, {from_date} to {to_day}, {to_date}'\n",
    "    else:\n",
    "        on_day = target_temporal_information['on day']\n",
    "        on_date = target_temporal_information['on date']\n",
    "        day_sentence = f'on {on_day}, {on_date}'\n",
    "        \n",
    "    return day_sentence\n",
    "\n",
    "def _get_formatted_day_sentence(\n",
    "    input_temporal_information: Dict[str, str],\n",
    "    target_date_dt: datetime,\n",
    "    is_target_more_days: bool,\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Get the sentence containing the day information of the cluster\n",
    "    formatted in a way that is more readable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_temporal_information : { str: str }\n",
    "        The dictionary containing the time information of the cluster.\n",
    "    target_date_dt : datetime\n",
    "        The datetime of the date of the target cluster.\n",
    "    is_target_more_days : bool\n",
    "        Whether the target cluster spans in more days.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The sentence containing the day information of the cluster\n",
    "        formatted in a way that is more readable.\n",
    "    \"\"\"\n",
    "    target_adjective = 'first' if is_target_more_days else 'same' \n",
    "    \n",
    "    # Case where the input temporal information spawns in more days.\n",
    "    if 'from day' in input_temporal_information:\n",
    "        input_from_date = input_temporal_information['from date']\n",
    "        input_from_date_dt = datetime.strptime(input_from_date, '%d/%m/%Y')\n",
    "\n",
    "        input_to_date = input_temporal_information['to date']\n",
    "        input_to_date_dt = datetime.strptime(input_to_date, '%d/%m/%Y')\n",
    "\n",
    "        input_from_day = input_temporal_information['from day']\n",
    "        input_to_day = input_temporal_information['to day']\n",
    "        \n",
    "        # If input end date is the same as the target date.\n",
    "        if input_to_date_dt == target_date_dt:\n",
    "            # If input start date is the day before the target date.\n",
    "            if input_from_date_dt == target_date_dt - timedelta(days=1):\n",
    "                return f'from the previous to the {target_adjective} day'\n",
    "            else:\n",
    "                return f'from {input_from_day}, {input_from_date} to the {target_adjective} day'\n",
    "        # If input end date is different from the target date.\n",
    "        else:\n",
    "            return f'from {input_from_day}, {input_from_date} to {input_to_day}, {input_to_date}'\n",
    "    # Case where the input temporal information spawns in one day.\n",
    "    else:\n",
    "        input_on_date = input_temporal_information['on date']\n",
    "        input_on_date_dt = datetime.strptime(input_on_date, '%d/%m/%Y')\n",
    "\n",
    "        input_on_day = input_temporal_information['on day']\n",
    "        # If input date is the same as the target date.\n",
    "        if input_on_date_dt == target_date_dt:\n",
    "            if is_target_more_days:\n",
    "                return f'on the {target_adjective} day'\n",
    "            else:\n",
    "                return ''\n",
    "        # If input date is the day before the target date.\n",
    "        elif input_on_date_dt == target_date_dt - timedelta(days=1):\n",
    "            return f'on the previous day'\n",
    "        # If input date is different from the target date.\n",
    "        else:\n",
    "            return f'on {input_on_day}, {input_on_date}'\n",
    "\n",
    "def _get_input_day_sentence(\n",
    "    target_temporal_information: Dict[str, Any],\n",
    "    input_temporal_information: Dict[str, Any],\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Get the sentence containing the day information of the cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_temporal_information : { str: Any }\n",
    "        The dictionary containing the time information of the target cluster.\n",
    "    input_temporal_information :  { str: Any }\n",
    "        The dictionary containing the time information of the input cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The sentence containing the day information of the cluster.\n",
    "    \"\"\"\n",
    "    # Case where the target temporal information spawns in more days.\n",
    "    if 'from day' in target_temporal_information:\n",
    "        target_from_date = target_temporal_information['from date']\n",
    "        target_from_date_dt = datetime.strptime(target_from_date, '%d/%m/%Y')\n",
    "        return _get_formatted_day_sentence(\n",
    "            input_temporal_information,\n",
    "            target_from_date_dt,\n",
    "            is_target_more_days=True)\n",
    "        \n",
    "    # Case where the target temporal information spawns in one day.\n",
    "    else:\n",
    "        target_on_date = target_temporal_information['on date']\n",
    "        target_on_date_dt = datetime.strptime(target_on_date, '%d/%m/%Y')\n",
    "        return _get_formatted_day_sentence(\n",
    "            input_temporal_information,\n",
    "            target_on_date_dt,\n",
    "            is_target_more_days=False)\n",
    "\n",
    "def _replace_template_placeholder(\n",
    "    sentence: str,\n",
    "    placeholder: str,\n",
    "    replacement: str,\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Replace the `placeholder` in the `sentence` with the `replacement`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The sentence containing the placeholder.\n",
    "    placeholder : str\n",
    "        The placeholder to replace.\n",
    "    replacement : str\n",
    "        The replacement of the placeholder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The sentence with the placeholder replaced.\n",
    "    \"\"\"\n",
    "    if placeholder == '{d}' and replacement == '':\n",
    "        sentence = sentence.replace('{d}, ', '')\n",
    "        sentence = sentence.replace(' {d}.', '.')\n",
    "\n",
    "    # Substitute `placeholder` in the sentence with the `replacement`.\n",
    "    sentence = sentence.replace(placeholder.upper(), replacement.capitalize())\n",
    "    sentence = sentence.replace(placeholder, replacement)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def _fill_first_paragraph_template(\n",
    "    predicted_cluster_kind: str,\n",
    "    time_sentence: str,\n",
    "    day_sentence: str,\n",
    "    average_speed: float,\n",
    "    street_sentences: List[str],\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Fill the template of the first paragraph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_cluster_kind : str\n",
    "        The type of the predicted cluster.\n",
    "    time_sentence : str\n",
    "        The sentence containing the time information.\n",
    "    day_sentence : str\n",
    "        The sentence containing the day information.\n",
    "    average_speed : float\n",
    "        The average speed of the target nodes in the cluster.\n",
    "    street_sentences : list of str\n",
    "        The list of the sentences containing the location information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The filled template of the first paragraph.\n",
    "    \"\"\"\n",
    "    sentence = random.choice(first_paragraph_sentences)\n",
    "    # substitute prediction verb\n",
    "    sentence = sentence.replace('{prediction}', random.choice(prediction_verbs))\n",
    "    # Add the cluster type information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{c}', predicted_cluster_kind)\n",
    "    # Add the name of the street information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{w}', street_sentences[0][0])\n",
    "    sentence = _replace_template_placeholder(sentence, '{w:}', street_sentences[0][1])\n",
    "    # Add the day information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{d}', day_sentence)\n",
    "    # Add the time information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{t}', time_sentence)\n",
    "    # Add the average speed information to the sentence.\n",
    "    sentence = sentence.replace('{s}', f'{average_speed:.2f}')\n",
    "    if len(street_sentences) > 1:\n",
    "        extra_involved_street_sentence = random.choice(extra_involved_street_sentences)\n",
    "        other_locations_sentences = _link_other_locations_sentences(street_sentences[1:])\n",
    "        other_locations_sentence = other_locations_sentences[0]\n",
    "        if not other_locations_sentence.startswith(','):\n",
    "            other_locations_sentence = ' ' + other_locations_sentence\n",
    "        other_locations_sentence_adv = other_locations_sentences[1]\n",
    "        extra_involved_street_sentence = _replace_template_placeholder(extra_involved_street_sentence, '{w}', other_locations_sentence)\n",
    "        extra_involved_street_sentence = _replace_template_placeholder(extra_involved_street_sentence, '{w:}', other_locations_sentence_adv)\n",
    "        extra_involved_street_sentence = _replace_template_placeholder(extra_involved_street_sentence, '{c}', predicted_cluster_kind)\n",
    "        return sentence + ' ' + extra_involved_street_sentence\n",
    "    return sentence\n",
    "\n",
    "def _fill_other_paragraph(\n",
    "    input_cluster_kind: str,\n",
    "    formatted_cluster_type: str,\n",
    "    connector: str,\n",
    "    time_sentence: str,\n",
    "    day_sentence: str,\n",
    "    average_speed: float,\n",
    "    street_sentences: List[str],\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Fill the template of the other paragraphs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_cluster_kind : str\n",
    "        The type of the input cluster.\n",
    "    formatted_cluster_type : str\n",
    "        The type of the input cluster formatted.\n",
    "    connector : str\n",
    "        The connector of the paragraph.\n",
    "    time_sentence : str\n",
    "        The sentence containing the time information.\n",
    "    day_sentence : str\n",
    "        The sentence containing the day information.\n",
    "    average_speed : float\n",
    "        The average speed of the target nodes in the cluster.\n",
    "    street_sentence : list of str\n",
    "        The list of the sentences containing the location information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The filled template of the other paragraph.\n",
    "    \"\"\"\n",
    "    sentence = connector + random.choice(second_paragraph_sentences)\n",
    "    # Add the input type information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{c}', formatted_cluster_type)\n",
    "    # substitute prediction verb\n",
    "    sentence = sentence.replace('{prediction}', random.choice(prediction_verbs))\n",
    "    # Add the name of the street information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{w}', street_sentences[0][0])\n",
    "    sentence = _replace_template_placeholder(sentence, '{w:}', street_sentences[0][1])\n",
    "    # Add the day information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{d}', day_sentence)\n",
    "    # Add the time information to the sentence.\n",
    "    sentence = _replace_template_placeholder(sentence, '{t}', time_sentence)\n",
    "    # Add the average speed information to the sentence.\n",
    "    sentence = sentence.replace('{s}', f'{average_speed:.2f}')\n",
    "    if len(street_sentences) > 1:\n",
    "        extra_involved_street_sentence = random.choice(extra_involved_street_sentences)\n",
    "        other_locations_sentences = _link_other_locations_sentences(street_sentences[1:])\n",
    "        other_locations_sentence = other_locations_sentences[0]\n",
    "        if not other_locations_sentence.startswith(','):\n",
    "            other_locations_sentence = ' ' + other_locations_sentence\n",
    "        other_locations_sentence_adv = other_locations_sentences[1]\n",
    "        extra_involved_street_sentence = _replace_template_placeholder(extra_involved_street_sentence, '{w}', other_locations_sentence)\n",
    "        extra_involved_street_sentence = _replace_template_placeholder(extra_involved_street_sentence, '{w:}', other_locations_sentence_adv)\n",
    "        extra_involved_street_sentence = _replace_template_placeholder(extra_involved_street_sentence, '{c}', input_cluster_kind)\n",
    "        return sentence + ' ' + extra_involved_street_sentence\n",
    "    return sentence\n",
    "\n",
    "def _get_cluster_time_span(\n",
    "    temporal_information: Dict[str, str],\n",
    "    ) -> Tuple[datetime, datetime]:\n",
    "    \"\"\"\n",
    "    Get the time span of the cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    temporal_information : { str: str}\n",
    "        The dictionary containing the time information of the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datetime\n",
    "        The time datetime of the beginning of the cluster.\n",
    "    datetime\n",
    "        The time datetime of the end of the cluster.\n",
    "    \"\"\"\n",
    "    if 'from time' in temporal_information:\n",
    "        from_time = temporal_information['from time']\n",
    "        from_time_datetime = datetime.strptime(from_time, '%H:%M')\n",
    "        to_time = temporal_information['to time']\n",
    "        to_time_datetime = datetime.strptime(to_time, '%H:%M')\n",
    "        return (from_time_datetime.time(), to_time_datetime.time())\n",
    "    else:\n",
    "        on_time = temporal_information['on time']\n",
    "        time_datetime = datetime.strptime(on_time, '%H:%M')\n",
    "        return (time_datetime.time(), time_datetime.time())\n",
    "\n",
    "def _get_cluster_day_span(\n",
    "    temporal_information: Dict[str, str],\n",
    "    ) -> Tuple[datetime, datetime]:\n",
    "    \"\"\"\n",
    "    Get the day span of the cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    temporal_information : { str: str }\n",
    "        The dictionary containing the time information of the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datetime\n",
    "        The day datetime of the beginning of the cluster.\n",
    "    datetime\n",
    "        The day datetime of the end of the cluster.\n",
    "    \"\"\"\n",
    "    # Get the clusters sorted by time.\n",
    "    if 'from day' in temporal_information:\n",
    "        from_date = temporal_information['from date']\n",
    "        from_date_datetime = datetime.strptime(from_date, '%d/%m/%Y')\n",
    "        to_date = temporal_information['to date']\n",
    "        to_date_datetime = datetime.strptime(to_date, '%d/%m/%Y')\n",
    "        from_time, to_time = _get_cluster_time_span(temporal_information)\n",
    "        return (datetime.combine(from_date_datetime, from_time),\n",
    "                datetime.combine(to_date_datetime, to_time))\n",
    "    else:\n",
    "        on_date = temporal_information['on date']\n",
    "        on_date_datetime = datetime.strptime(on_date, '%d/%m/%Y')\n",
    "        from_time, to_time = _get_cluster_time_span(temporal_information)\n",
    "        return (datetime.combine(on_date_datetime, from_time),\n",
    "                datetime.combine(on_date_datetime, to_time))\n",
    "\n",
    "def _get_repetition_of_cluster_type_information(\n",
    "    cluster_type_information: str,\n",
    "    previous_cluster_type_information: List[str],\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    Get the number of times the cluster type information has been repeated\n",
    "    in the previous clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster_type_information : str\n",
    "        The cluster type information.\n",
    "    previous_cluster_type_information : list of str\n",
    "        The list of the previous cluster type information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of times the cluster type information has been repeated\n",
    "        in the previous clusters.\n",
    "    \"\"\"\n",
    "    equal_clusters_count = 0\n",
    "    for c in previous_cluster_type_information:\n",
    "        if cluster_type_information == c:\n",
    "            equal_clusters_count += 1\n",
    "    return equal_clusters_count\n",
    "\n",
    "def _get_first_paragraph_plus_end_sentence(\n",
    "    first_paragraph: str,\n",
    "    cluster_types: List[str]\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Get the first paragraph end sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first_paragraph : str\n",
    "        The first paragraph.\n",
    "    cluster_types : list of str\n",
    "        The list of the cluster types present in the explanation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The first paragraph with the added end sentence.\n",
    "    \"\"\"\n",
    "    first_paragraph_end_sentence = random.choice(first_paragraph_end_sentences)\n",
    "    first_paragraph += ' ' + first_paragraph_end_sentence\n",
    "    \n",
    "    n_congestions = 0.\n",
    "    n_free_flows = 0.\n",
    "    \n",
    "    endings = {\n",
    "        (0, 1): ' free flow.',\n",
    "        (1, 0): ' congestion.',\n",
    "        (1, 1): ' congestion and a free flow.',\n",
    "        (0, len(cluster_types)): ' series of free flows.',\n",
    "        (len(cluster_types), 0): ' series of congestions.',\n",
    "        (1, len(cluster_types) - 1): ' series of free flows and a congestion.',\n",
    "        (len(cluster_types) - 1, 1): ' series of congestions and a free flow.',\n",
    "    }\n",
    "    \n",
    "    for cluster_type in cluster_types:\n",
    "        if cluster_type in['congestion', 'severe congestion']:\n",
    "            n_congestions += 1\n",
    "        elif cluster_type == 'free flow':\n",
    "            n_free_flows += 1\n",
    "    if (n_congestions, n_free_flows) in endings.keys():\n",
    "        return first_paragraph + endings[(n_congestions, n_free_flows)]\n",
    "    else:\n",
    "        return first_paragraph + ' series of congestions and free flows.'\n",
    "\n",
    "def get_verbal_explanation(\n",
    "    x: np.ndarray,\n",
    "    x_times: np.ndarray,\n",
    "    x_clusters: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    y_times: np.ndarray\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Get the verbal explanation of the prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        The input data.\n",
    "    x_times : ndarray\n",
    "        The input data times.\n",
    "    x_clusters : ndarray\n",
    "        The input data clusters.\n",
    "    y : ndarray\n",
    "        The target data.\n",
    "    y_times : ndarray\n",
    "        The target data times.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The verbal explanation of the prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the values of the selected target nodes.\n",
    "    target_node_values = y[y > 0]\n",
    "\n",
    "    # Get the type of the target nodes cluster (eg.: congestion, free flow).\n",
    "    target_cluster_type = _get_cluster_type(target_node_values)\n",
    "\n",
    "    # Get the indices of the non-null values of the target nodes.\n",
    "    y_indices = np.nonzero(y)\n",
    "\n",
    "    # Translate the temporal information.\n",
    "    target_temporal_information = _get_cluster_time_info(y_times, y_indices[0])\n",
    "    day_sentence = _get_target_day_sentence(target_temporal_information)\n",
    "    time_sentence = _get_time_sentence(target_temporal_information)\n",
    "\n",
    "    # Translate the location information.\n",
    "    target_street_information = _get_cluster_location_info(node_info, y_indices[1])\n",
    "    street_sentence = _get_target_location_sentence(target_street_information)\n",
    "\n",
    "    # Get the average speed of the target nodes.\n",
    "    target_average_speed = target_node_values.mean()\n",
    "\n",
    "    # Get the first paragraph.\n",
    "    first_paragraph = _fill_first_paragraph_template(\n",
    "        target_cluster_type,\n",
    "        time_sentence,\n",
    "        day_sentence,\n",
    "        target_average_speed,\n",
    "        street_sentence)\n",
    "\n",
    "    # Set the list of the other paragraphs.\n",
    "    other_paragraphs = []\n",
    "\n",
    "    # Get the input clusters IDs.\n",
    "    input_clusters_ids = [c for c in np.unique(x_clusters) if c != -1]\n",
    "\n",
    "    input_clusters_with_information = []\n",
    "\n",
    "    for c in input_clusters_ids:\n",
    "        # Get the values of the nodes of the cluster.\n",
    "        input_node_values = x[x_clusters == c]\n",
    "\n",
    "        # Get the type of the cluster.\n",
    "        input_cluster_type = _get_cluster_type(input_node_values)\n",
    "\n",
    "        # Get the indices of the clusters in the input data.\n",
    "        x_indices = np.where(x_clusters == c)\n",
    "\n",
    "        # Get the temporal information.\n",
    "        input_temporal_information = _get_cluster_time_info(x_times, x_indices[0])\n",
    "\n",
    "        # Get the location information.\n",
    "        input_location_information = _get_cluster_location_info(node_info, x_indices[1])\n",
    "\n",
    "        # Get the average speed of the target nodes.\n",
    "        input_average_speed = input_node_values.mean()\n",
    "\n",
    "        # Set the input information in a dictionary.\n",
    "        input_information = {\n",
    "            'type': input_cluster_type,\n",
    "            'temporal': input_temporal_information,\n",
    "            'location': input_location_information,\n",
    "            'average_speed': input_average_speed\n",
    "        }\n",
    "        \n",
    "        # Add the cluster information to the list.\n",
    "        input_clusters_with_information.append((c, input_information))\n",
    "\n",
    "    first_paragraph = _get_first_paragraph_plus_end_sentence(\n",
    "        first_paragraph,\n",
    "        [inf['type'] for _, inf in input_clusters_with_information])\n",
    "\n",
    "    # Sort the clusters by the time they occur and get just the information.\n",
    "    input_clusters_with_information = sorted(\n",
    "        input_clusters_with_information,\n",
    "        key=lambda x: _get_cluster_day_span(x[1]['temporal']))\n",
    "\n",
    "    for i, (_, info) in enumerate(input_clusters_with_information):\n",
    "        # Get the type of the input cluster.\n",
    "        input_cluster_type = info['type']\n",
    "        same_cluster_type_count = _get_repetition_of_cluster_type_information(\n",
    "            input_cluster_type,\n",
    "            [inf['type'] for _, inf in input_clusters_with_information[:i]])\n",
    "        formatted_cluster_type = f'contributing {info[\"type\"]}'\n",
    "        if i > 0 and same_cluster_type_count == 0:\n",
    "            formatted_cluster_type = f'a {formatted_cluster_type}'\n",
    "        elif i > 0 and same_cluster_type_count == 1:\n",
    "            formatted_cluster_type = f'{random.choice(another_connectors)} {formatted_cluster_type}'\n",
    "        elif i > 0 and same_cluster_type_count > 1:\n",
    "            formatted_cluster_type = f'yet {random.choice(another_connectors)} {formatted_cluster_type}'\n",
    "\n",
    "        if i == 0:\n",
    "            paragraph_connector = f'{random.choice(second_paragraph_connectors)} {random.choice(second_paragraph_verbs)}'\n",
    "        elif i == len(input_clusters_ids) - 1:\n",
    "            paragraph_connector = f'{random.choice(final_paragraph_connectors)} {random.choice(second_paragraph_verbs)}'\n",
    "        else:\n",
    "            paragraph_connector = f'{random.choice(other_paragraphs_connectors)} {random.choice(second_paragraph_verbs)}'\n",
    "\n",
    "\n",
    "        # Translate the temporal information.\n",
    "        #previous_temporal_information = [\n",
    "        #    inf['temporal'] for _, inf in input_clusters_with_information[:i]]\n",
    "        day_sentence = _get_input_day_sentence(\n",
    "            target_temporal_information,\n",
    "            info['temporal'])\n",
    "        time_sentence = _get_time_sentence(info['temporal'])\n",
    "        #previous_temporal_information)\n",
    "\n",
    "        # Translate the location information.\n",
    "        previous_location_information = [\n",
    "            inf['location'] for _, inf in input_clusters_with_information[:i]]\n",
    "        input_location_sentence = _get_target_location_sentence(\n",
    "            info['location'],\n",
    "            previous_location_information\n",
    "            )\n",
    "\n",
    "        # Get the average speed of the target nodes.\n",
    "        input_average_speed = info['average_speed']\n",
    "        \n",
    "        # Get the other paragraph.\n",
    "        other_paragraph = _fill_other_paragraph(\n",
    "            input_cluster_type,\n",
    "            formatted_cluster_type,\n",
    "            paragraph_connector,\n",
    "            time_sentence,\n",
    "            day_sentence,\n",
    "            input_average_speed,\n",
    "            input_location_sentence)\n",
    "\n",
    "        other_paragraphs.append(other_paragraph)\n",
    "\n",
    "    # Get the explanation.\n",
    "    explanation = first_paragraph + '\\n\\n' + '\\n\\n'.join(other_paragraphs)\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A severe congestion was predicted to occur on Glendale Freeway at km 10 on Wednesday, 04/06/2012, from 07:45 to 08:40, with an average speed of 37.00 km/h. The motivation was a series of congestions and free flows.\n",
      "\n",
      "An initial contributing free flow took place, at 107.32 km/h, on Glendale Freeway at kms 8 and 10 from 06:45 to 07:15.\n",
      "\n",
      "Subsequently, an extra contributing free flow occurred, at 101.33 km/h, on, again, Glendale Freeway at kms 7, 8, 10 and 11 from 06:45 to 07:25. The free flow also affected Golden State Freeway at kms 4 and 7 and Ventura Freeway at km 5.\n",
      "\n",
      "After that, a contributing congestion happened on, yet again, Glendale Freeway at kms 8, 9 and 10, with an average speed of 75.95 km/h, from 06:45 to 07:40.\n",
      "\n",
      "After this, yet an additional contributing free flow manifested, with an average speed of 101.91 km/h, on, yet again, Glendale Freeway at kms 10 and 11 occurring from 06:45 to 07:40. The free flow extended on, once more, Golden State Freeway at km 7.\n",
      "\n",
      "Eventually, a contributing severe congestion occurred, averaging at a speed of 36.78 km/h, on, yet another time, Glendale Freeway at km 10 from 07:20 to 07:40.\n"
     ]
    }
   ],
   "source": [
    "print(get_verbal_explanation(sample_x, sample_x_time, clusters, sample_y, sample_y_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
