{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_random_seed(random_seed: int = 42) -> None:\n",
    "    \"\"\"Set the random seed for reproducibility. The seed is set for the random library, the numpy library and the pytorch \n",
    "    library. Moreover the environment variable `TF_DETERMINISTIC_OPS` is set as \"1\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random_seed : int, optional\n",
    "        The random seed to use for reproducibility (default 42).\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/LMissher/STGNN/blob/main/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch_geometric_temporal.dataset.pems_bay import PemsBayDatasetLoader\n",
    "ds = PemsBayDatasetLoader(os.path.join('..', 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "node_values = np.load('../data/pems_node_values.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52105, 325, 2)\n"
     ]
    }
   ],
   "source": [
    "print(node_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "adj = np.load('../data/pems_adj_mat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325, 325)\n"
     ]
    }
   ],
   "source": [
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(node_values, test_size=.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41684, 325, 2) (10421, 325, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_stack(a, stepsize=T):\n",
    "    \n",
    "    n_instances, n_nodes, n_features = a.shape\n",
    "    resulting_batches = n_instances // stepsize\n",
    "    return np.stack(\n",
    "        [a[i * stepsize : i * stepsize + stepsize].reshape(n_nodes, n_features, stepsize)\n",
    "         for i in range(resulting_batches)] )\n",
    "#slided_x_train = sliding_window_view(x_train, 12, axis=0)\n",
    "#x_train, y_train = np.array([x for x in x_train[::T]]), np.array([x for x in x_train[T::T]])\n",
    "x_train = window_stack(x_train)\n",
    "x_test = window_stack(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3473, 325, 2, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = np.array([x for x in x_train[::2]]), np.array([x for x in x_train[1::2]])\n",
    "x_test, y_test = np.array([x for x in x_test[::2]]), np.array([x for x in x_test[1::2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1737, 325, 2, 12) (1736, 325, 2, 12)\n",
      "(434, 325, 2, 12) (434, 325, 2, 12)\n",
      "(1736, 325, 2, 12) (1736, 325, 2, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "x_train = x_train[:-1]\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class S_GNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, A_hat: torch.Tensor, hidden_dim: int = 64) -> None:\n",
    "        super().__init__()\n",
    "        # Module to obtain the latent representation of the input.\n",
    "        # TODO: check if the latent representation is ok\n",
    "        self.latent_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            #nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        # Linear layer to model the spatial feature extraction.\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "        self.A_hat = A_hat\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Get the latent representation of the input.\n",
    "        p = self.latent_encoder(x)\n",
    "\n",
    "        # Apply score function.\n",
    "        score = p @ p.transpose(-1, -2)\n",
    "        # Pair-wise relation between any road node. Note, for stability of exp see https://effectivemachinelearning.com/PyTorch/7._Numerical_stability_in_PyTorch\n",
    "        # TODO: Are the phi in the\n",
    "        #print('exp', torch.relu(score).exp())\n",
    "        score = torch.sigmoid(score)\n",
    "        exp = torch.exp(score - torch.max(score))\n",
    "        R = exp / exp.sum()\n",
    "        \n",
    "        #R = torch.relu(score).exp() / torch.relu(score).exp().sum()\n",
    "        # TODO: A_hat should probably be provided by the model since it is just a refined adjacency matrix, unless we pass the adjacency matrix as an input\n",
    "        #A_hat = p + torch.eye(p.shape[0], p.shape[1], device=x.device)\n",
    "        # Get refined adjacency matrix: A_hat = A + I\n",
    "        #A_hat = torch.rand(x.shape[0], x.shape[0], device=DEVICE) #R # TODO: CHANGE\n",
    "        # Get the sparsified relation matrix\n",
    "        A_hat = self.A_hat.expand_as(R)\n",
    "        #print(A_hat.shape)\n",
    "        #print(R.shape)\n",
    "        R_hat = R * (A_hat > 0).float() \n",
    "        R_hat += torch.eye(R_hat.shape[-2], R_hat.shape[-1], device=x.device)\n",
    "        #print('R_hat', R_hat.shape)\n",
    "        # R_hat = torch.mul((self.A_hat > 0).float(), R) + torch.eye(R.shape[0], R.shape[1], device=x.device) #torch.eye(p.shape[0], p.shape[1], device=x.device)\n",
    "        # Get refined degree matrix for R_hat\n",
    "        D_hat = (R_hat.sum(-1) ** -.5)\n",
    "        #D_hat[torch.isinf(D_hat)] = 0.\n",
    "        D_hat = torch.diag_embed(D_hat)\n",
    "        #print('D-hat', D_hat)\n",
    "\n",
    "        # TODO: handle infinities and nones\n",
    "        A = D_hat @ R_hat @ D_hat\n",
    "\n",
    "        out = torch.relu(self.linear(A @ x))\n",
    "        #print(A, x)\n",
    "        return out\n",
    "\n",
    "'''class GRU(nn.Module):\n",
    "    def __init__(self, n_input_features: int, n_hidden_state_features: int) -> None:\n",
    "        super().__init__()\n",
    "        # Update gate layers.\n",
    "        self.z_x_linear = nn.Linear(n_input_features, n_hidden_state_features)\n",
    "        self.z_h_linear = nn.Linear(n_hidden_state_features, n_hidden_state_features)\n",
    "        # Reset gate layers.\n",
    "        self.r_x_linear = nn.Linear(n_input_features, n_hidden_state_features)\n",
    "        self.r_h_linear = nn.Linear(n_hidden_state_features, n_hidden_state_features)\n",
    "        # State gate layers.\n",
    "        self.h_x_linear = nn.Linear(n_input_features, n_hidden_state_features)\n",
    "        self.h_h_linear = nn.Linear(n_hidden_state_features, n_hidden_state_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor):\n",
    "        # Update Gate.\n",
    "        z_x = self.z_x_linear(x)\n",
    "        z_h = self.z_h_linear(h)\n",
    "        z_t = torch.sigmoid(z_x + z_h)\n",
    "        \n",
    "        # Reset Gate.\n",
    "        r_x = self.r_x_linear(x)\n",
    "        r_h = self.r_h_linear(h)\n",
    "        r_t = torch.sigmoid(r_x + r_h)\n",
    "        \n",
    "        # State gate.\n",
    "        h_x = self.h_x_linear(x)\n",
    "        h_h = self.h_h_linear(h)\n",
    "        h_t = torch.tanh(h_x + r_t * h_h)\n",
    "        \n",
    "        # Get GRU output.\n",
    "        out = (1 - z_t) * h_t + z_t * h\n",
    "        return out'''\n",
    "        \n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, n_input_features: int, n_hidden_state_features: int) -> None:\n",
    "        super().__init__()\n",
    "        # Update gate layers.\n",
    "        self.z_x_linear = nn.Linear(n_input_features, n_hidden_state_features)\n",
    "        self.z_h_linear = nn.Linear(n_hidden_state_features, n_hidden_state_features)\n",
    "        # Reset gate layers.\n",
    "        self.r_x_linear = nn.Linear(n_input_features, n_hidden_state_features)\n",
    "        self.r_h_linear = nn.Linear(n_hidden_state_features, n_hidden_state_features)\n",
    "        # State gate layers.\n",
    "        self.h_x_linear = nn.Linear(n_input_features, n_hidden_state_features)\n",
    "        self.h_h_linear = nn.Linear(n_hidden_state_features, n_hidden_state_features)\n",
    "        \n",
    "        #self.out = nn.Linear(n_hidden_state_features, n_input_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor):\n",
    "        # Update Gate.\n",
    "        z_x = self.z_x_linear(x)\n",
    "        z_h = self.z_h_linear(h)\n",
    "        z_t = torch.sigmoid(z_x + z_h)\n",
    "        \n",
    "        # Reset Gate.\n",
    "        r_x = self.r_x_linear(x)\n",
    "        r_h = self.r_h_linear(h)\n",
    "        r_t = torch.sigmoid(r_x + r_h)\n",
    "        \n",
    "        # State gate.\n",
    "        h_x = self.h_x_linear(x)\n",
    "        h_h = self.h_h_linear(h)\n",
    "        h_t = torch.tanh(h_x + r_t * h_h)\n",
    "        \n",
    "        # Get GRU output.\n",
    "        out = (1 - z_t) * h_t + z_t * h\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_nodes: int, len_timeseries: int, hidden_dimension: int, n_heads: int = 4) -> None:\n",
    "        super().__init__()\n",
    "        #_, n_features = input_size\n",
    "        self.queries_linear = nn.Linear(len_timeseries, len_timeseries)\n",
    "        self.keys_linear = nn.Linear(len_timeseries, len_timeseries)\n",
    "        self.values_linear = nn.Linear(len_timeseries, len_timeseries)\n",
    "        #self.multi_head_attention = nn.MultiheadAttention(n_features, n_heads)\n",
    "        self.multi_head_attention_list = nn.ModuleList([nn.MultiheadAttention(len_timeseries, n_heads) for _ in range(n_nodes)])\n",
    "        self.normalization = nn.LayerNorm(len_timeseries)\n",
    "        self.normalization_out = nn.LayerNorm(len_timeseries)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(len_timeseries, hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension, len_timeseries)\n",
    "        )\n",
    "        \n",
    "    #def _attention(Q, K, V):\n",
    "    #    d = K.shape[-1]\n",
    "    #    return torch.softmax(((Q @ K.transpose(1, 0)) / d ** .5) @ V)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Get queries, keys and values.\n",
    "        #Q = x.clone() #self.queries_linear(x)\n",
    "        #K = x.clone() #self.keys_linear(x)\n",
    "        #V = x.clone() #self.values_linear(x)\n",
    "        Q = self.queries_linear(x)\n",
    "        K = self.keys_linear(x)\n",
    "        V = self.values_linear(x)\n",
    "        \n",
    "        # Multi head attention mechanism.\n",
    "        out = []\n",
    "        for i in range(x.shape[1]):\n",
    "            out_, _ = self.multi_head_attention_list[i](Q[:, i], K[:, i], V[:, i])\n",
    "            out.append(out_)\n",
    "        out = torch.stack(out, 1)\n",
    "        \n",
    "        # Apply residual connection and batch normalization.\n",
    "        out += x\n",
    "        norm = self.normalization(out)\n",
    "        \n",
    "        # Apply feed forward module.\n",
    "        out = self.feed_forward(norm)\n",
    "        \n",
    "        # Apply residual connection and batch normalization.\n",
    "        out += norm\n",
    "        return self.normalization_out(out)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_features: int, len_timeseries: int) -> None:\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(n_features, len_timeseries)\n",
    "        \n",
    "        position = torch.arange(len_timeseries)#.unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(n_features) * (-math.log(10000.0) / n_features)).unsqueeze(1)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:, 0::2]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:, 1::2]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.autograd.Variable(self.pe, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalGNN(nn.Module):\n",
    "    def __init__(self, n_nodes, n_features, len_timeseries, adj: np.ndarray, device: str, hidden_dimension = 64):\n",
    "        super().__init__()\n",
    "        adj = torch.tensor(adj, dtype=torch.float32, requires_grad=False, device=device)\n",
    "        A_hat = adj + torch.eye(adj.shape[0], adj.shape[1], device=device)\n",
    "        # self.start_emb = nn.Linear(infea, outfea)\n",
    "        # self.end_emb = nn.Linear(outfea, infea)\n",
    "        self.s_gnns = nn.ModuleList([S_GNN(n_features, A_hat) for _ in range(len_timeseries)])\n",
    "        self.hidden_s_gnns = nn.ModuleList([S_GNN(hidden_dimension, A_hat) for _ in range(len_timeseries -1)])\n",
    "        self.grus = nn.ModuleList([GRU(n_features, hidden_dimension) for _ in range(len_timeseries)])\n",
    "        self.decoders = nn.ModuleList([nn.Linear(hidden_dimension, n_features) for _ in range(len_timeseries)])\n",
    "        self.positional_encoding = PositionalEncoding(n_features, len_timeseries)\n",
    "        #self.transformers = nn.ModuleList([Transformer(n_nodes, hidden_dimension, hidden_dimension=hidden_dimension) for _ in range(len_timeseries)])\n",
    "        self.transformer = Transformer(n_nodes, len_timeseries, hidden_dimension=hidden_dimension)\n",
    "        \n",
    "        self.prediction_layer = nn.Sequential(\n",
    "            nn.Linear(len_timeseries, len_timeseries),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(len_timeseries, len_timeseries)\n",
    "            #nn.Upsample(size=(1, output_graphs, n_nodes, n_features))\n",
    "        )\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_features = n_features\n",
    "        self.len_timeseries = len_timeseries\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.unsqueeze(-1)\n",
    "        # x = self.start_emb(x)\n",
    "        sgnn_outs = []\n",
    "        for i in range(self.len_timeseries):\n",
    "            timestamp = x[:, :, :, i]\n",
    "            #print(timestamp.shape)\n",
    "            x_ = self.s_gnns[i](timestamp)\n",
    "            if i > 0:\n",
    "                hidden_state = sgnn_outs[i-1] \n",
    "            else:\n",
    "                batch_size, n_nodes, n_features, _ = x.shape\n",
    "                hidden_state = torch.zeros(\n",
    "                    (batch_size, n_nodes, self.hidden_dimension),\n",
    "                    device=DEVICE)\n",
    "            x_ = self.grus[i](x_, hidden_state)\n",
    "            sgnn_outs.append(x_)\n",
    "            if i < self.len_timeseries:\n",
    "                self.hidden_s_gnns[i-1](x_)\n",
    "        \n",
    "        #TODO: see how to handle this \n",
    "        sgnn_outs = [self.decoders[i](x_) for i, x_ in enumerate(sgnn_outs)]\n",
    "                \n",
    "        #x = self.positional_encoding(x)\n",
    "        \n",
    "        # TODO: stack row-wise and pass to the transformer\n",
    "        #print(sgnn_outs[0].shape)\n",
    "        # Stack the GRU outputs row-wise for each node\n",
    "        out = torch.stack(sgnn_outs, -1)\n",
    "        #print(out.shape)\n",
    "        out = self.positional_encoding(out)\n",
    "        # TODO: The weights of the transformer seem to be shared, pass just subsets of nodes to a single layer.\n",
    "        #transformer_outs = []\n",
    "        #for i in range(self.len_timeseries):\n",
    "        #    transformer_outs.append(self.transformers[i](out[:, i]))\n",
    "        #out = torch.stack(transformer_outs, 1)\n",
    "        out = self.transformer(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        out = self.prediction_layer(out)\n",
    "        #print('in shape', x.shape)\n",
    "        #print('out shape', out.shape)\n",
    "        #print(out.shape)\n",
    "        #print(out.squeeze(-1).shape)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpatioTemporalGNN(325, 2, T, adj, DEVICE) #.to(DEVICE) #STGNN(2, 4*16, 1, 16).to(device) #SpatioTemporalGNN(325, 2, T, adj, DEVICE)#.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatioTemporalGNN(\n",
       "  (s_gnns): ModuleList(\n",
       "    (0): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (1): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (2): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (3): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (4): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (5): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (6): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (7): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (8): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (9): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (10): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (11): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (hidden_s_gnns): ModuleList(\n",
       "    (0): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (2): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (3): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (4): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (5): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (6): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (7): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (8): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (9): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (10): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (grus): ModuleList(\n",
       "    (0): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (2): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (3): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (4): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (5): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (6): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (7): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (8): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (9): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (10): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (11): GRU(\n",
       "      (z_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (r_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (h_x_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (5): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (6): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (7): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (9): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (10): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (11): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer): Transformer(\n",
       "    (queries_linear): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (keys_linear): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (values_linear): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (multi_head_attention_list): ModuleList(\n",
       "      (0): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (1): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (2): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (3): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (4): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (5): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (6): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (7): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (8): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (9): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (10): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (11): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (12): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (13): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (14): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (15): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (16): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (17): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (18): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (19): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (20): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (21): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (22): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (23): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (24): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (25): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (26): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (27): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (28): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (29): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (30): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (31): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (32): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (33): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (34): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (35): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (36): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (37): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (38): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (39): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (40): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (41): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (42): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (43): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (44): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (45): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (46): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (47): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (48): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (49): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (50): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (51): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (52): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (53): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (54): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (55): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (56): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (57): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (58): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (59): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (60): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (61): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (62): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (63): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (64): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (65): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (66): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (67): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (68): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (69): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (70): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (71): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (72): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (73): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (74): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (75): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (76): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (77): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (78): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (79): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (80): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (81): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (82): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (83): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (84): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (85): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (86): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (87): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (88): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (89): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (90): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (91): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (92): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (93): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (94): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (95): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (96): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (97): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (98): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (99): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (100): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (101): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (102): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (103): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (104): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (105): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (106): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (107): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (108): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (109): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (110): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (111): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (112): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (113): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (114): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (115): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (116): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (117): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (118): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (119): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (120): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (121): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (122): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (123): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (124): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (125): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (126): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (127): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (128): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (129): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (130): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (131): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (132): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (133): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (134): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (135): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (136): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (137): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (138): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (139): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (140): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (141): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (142): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (143): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (144): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (145): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (146): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (147): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (148): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (149): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (150): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (151): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (152): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (153): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (154): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (155): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (156): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (157): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (158): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (159): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (160): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (161): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (162): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (163): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (164): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (165): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (166): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (167): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (168): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (169): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (170): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (171): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (172): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (173): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (174): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (175): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (176): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (177): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (178): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (179): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (180): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (181): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (182): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (183): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (184): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (185): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (186): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (187): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (188): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (189): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (190): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (191): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (192): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (193): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (194): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (195): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (196): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (197): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (198): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (199): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (200): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (201): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (202): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (203): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (204): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (205): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (206): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (207): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (208): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (209): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (210): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (211): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (212): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (213): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (214): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (215): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (216): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (217): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (218): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (219): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (220): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (221): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (222): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (223): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (224): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (225): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (226): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (227): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (228): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (229): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (230): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (231): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (232): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (233): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (234): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (235): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (236): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (237): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (238): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (239): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (240): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (241): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (242): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (243): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (244): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (245): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (246): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (247): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (248): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (249): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (250): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (251): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (252): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (253): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (254): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (255): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (256): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (257): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (258): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (259): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (260): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (261): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (262): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (263): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (264): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (265): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (266): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (267): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (268): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (269): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (270): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (271): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (272): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (273): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (274): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (275): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (276): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (277): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (278): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (279): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (280): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (281): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (282): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (283): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (284): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (285): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (286): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (287): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (288): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (289): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (290): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (291): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (292): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (293): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (294): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (295): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (296): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (297): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (298): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (299): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (300): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (301): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (302): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (303): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (304): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (305): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (306): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (307): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (308): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (309): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (310): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (311): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (312): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (313): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (314): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (315): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (316): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (317): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (318): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (319): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (320): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (321): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (322): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (323): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (324): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (normalization): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "    (normalization_out): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prediction_layer): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=12, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader, Dataset, T_co\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = x.shape[0]\n",
    "    def __getitem__(self, index) -> T_co:\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self) -> int:\n",
    "        return self.len\n",
    "\n",
    "train_set = TimeSeriesDataset(x_train, y_train)\n",
    "#test_set = TimeSeriesDataset(x_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model(next(iter(train_dataloader))[0].float().to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "def visualize_network(model: SpatioTemporalGNN, dataloader: DataLoader, device: str):\n",
    "    torch.cuda.empty_cache()\n",
    "    x, y = next(iter(dataloader))\n",
    "    x = x.type(torch.float32).to(device=device)\n",
    "    #y = y.type(torch.float32).to(device=device)\n",
    "    y_hat = model(x)\n",
    "    make_dot(y_hat.mean(), params=dict(model.named_modules())).render('Spatial Temporal GNN', format='png')\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#visualize_network(model, train_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import Callable\n",
    "\n",
    "# create a nn class (just-for-fun choice :-) \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        r = torch.sqrt(self.mse(yhat,y))\n",
    "        #print(r)\n",
    "        return(r)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "# loss = criterion(yhat,y)\n",
    "\n",
    "def loss(y_pred, y_true):\n",
    "    # TODO: transform that in Mean absolute error per timeseries, dim=...\n",
    "    l = torch.linalg.norm(y_pred - y_true, ord=1, dim=-2)\n",
    "    return torch.mean(l)\n",
    "\n",
    "def train(train_dataloader: DataLoader, model: SpatioTemporalGNN, \n",
    "          optimizer: torch.optim.Optimizer, lr_scheduler, loss_function: Callable, device: str, epochs: int = 10) -> None:\n",
    "          #steps_validate: int = 100, checkpoint: Optional[Checkpoint] = None, early_stopping: Optional[EarlyStopping] = None, \n",
    "          #reload_best_weights: bool = True) -> None:\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_f1_macro_history = []\n",
    "    \n",
    "    rmse_loss = RMSELoss()\n",
    "\n",
    "    # Total steps to perform\n",
    "    # tot_steps = len(train_dataloader) * epochs\n",
    "    # Number of step already done\n",
    "    n_steps = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # Iterate across the epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set up display element\n",
    "        #disp = display('', display_id=True)\n",
    "\n",
    "        # Remove unused tensors from gpu memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Initialize running losses\n",
    "        running_loss = 0.0\n",
    "        running_rmse = 0.0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        # Number of batches for the current update step\n",
    "        # batch_steps = 0\n",
    "\n",
    "        for batch_idx, data in enumerate(train_dataloader, 0):\n",
    "            # Increment the number of batch steps\n",
    "            batch_steps = batch_idx + 1\n",
    "\n",
    "            # Get the data\n",
    "            x, y = data\n",
    "            x = x.type(torch.float32).to(device=device)\n",
    "            y = y.type(torch.float32).to(device=device)\n",
    "\n",
    "            # Compute output\n",
    "            output = model(x)\n",
    "            \n",
    "            # Loss\n",
    "            loss = loss_function(output, y)\n",
    "            rmse = rmse_loss(output, y)\n",
    "            running_loss += loss.item()\n",
    "            running_rmse += rmse.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #nb_tr_steps += 1\n",
    "            loss.backward()\n",
    "            \n",
    "            # When using GPU\n",
    "            optimizer.step()\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            '''if batch_idx % steps_validate == steps_validate - 1:\n",
    "                model.eval()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # Compute both the token importances validation loss and the answer generation validation loss\n",
    "                val_loss, val_f1_macro = _loss_validate(model, val_dataloader, loss_function, device)\n",
    "                \n",
    "                # Update validation loss history\n",
    "                val_loss_history.append([n_steps, val_loss.item()])\n",
    "                val_f1_macro_history.append([n_steps, val_f1_macro])\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                if checkpoint is not None:\n",
    "                    checkpoint.save_best(val_f1_macro, train_loss_history=train_loss_history,\n",
    "                                         val_loss_history=val_loss_history, val_f1_macro_history=val_f1_macro_history)\n",
    "                if early_stopping is not None:\n",
    "                    early_stopping.update(val_f1_macro)\n",
    "                    if early_stopping.is_stop_condition_met():\n",
    "                        print('Early stopping')\n",
    "                        return train_loss_history, val_loss_history, val_f1_macro_history\n",
    "\n",
    "                model.train()'''\n",
    "\n",
    "\n",
    "            # Update training history and print           \n",
    "            train_loss_history.append(loss.detach().cpu())\n",
    "            \n",
    "            epoch_time = time() - start_time\n",
    "            batch_time = epoch_time / batch_steps\n",
    "            \n",
    "            # TODO: function to print batch string\n",
    "            print(\n",
    "                f'epoch: {epoch + 1}/{epochs}',\n",
    "                f'[{batch_steps}/{len(train_dataloader)}],',\n",
    "                f'{epoch_time:.0f}s {batch_time * 1e3:.0f}ms/step,',\n",
    "                #f'lr base: {optimizer.param_groups[0][\"lr\"]:.3g} lr head: {optimizer.param_groups[1][\"lr\"]:.3g}, ' +\n",
    "                f'loss: {running_loss / batch_steps:.3g},',\n",
    "                f'RMSE: {running_rmse / batch_steps:.3g},',\n",
    "                f'lr: {optimizer.param_groups[0][\"lr\"]:.3g} weight decay: {optimizer.param_groups[0][\"weight_decay\"]}',\n",
    "                '               ' if batch_steps < len(train_dataloader) else '',\n",
    "                end='\\r' if batch_steps < len(train_dataloader) else None,\n",
    "                )\n",
    "\n",
    "            n_steps += 1\n",
    "\n",
    "        model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Compute both the token importances validation loss and the answer generation validation loss\n",
    "        #val_loss, val_f1_macro = _loss_validate(model, val_dataloader, loss_function, device, print_result=False)\n",
    "        # Update validation loss history\n",
    "        #val_loss_history.append([n_steps, val_loss.item()])\n",
    "        #val_f1_macro_history.append([n_steps, val_f1_macro])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "        print(\n",
    "            f'epoch: {epoch + 1}/{epochs},',\n",
    "            f'{epoch_time:.0f}s,',\n",
    "            f'train loss: {running_loss / batch_steps:.3g},',\n",
    "            f'train RMSE: {running_rmse / batch_steps:.3g},',\n",
    "            f'lr: {optimizer.param_groups[0][\"lr\"]:.3g} weight decay: {optimizer.param_groups[0][\"weight_decay\"]}'\n",
    "            #f'loss: {running_loss / batch_steps:.3g} val loss:, {val_loss.mean():.3g}, ' + \n",
    "            #f'val f1 macro: {val_f1_macro * 100:.3g} %'\n",
    "            )\n",
    "        print('===============================================================================================')\n",
    "        \n",
    "        '''if checkpoint is not None:\n",
    "            checkpoint.save_best(val_f1_macro, train_loss_history=train_loss_history, val_loss_history=val_loss_history,\n",
    "                            val_f1_macro_history=val_f1_macro_history)\n",
    "\n",
    "        if early_stopping is not None:\n",
    "            early_stopping.update(val_f1_macro)\n",
    "            if early_stopping.is_stop_condition_met():\n",
    "                print('Early stopping')\n",
    "                return train_loss_history, val_loss_history, val_f1_macro_history''';\n",
    "        model.train()\n",
    "        lr_scheduler.step(running_rmse / batch_steps)\n",
    "\n",
    "    '''if checkpoint is not None and reload_best_weights:\n",
    "        _ = checkpoint.load_best()''';\n",
    "\n",
    "    model.eval()\n",
    "    return np.array(train_loss_history), np.array(val_loss_history), np.array(val_f1_macro_history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "print(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=False, threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=2e-6, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1000 [28/28], 109s 3897ms/step, loss: 60, RMSE: 42.8, lr: 0.01 weight decay: 0.01 1                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 1/1000, 109s, train loss: 60, train RMSE: 42.8, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 2/1000 [28/28], 106s 3789ms/step, loss: 31.9, RMSE: 23.3, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 2/1000, 106s, train loss: 31.9, train RMSE: 23.3, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 3/1000 [28/28], 106s 3790ms/step, loss: 6.02, RMSE: 7.29, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 3/1000, 106s, train loss: 6.02, train RMSE: 7.29, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 4/1000 [28/28], 106s 3802ms/step, loss: 5.24, RMSE: 6.94, lr: 0.01 weight decay: 0.01               \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 4/1000, 106s, train loss: 5.24, train RMSE: 6.94, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 5/1000 [28/28], 107s 3827ms/step, loss: 5.2, RMSE: 6.92, lr: 0.01 weight decay: 0.01                 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 5/1000, 107s, train loss: 5.2, train RMSE: 6.92, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 6/1000 [28/28], 106s 3778ms/step, loss: 5.15, RMSE: 6.9, lr: 0.01 weight decay: 0.01                 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 6/1000, 106s, train loss: 5.15, train RMSE: 6.9, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 7/1000 [28/28], 106s 3791ms/step, loss: 5.04, RMSE: 6.72, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 7/1000, 106s, train loss: 5.04, train RMSE: 6.72, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 8/1000 [28/28], 107s 3836ms/step, loss: 4.87, RMSE: 6.52, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 8/1000, 107s, train loss: 4.87, train RMSE: 6.52, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 9/1000 [28/28], 108s 3865ms/step, loss: 4.73, RMSE: 6.26, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 9/1000, 108s, train loss: 4.73, train RMSE: 6.26, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 10/1000 [28/28], 105s 3760ms/step, loss: 4.68, RMSE: 6.18, lr: 0.01 weight decay: 0.01               \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 10/1000, 105s, train loss: 4.68, train RMSE: 6.18, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 11/1000 [28/28], 106s 3797ms/step, loss: 4.73, RMSE: 6.21, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 11/1000, 106s, train loss: 4.73, train RMSE: 6.21, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 12/1000 [28/28], 108s 3847ms/step, loss: 4.63, RMSE: 6.08, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 12/1000, 108s, train loss: 4.63, train RMSE: 6.08, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 13/1000 [28/28], 107s 3813ms/step, loss: 4.54, RMSE: 5.99, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 13/1000, 107s, train loss: 4.54, train RMSE: 5.99, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 14/1000 [28/28], 105s 3743ms/step, loss: 4.49, RMSE: 5.94, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 14/1000, 105s, train loss: 4.49, train RMSE: 5.94, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 15/1000 [28/28], 105s 3759ms/step, loss: 4.42, RMSE: 5.86, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 15/1000, 105s, train loss: 4.42, train RMSE: 5.86, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 16/1000 [28/28], 80s 2864ms/step, loss: 4.48, RMSE: 5.98, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 16/1000, 80s, train loss: 4.48, train RMSE: 5.98, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 17/1000 [28/28], 36s 1296ms/step, loss: 4.41, RMSE: 5.85, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 17/1000, 36s, train loss: 4.41, train RMSE: 5.85, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 18/1000 [28/28], 37s 1328ms/step, loss: 4.29, RMSE: 5.74, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 18/1000, 37s, train loss: 4.29, train RMSE: 5.74, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 19/1000 [28/28], 37s 1319ms/step, loss: 4.31, RMSE: 5.78, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 19/1000, 37s, train loss: 4.31, train RMSE: 5.78, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 20/1000 [28/28], 37s 1304ms/step, loss: 4.33, RMSE: 5.87, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 20/1000, 37s, train loss: 4.33, train RMSE: 5.87, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 21/1000 [28/28], 38s 1352ms/step, loss: 4.43, RMSE: 5.95, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 21/1000, 38s, train loss: 4.43, train RMSE: 5.95, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 22/1000 [28/28], 37s 1314ms/step, loss: 4.41, RMSE: 5.91, lr: 0.01 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 22/1000, 37s, train loss: 4.41, train RMSE: 5.91, lr: 0.01 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 23/1000 [28/28], 37s 1315ms/step, loss: 4.22, RMSE: 5.74, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 23/1000, 37s, train loss: 4.22, train RMSE: 5.74, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 24/1000 [28/28], 37s 1313ms/step, loss: 4.21, RMSE: 5.73, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 24/1000, 37s, train loss: 4.21, train RMSE: 5.73, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 25/1000 [28/28], 37s 1319ms/step, loss: 4.16, RMSE: 5.69, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 25/1000, 37s, train loss: 4.16, train RMSE: 5.69, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 26/1000 [28/28], 37s 1334ms/step, loss: 4.14, RMSE: 5.66, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 26/1000, 37s, train loss: 4.14, train RMSE: 5.66, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 27/1000 [28/28], 37s 1310ms/step, loss: 4.12, RMSE: 5.62, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 27/1000, 37s, train loss: 4.12, train RMSE: 5.62, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 28/1000 [28/28], 37s 1326ms/step, loss: 4.11, RMSE: 5.61, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 28/1000, 37s, train loss: 4.11, train RMSE: 5.61, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 29/1000 [28/28], 37s 1314ms/step, loss: 4.11, RMSE: 5.64, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 29/1000, 37s, train loss: 4.11, train RMSE: 5.64, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 30/1000 [28/28], 37s 1323ms/step, loss: 4.11, RMSE: 5.63, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 30/1000, 37s, train loss: 4.11, train RMSE: 5.63, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 31/1000 [28/28], 37s 1318ms/step, loss: 4.13, RMSE: 5.66, lr: 0.001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 31/1000, 37s, train loss: 4.13, train RMSE: 5.66, lr: 0.001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 32/1000 [28/28], 37s 1321ms/step, loss: 4.14, RMSE: 5.67, lr: 0.0001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 32/1000, 37s, train loss: 4.14, train RMSE: 5.67, lr: 0.0001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 33/1000 [28/28], 37s 1307ms/step, loss: 4.08, RMSE: 5.58, lr: 0.0001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 33/1000, 37s, train loss: 4.08, train RMSE: 5.58, lr: 0.0001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 34/1000 [28/28], 37s 1328ms/step, loss: 4.11, RMSE: 5.63, lr: 0.0001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 34/1000, 37s, train loss: 4.11, train RMSE: 5.63, lr: 0.0001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 35/1000 [28/28], 37s 1305ms/step, loss: 4.1, RMSE: 5.61, lr: 0.0001 weight decay: 0.01                 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 35/1000, 37s, train loss: 4.1, train RMSE: 5.61, lr: 0.0001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 36/1000 [28/28], 37s 1329ms/step, loss: 4.13, RMSE: 5.64, lr: 0.0001 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 36/1000, 37s, train loss: 4.13, train RMSE: 5.64, lr: 0.0001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 37/1000 [28/28], 37s 1320ms/step, loss: 4.17, RMSE: 5.7, lr: 0.0001 weight decay: 0.01                 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 37/1000, 37s, train loss: 4.17, train RMSE: 5.7, lr: 0.0001 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 38/1000 [28/28], 37s 1321ms/step, loss: 4.1, RMSE: 5.63, lr: 1e-05 weight decay: 0.01                 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 38/1000, 37s, train loss: 4.1, train RMSE: 5.63, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 39/1000 [28/28], 38s 1340ms/step, loss: 4.08, RMSE: 5.57, lr: 1e-05 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 39/1000, 38s, train loss: 4.08, train RMSE: 5.57, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 40/1000 [28/28], 37s 1318ms/step, loss: 4.13, RMSE: 5.65, lr: 1e-05 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 40/1000, 37s, train loss: 4.13, train RMSE: 5.65, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 41/1000 [28/28], 37s 1338ms/step, loss: 4.11, RMSE: 5.63, lr: 1e-05 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 41/1000, 37s, train loss: 4.11, train RMSE: 5.63, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 42/1000 [28/28], 37s 1310ms/step, loss: 4.1, RMSE: 5.62, lr: 1e-05 weight decay: 0.01                 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 42/1000, 37s, train loss: 4.1, train RMSE: 5.62, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 43/1000 [28/28], 37s 1314ms/step, loss: 4.06, RMSE: 5.56, lr: 1e-05 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 43/1000, 37s, train loss: 4.06, train RMSE: 5.56, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 44/1000 [28/28], 37s 1312ms/step, loss: 4.11, RMSE: 5.63, lr: 1e-05 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 44/1000, 37s, train loss: 4.11, train RMSE: 5.63, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 45/1000 [28/28], 37s 1333ms/step, loss: 4.08, RMSE: 5.6, lr: 1e-05 weight decay: 0.01                 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 45/1000, 37s, train loss: 4.08, train RMSE: 5.6, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 46/1000 [28/28], 37s 1310ms/step, loss: 4.09, RMSE: 5.59, lr: 1e-05 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 46/1000, 37s, train loss: 4.09, train RMSE: 5.59, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 47/1000 [28/28], 37s 1315ms/step, loss: 4.09, RMSE: 5.61, lr: 1e-05 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 47/1000, 37s, train loss: 4.09, train RMSE: 5.61, lr: 1e-05 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 48/1000 [28/28], 37s 1318ms/step, loss: 4.07, RMSE: 5.57, lr: 2e-06 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 48/1000, 37s, train loss: 4.07, train RMSE: 5.57, lr: 2e-06 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 49/1000 [28/28], 37s 1333ms/step, loss: 4.14, RMSE: 5.67, lr: 2e-06 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 49/1000, 37s, train loss: 4.14, train RMSE: 5.67, lr: 2e-06 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 50/1000 [28/28], 37s 1335ms/step, loss: 4.11, RMSE: 5.63, lr: 2e-06 weight decay: 0.01                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "epoch: 50/1000, 37s, train loss: 4.11, train RMSE: 5.63, lr: 2e-06 weight decay: 0.01\n",
      "===============================================================================================\n",
      "epoch: 51/1000 [6/28], 8s 1344ms/step, loss: 4.1, RMSE: 5.65, lr: 2e-06 weight decay: 0.01                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Spatial Temporal GNN.ipynb Cell 33\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, lr_scheduler\u001b[39m=\u001b[39;49mlr_scheduler, loss_function\u001b[39m=\u001b[39;49mloss, device\u001b[39m=\u001b[39;49mDEVICE, epochs\u001b[39m=\u001b[39;49m\u001b[39m1_000\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Spatial Temporal GNN.ipynb Cell 33\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Compute output\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m output \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(output, y)\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Spatial Temporal GNN.ipynb Cell 33\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m timestamp \u001b[39m=\u001b[39m x[:, :, :, i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#print(timestamp.shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m x_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ms_gnns[i](timestamp)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     hidden_state \u001b[39m=\u001b[39m sgnn_outs[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Spatial Temporal GNN.ipynb Cell 33\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#print('D-hat', D_hat)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# TODO: handle infinities and nones\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m A \u001b[39m=\u001b[39m D_hat \u001b[39m@\u001b[39m R_hat \u001b[39m@\u001b[39m D_hat\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(A \u001b[39m@\u001b[39;49m x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m#print(A, x)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Spatial%20Temporal%20GNN.ipynb#X44sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1184\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1181\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[0;32m   1182\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m-> 1184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1185\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[0;32m   1186\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataloader=train_dataloader, model=model, optimizer=optimizer, lr_scheduler=lr_scheduler, loss_function=loss, device=DEVICE, epochs=1_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bb5e395f71e970ef2dfa88b10e29155c2f154fbffe5a547ccb4cc942724aa68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
