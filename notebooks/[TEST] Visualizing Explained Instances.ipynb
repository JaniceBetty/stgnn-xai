{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.explanation.navigator.model import Navigator\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the STGNN in evaluation mode.\n",
    "spatial_temporal_gnn.eval();\n",
    "\n",
    "# Get the Navigator and load the checkpoints.\n",
    "navigator = Navigator(DEVICE)\n",
    "\n",
    "navigator_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                          'navigator_metr_la.pth')\n",
    "\n",
    "navigator_checkpoints = torch.load(navigator_checkpoints_path)\n",
    "navigator.load_state_dict(navigator_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the Navigator in evaluation mode.\n",
    "navigator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the data scaler.\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the explained data.\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test.npy'))\n",
    "\n",
    "# Get the time information of the explained data.\n",
    "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test_time.npy'))\n",
    "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(BASE_DATA_DIR, 'structured', 'node_locations.pkl'), 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "sample_x, sample_y, sample_x_time, sample_y_time = x_test[i], y_test[i], x_test_time[i], y_test_time[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n_timesteps, n_nodes, _ = y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_adjacency_distance_matrix)\n",
    "\n",
    "adj_distance_matrix = get_adjacency_distance_matrix(adj_matrix, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_temporal_distance_matrix)\n",
    "\n",
    "temporal_distance_matrix = get_temporal_distance_matrix(n_nodes, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best parameters based on the results of the grid search.\n",
    "\n",
    "EPS = .5\n",
    "MIN_SAMPLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_clusters(\n",
    "    instance: np.ndarray,\n",
    "    adj_distance_matrix: np.ndarray,\n",
    "    temporal_distance_matrix: np.ndarray,\n",
    "    speed_distance_weight: float = 2,\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the clusters of the given instance using the DBSCAN algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance : ndarray\n",
    "        The spatial-temporal graph instance to cluster.\n",
    "    adj_distance_matrix : ndarray\n",
    "        The adjacency matrix of the nodes in the graph measured in distance\n",
    "        between 0 and 1.\n",
    "    temporal_distance_matrix : ndarray\n",
    "        The matrix measuring the distance between the time steps\n",
    "        of the nodes in the graph between 0 and 1.\n",
    "    eps : float\n",
    "        The maximum distance between two samples for one to be considered\n",
    "        as in the neighborhood of the other.\n",
    "    min_samples : int\n",
    "        The number of samples in a neighborhood for a point for it to be\n",
    "        considered as a core point.\n",
    "    speed_distance_weight : float, optional\n",
    "        The weight of the speed distance in the clustering process,\n",
    "        by default 3.\n",
    "    remove_zeros : bool, optional\n",
    "        Whether to remove the nodes that are not present in the instance\n",
    "        during the clustering process, by default False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        The clusters of the given instance.\n",
    "    \"\"\"\n",
    "    n_timesteps, n_nodes, _ = instance.shape\n",
    "\n",
    "    # Reshape the instance to be a column vector.\n",
    "    reshaped_instance = instance.reshape(-2, 1)\n",
    "\n",
    "    # Compute the distance matrix between the speed of the nodes in the graph.\n",
    "    speed_distance_matrix = cdist(reshaped_instance, reshaped_instance, 'euclidean')\n",
    "    # Normalize the distance matrix between 0 and 1.\n",
    "    speed_distance_matrix /= np.max(speed_distance_matrix)\n",
    "\n",
    "    # Compute the weighted distance matrix between the nodes in the graph\n",
    "    # in terms of speed and the spatial and temporal distances.\n",
    "    distance_matrix = speed_distance_matrix * speed_distance_weight +\\\n",
    "        adj_distance_matrix + temporal_distance_matrix\n",
    "\n",
    "    # Normalize the distance matrix between 0 and 1.\n",
    "    #distance_matrix /= np.max(distance_matrix)\n",
    "    # Set the distance between nodes that are not connected to an\n",
    "    # unreachable value.\n",
    "    distance_matrix[adj_distance_matrix == 1] = 1_000\n",
    "    \n",
    "    # Set the distance between nodes that are not present in the instance\n",
    "    # to an unreachable value.\n",
    "    non_zeros = np.where(reshaped_instance != 0)[0]\n",
    "    # Reduce distance matrix by solely considering the nodes that are\n",
    "    # present in the instance.\n",
    "    distance_matrix = distance_matrix[non_zeros, :][:, non_zeros]\n",
    "\n",
    "    # Compute the clusters of the given instance using the DBSCAN algorithm.\n",
    "    kmedoid = KMedoids(metric='precomputed', n_clusters=5, max_iter=100_000, init='k-medoids++')\n",
    "    clusters = kmedoid.fit_predict(distance_matrix)\n",
    "\n",
    "    # Add a dummy dimension to the clusters array.\n",
    "    clusters = np.expand_dims(clusters, axis=1)\n",
    "\n",
    "    # Create a cluster vector with dummy -1 values.\n",
    "    clusters_vector = np.full_like(reshaped_instance, -1)\n",
    "    clusters_vector[non_zeros] = clusters[:]\n",
    "\n",
    "    # Reshape the clusters array to have the same shape as the instance.\n",
    "    clusters_vector = clusters_vector.reshape(n_timesteps, n_nodes, 1)\n",
    "    \n",
    "    clusters_vector = clusters_vector.astype(int)\n",
    "\n",
    "    return clusters_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn_extra\\cluster\\_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clusters_x = get_clusters(sample_x[..., 0:1], adj_distance_matrix, temporal_distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from src.explanation.clustering.clustering import get_clusters\\n\\nclusters_x = get_clusters(\\n    sample_x[..., :1],\\n    adj_distance_matrix,\\n    temporal_distance_matrix,\\n    eps=EPS,\\n    min_samples=MIN_SAMPLES,\\n    remove_zeros=True)'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from src.explanation.clustering.clustering import get_clusters\n",
    "\n",
    "clusters_x = get_clusters(\n",
    "    sample_x[..., :1],\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    eps=EPS,\n",
    "    min_samples=MIN_SAMPLES,\n",
    "    remove_zeros=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0, ..., 11, 11, 11], dtype=int64),\n",
       " array([  0,   0,   1, ..., 205, 206, 206], dtype=int64),\n",
       " array([1, 2, 1, ..., 2, 1, 2], dtype=int64))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_x = clusters_x.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, 0, 1, 2, 3, 4], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(clusters_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.unique(clusters_x):\n",
    "    if c == -1:\n",
    "        clusters_x[clusters_x == c] = ' '\n",
    "    else:\n",
    "        clusters_x[clusters_x == c] = f'cluster {c}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_y = (sample_y > 0).astype(np.int64).astype(object)\n",
    "clusters_y[clusters_y == 0] = ' '\n",
    "clusters_y[clusters_y == 1] = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_x.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.analyisis import get_node_values_with_clusters_and_location_dataframe\n",
    "\n",
    "df = get_node_values_with_clusters_and_location_dataframe(sample_x[..., 0:1], clusters_x, node_pos_dict, locations_df, sample_x_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', 'cluster 1', 'cluster 0', 'cluster 2', 'cluster 4',\n",
       "       'cluster 3'], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "icons = {\n",
    "    ' ': 'cancel',\n",
    "    'cluster 0': 'star',\n",
    "    'cluster 1': 'circle',\n",
    "    'cluster 2':'heart',\n",
    "    'cluster 3': 'play',\n",
    "    'cluster 4': 'pause',\n",
    "    'target': 'certified'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['icon'] = df['cluster'].apply(lambda x: icons[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8cdd7f1c9f4ee489896ca1d9a8f9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(config={'version': 'v1', 'config': {'visState': {'filters': [{'dataId': ['data'], 'id': 'r4gzjf87n', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.data.data_analysis import show_kepler_map\n",
    "\n",
    "# Font size 27\n",
    "show_kepler_map(\n",
    "    df, config_file_path='../config/kepler/metr-la/visualization_test_x.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0740fee0e79c446f84e6df73a51b291c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(config={'version': 'v1', 'config': {'visState': {'filters': [{'dataId': ['data'], 'id': 'r4gzjf87n', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.data.data_analysis import show_kepler_map\n",
    "\n",
    "show_kepler_map(\n",
    "    df, config_file_path='../config/kepler/metr-la/visualization_test_x_clusters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Save m config\n",
    "import json\n",
    "\n",
    "with open('../config/kepler/visualization_test_x_clusters.json', 'w') as f:\n",
    "    json.dump(m.config, f)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.analyisis import (\n",
    "    get_node_values_with_clusters_and_location_dataframe)\n",
    "\n",
    "df = get_node_values_with_clusters_and_location_dataframe(sample_y, clusters_y, node_pos_dict, locations_df, sample_y_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', 'target'], dtype=object)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['icon'] = df['cluster'].apply(lambda x: icons[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f808c885ee4348be914df1ac273e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(config={'version': 'v1', 'config': {'visState': {'filters': [{'dataId': ['data'], 'id': 'qxv5h0is9', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.data.data_analysis import show_kepler_map\n",
    "\n",
    "show_kepler_map(\n",
    "    df, config_file_path='../config/kepler/metr-la/visualization_test_y.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Save m config\n",
    "import json\n",
    "\n",
    "with open('../config/kepler/visualization_test_y.json', 'w') as f:\n",
    "    json.dump(m.config, f)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
