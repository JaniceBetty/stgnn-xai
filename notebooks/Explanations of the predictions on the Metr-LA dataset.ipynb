{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the event set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialTemporalGNN(\n",
       "  (encoder): Linear(in_features=9, out_features=64, bias=False)\n",
       "  (s_gnns): ModuleList(\n",
       "    (0-11): 12 x S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (hidden_s_gnns): ModuleList(\n",
       "    (0-10): 11 x S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (grus): ModuleList(\n",
       "    (0-11): 12 x GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (positional_encoder): PositionalEncoder()\n",
       "  (transformer): Transformer(\n",
       "    (queries_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (keys_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (values_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (normalization): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (normalization_out): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (prediction_head): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix and the matrix itself.\n",
    "header, _, adj_matrix = adj_matrix_structure\n",
    "\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "spatial_temporal_gnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from src.spatial_temporal_gnn.prediction import predict\n",
    "\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_train.npy'))\n",
    "y_train = predict(spatial_temporal_gnn, x_train, scaler, DEVICE)\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_val.npy'))\n",
    "y_val = predict(spatial_temporal_gnn, x_val, scaler, DEVICE)\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_test.npy'))\n",
    "y_test = predict(spatial_temporal_gnn, x_test, scaler, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from src.data.data_analysis import days_encoder\n",
    "\n",
    "def get_largest_event_set(data: np.ndarray\n",
    "                          ) -> List[Tuple[str, Optional[int], Optional[int]]]:\n",
    "    n_time_steps, n_nodes, _ = data.shape[-3:]\n",
    "    \n",
    "    # Get the largest event set related to the speed.\n",
    "    speed_events = [\n",
    "        (0, time_step, node) \n",
    "        for time_step in range(n_time_steps) \n",
    "        for node in range(n_nodes) \n",
    "        if data[..., time_step, node, 0] > 0]\n",
    "    # Get the largest event set related to the time of day.\n",
    "    time_of_day_events = [\n",
    "        (1, time_step, None) for time_step in range(n_time_steps)]\n",
    "    # Get the largest event set related to the day of week.\n",
    "    day_of_week_events = [(2, None, None)]\n",
    "    # Get the largest event set related to the kind of day.\n",
    "    #kind_of_day_events = [(3, None, None)]\n",
    "    \n",
    "    return speed_events + time_of_day_events + day_of_week_events #+\\\n",
    "    #    kind_of_day_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_largest_event_set(x_test[11])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map the event set to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "def remove_features_by_events(\n",
    "    data: Union[np.ndarray, torch.FloatTensor],\n",
    "    events: List[Tuple[int, Optional[int], Optional[int]]]\n",
    "    ) -> Union[np.ndarray, torch.FloatTensor]:\n",
    "    if isinstance(data, torch.FloatTensor):\n",
    "        filtered_data = data.clone()\n",
    "    else:\n",
    "        filtered_data = data.copy()\n",
    "    n_time_steps, n_nodes, n_features = filtered_data.shape[-3:]\n",
    "    \n",
    "    speed_events = [tuple(event) for event in events if event[0] == 0]\n",
    "    time_of_day_events = [tuple(event) for event in events if event[0] == 1]\n",
    "    day_of_week_events = [tuple(event) for event in events if event[0] == 2]\n",
    "    #kind_of_day_events = [event for event in events if event[0] == 3]\n",
    "    \n",
    "    if n_features > 1 and not len(day_of_week_events):\n",
    "        filtered_data[..., -7:] = 0\n",
    "    \n",
    "    for time_step in range(n_time_steps):\n",
    "        for node in range(n_nodes):\n",
    "            if (0, time_step, node) not in speed_events:\n",
    "                filtered_data[..., time_step, node, 0] = 0\n",
    "\n",
    "            '''if n_features > 1 and len(kind_of_day_events):\n",
    "                if 1 in data[..., -7:-2]:\n",
    "                    filtered_data[..., time_step, node, -7:-2] = 1\n",
    "                else:\n",
    "                    filtered_data[..., time_step, node, -2:] = 1'''\n",
    "\n",
    "        if n_features > 1 and (1, time_step, None) not in time_of_day_events:\n",
    "            filtered_data[..., time_step, :, 1] = -1\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_event_from_data(\n",
    "    data: Union[np.ndarray, torch.FloatTensor],\n",
    "    event: Union[np.ndarray, torch.FloatTensor]\n",
    "    ) -> Union[np.ndarray, torch.FloatTensor]:\n",
    "    # n_time_steps, n_nodes, _ = data.shape[-3:]\n",
    "    event_kind = event[0]\n",
    "    time_step = int(event[1].item())\n",
    "    node = int(event[2].item())\n",
    "    if event_kind == 0:\n",
    "        data[..., time_step, node, 0] = 0\n",
    "    elif event_kind == 1:\n",
    "        data[..., time_step, :, 1] = -1\n",
    "    elif event_kind == 2:\n",
    "        data[..., -7:] = 0\n",
    "    #elif event[0] == 3:\n",
    "    #    pass\n",
    "\n",
    "    '''if event_kind in [0, 1]: \n",
    "        for time_step in range(n_time_steps):\n",
    "            for node in range(n_nodes):\n",
    "                if 1 in data[..., -7:-2]:\n",
    "                    data[..., time_step, node, -7:-2] = 1\n",
    "                else:\n",
    "                    data[..., time_step, node, -2:] = 1'''\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = remove_features_by_events(x_test[0], [(1, 0, None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (event_type, speed, time_of_day, ...day, speed_target, time_of_day_target, ...day_target)\n",
    "\n",
    "# ->\n",
    "\n",
    "# -> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 207, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "def simulate_model(\n",
    "    instance: torch.FloatTensor, candidate_event: torch.FloatTensor,\n",
    "    candidate_event_score: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    # Get 1 or 0 with probability equal to the score.\n",
    "    result = torch.bernoulli(candidate_event_score)\n",
    "    for i in range(len(result)):\n",
    "        if result[i]:\n",
    "            instance[i] = remove_single_event_from_data(\n",
    "                instance[i], candidate_event[i])\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Navigator(nn.Module):\n",
    "    def __init__(self, device: str, hidden_features: int = 64) -> None:\n",
    "        super().__init__()\n",
    "        # Set the linear encoder.\n",
    "        self.linear_encoder = nn.LazyLinear(hidden_features)\n",
    "        # Set the linear decoder.\n",
    "        self.linear_decoder = nn.Linear(hidden_features, 1)\n",
    "        # Set the device that is used for training and querying the model.\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, candidate_event: torch.FloatTensor, target_events: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # Flatten the target events.\n",
    "        target_events = target_events.flatten(start_dim=1)\n",
    "        # Concatenate the candidate event and the target events.\n",
    "        x = torch.cat((candidate_event, target_events), dim=1)\n",
    "        # Encode the input.\n",
    "        out = self.linear_encoder(x)\n",
    "        # Decode the output to get the logits prediction.\n",
    "        out = self.linear_decoder(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Navigator(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Optional, Tuple\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EventsDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"Initialize the dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            The input values of the dataset.\n",
    "        y : ndarray\n",
    "            The ground truth of the dataset's input data.\n",
    "        \"\"\"\n",
    "        res_x = []\n",
    "        res_events = []\n",
    "        res_y = []\n",
    "        for input, target in zip(x, y):\n",
    "            input_events = get_largest_event_set(input)\n",
    "            for event in input_events:\n",
    "                res_x.append(input)\n",
    "                res_events.append(event)\n",
    "                res_y.append(target)\n",
    "        \n",
    "        self.x = res_x\n",
    "        self.events = res_events\n",
    "        self.y = res_y\n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get a dataset \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            The index from where to extract the dataset instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The input data at the given index.\n",
    "        ndarray\n",
    "            The ground truth with respect to the input data at the\n",
    "            given index.\n",
    "        \"\"\"\n",
    "        x = self.x[index]\n",
    "        event = self.events[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        target_events = np.array(get_largest_event_set(y))\n",
    "        \n",
    "        probs = np.array(np.arange(1, len(target_events) + 1) / np.sum(np.arange(1, len(target_events) + 1)))\n",
    "        probs = probs[::-1]\n",
    "\n",
    "        [target_size] = np.random.choice(range(1, len(target_events) + 1), 1, p=probs)\n",
    "        \n",
    "        selected_events_idx = np.random.choice(np.arange(len(target_events)), target_size, replace=False)\n",
    "        selected_events = target_events[selected_events_idx]\n",
    "        y = remove_features_by_events(y, selected_events.tolist())\n",
    "        #print(y)\n",
    "\n",
    "        return x, y, torch.Tensor(event)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the length of the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "\n",
    "def get_dataloader(x: np.ndarray, y: np.ndarray, \n",
    "                   batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    dataset = EventsDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(x_train, y_train, 64, True)\n",
    "val_loader = get_dataloader(x_val, y_val, 64, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x  = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spatial_temporal_gnn.training import Checkpoint\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=2e-6)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=.1, patience=10, verbose=False,\n",
    "    threshold=.001, threshold_mode='rel', cooldown=0, min_lr=1e-5, eps=1e-08)\n",
    "\n",
    "checkpoint_file_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                    'navigator_metr_la.pth')\n",
    "checkpoint = Checkpoint(checkpoint_file_path)\n",
    "\n",
    "EPOCHS = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.spatial_temporal_gnn.metrics import MAE, RMSE, MAPE\n",
    "from src.spatial_temporal_gnn.training import Checkpoint\n",
    "from src.data.data_processing import Scaler\n",
    "\n",
    "def train(\n",
    "    model: Navigator, optimizer: torch.optim.Optimizer,\n",
    "    train_dataloader: DataLoader, val_dataloader: DataLoader,\n",
    "    spatial_temporal_gnn: SpatialTemporalGNN, scaler: Scaler,\n",
    "    epochs: int, checkpoint: Optional[Checkpoint] = None,\n",
    "    lr_scheduler: Optional[object] = None,\n",
    "    reload_best_weights: bool = True) -> Dict[str, np.ndarray]:\n",
    "    # Get the device that is used for training and querying the model.\n",
    "    device = model.device\n",
    "\n",
    "    # Initialize the training criterions.\n",
    "    mae_criterion = MAE()\n",
    "    rmse_criterion = RMSE()\n",
    "    mape_criterion = MAPE()\n",
    "\n",
    "    # Initialize the histories.\n",
    "    metrics = ['train_mae', 'train_rmse', 'train_mape', 'val_mae', 'val_rmse',\n",
    "               'val_mape']\n",
    "    history = { m: [] for m in metrics }\n",
    "\n",
    "    # Set model in training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate across the epochs.\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        # Remove unused tensors from gpu memory.\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Initialize the running errors.\n",
    "        running_train_mae = 0.\n",
    "        running_train_rmse = 0.\n",
    "        running_train_mape = 0.\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        for batch_idx, (x, y, event) in enumerate(train_dataloader):\n",
    "            # Increment the number of batch steps.\n",
    "            batch_steps = batch_idx + 1\n",
    "\n",
    "            # Get the data.\n",
    "            x = x.type(torch.float32).to(device=device)\n",
    "            event = event.type(torch.float32).to(device=device)\n",
    "            y = y.type(torch.float32).to(device=device)\n",
    "            \n",
    "            event_score = model(event, y)\n",
    "            \n",
    "            x = simulate_model(x, event, event_score.sigmoid())\n",
    "\n",
    "            x = scaler.scale(x)\n",
    "\n",
    "            # Compute the Spatial-Temporal GNN model predictions.\n",
    "            y_pred = spatial_temporal_gnn(x)\n",
    "\n",
    "            # Compute the loss on the scaled results and ground truth.\n",
    "            y_pred = scaler.un_scale(y_pred)\n",
    "\n",
    "            loss = mae_criterion(y_pred, y)\n",
    "            \n",
    "            #print(y)\n",
    "\n",
    "            # Compute errors and update running errors.\n",
    "            with torch.no_grad():\n",
    "                rmse = rmse_criterion(y_pred, y)\n",
    "                mape = mape_criterion(y_pred, y)\n",
    "\n",
    "            running_train_mae += loss.item()\n",
    "            running_train_rmse += rmse.item()\n",
    "            running_train_mape += mape.item()\n",
    "\n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use MAE as the loss function for backpropagation.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get the batch time.\n",
    "            epoch_time = time() - start_time\n",
    "            batch_time = epoch_time / batch_steps\n",
    "\n",
    "            # Print the batch results.\n",
    "            print(\n",
    "                f'[{batch_steps}/{len(train_dataloader)}] -',\n",
    "                f'{epoch_time:.0f}s {batch_time * 1e3:.0f}ms/step -',\n",
    "\n",
    "                f'train {{ MAE (loss): {running_train_mae / batch_steps:.3g} -',\n",
    "                f'RMSE: {running_train_rmse / batch_steps:.3g} -',\n",
    "                f'MAPE: {running_train_mape * 100. / batch_steps:.3g}% }} -',\n",
    "\n",
    "                f'lr: {optimizer.param_groups[0][\"lr\"]:.3g} -',\n",
    "                f'weight decay: {optimizer.param_groups[0][\"weight_decay\"]}',\n",
    "                '             ' if batch_steps < len(train_dataloader) else '',\n",
    "                end='\\r')\n",
    "\n",
    "        # Set the model in evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        # Get the average training errors and update the history.\n",
    "        train_mae = running_train_mae / len(train_dataloader)\n",
    "        train_rmse = running_train_rmse / len(train_dataloader)\n",
    "        train_mape = running_train_mape / len(train_dataloader)\n",
    "\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['train_mape'].append(train_mape)\n",
    "\n",
    "        # Get the validation results and update the history.\n",
    "        val_results = validate(model, val_dataloader, spatial_temporal_gnn,\n",
    "                               scaler)\n",
    "        val_mae, val_rmse, val_mape = val_results\n",
    "\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mape'].append(val_mape)\n",
    "\n",
    "        # Save the checkpoints if demanded.\n",
    "        if checkpoint is not None:\n",
    "            err_sum = val_mae + val_rmse + val_mape\n",
    "            checkpoint.save_best(model, optimizer, err_sum)\n",
    "\n",
    "        # Print the epoch results.\n",
    "        print(\n",
    "            f'[{len(train_dataloader)}/{len(train_dataloader)}] -',\n",
    "            f'{epoch_time:.0f}s -',\n",
    "\n",
    "            f'train: {{ MAE (loss): {train_mae:.3g} -',\n",
    "            f'RMSE: {train_rmse:.3g} -',\n",
    "            f'MAPE: {train_mape * 100.:.3g}% }} -',\n",
    "\n",
    "            f'val: {{ MAE: {val_mae:.3g} -',\n",
    "            f'RMSE: {val_rmse:.3g} -',\n",
    "            f'MAPE: {val_mape * 100.:.3g}% }} -',\n",
    "\n",
    "            f'lr: {optimizer.param_groups[0][\"lr\"]:.3g} -',\n",
    "            f'weight decay: {optimizer.param_groups[0][\"weight_decay\"]}')\n",
    "\n",
    "        # Update the learning rate scheduler.\n",
    "        lr_scheduler.step(train_mae)\n",
    "\n",
    "        # Set model in training mode.\n",
    "        model.train()\n",
    "\n",
    "    # Load the best weights of the model if demanded.\n",
    "    if checkpoint is not None and reload_best_weights:\n",
    "        checkpoint.load_best_weights(model)\n",
    "\n",
    "    # Set the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Remove unused tensors from gpu memory.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Turn the history to numpy arrays.\n",
    "    for k, v in history.items():\n",
    "        history[k] = np.array(v)\n",
    "\n",
    "    return history\n",
    "\n",
    "def validate(\n",
    "    model: Navigator, val_dataloader: DataLoader,\n",
    "    spatial_temporal_gnn: SpatialTemporalGNN, scaler: Scaler\n",
    "    ) -> Tuple[float, float, float]:\n",
    "    device = model.device\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize the validation criterions.\n",
    "    mae_criterion = MAE()\n",
    "    rmse_criterion = RMSE()\n",
    "    mape_criterion = MAPE()\n",
    "\n",
    "    # Inizialize running errors.\n",
    "    running_val_mae = 0.\n",
    "    running_val_rmse = 0.\n",
    "    running_val_mape = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (x, y, event) in enumerate(val_dataloader):\n",
    "            # Get the data.\n",
    "            x = x.type(torch.float32).to(device=device)\n",
    "            event = event.type(torch.float32).to(device=device)\n",
    "            y = y.type(torch.float32).to(device=device)\n",
    "\n",
    "            event_score = model(event, y)\n",
    "            \n",
    "            x = simulate_model(x, event, event_score.sigmoid())\n",
    "\n",
    "            x = scaler.scale(x)\n",
    "\n",
    "            # Compute the Spatial-Temporal GNN model predictions.\n",
    "            y_pred = spatial_temporal_gnn(x)\n",
    "\n",
    "            # Un-scale the predictions.\n",
    "            y_pred = scaler.un_scale(y_pred)\n",
    "\n",
    "            # Get the prediction errors and update the running errors.\n",
    "            mae = mae_criterion(y_pred, y)\n",
    "            rmse = rmse_criterion(y_pred, y)\n",
    "            mape = mape_criterion(y_pred, y)\n",
    "\n",
    "            running_val_mae += mae.item()\n",
    "            running_val_rmse += rmse.item()\n",
    "            running_val_mape += mape.item()\n",
    "\n",
    "    # Remove unused tensors from gpu memory.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Get the average MAE, RMSE and MAPE scores.\n",
    "    val_mae = running_val_mae / len(val_dataloader)\n",
    "    val_rmse = running_val_rmse / len(val_dataloader)\n",
    "    val_mape = running_val_mape / len(val_dataloader)\n",
    "\n",
    "    return val_mae, val_rmse, val_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00104 - RMSE: 0.0231 - MAPE: 0.00177% } - val: { MAE: 0.00411 - RMSE: 0.0449 - MAPE: 0.0129% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 2/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.0013 - RMSE: 0.0273 - MAPE: 0.00223% } - val: { MAE: 0.0029 - RMSE: 0.0308 - MAPE: 0.00866% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 3/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00112 - RMSE: 0.0255 - MAPE: 0.00192% } - val: { MAE: 0.00418 - RMSE: 0.0445 - MAPE: 0.0128% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 4/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.000995 - RMSE: 0.021 - MAPE: 0.00169% } - val: { MAE: 0.00275 - RMSE: 0.0293 - MAPE: 0.00819% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 5/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00115 - RMSE: 0.0227 - MAPE: 0.00197% } - val: { MAE: 0.00351 - RMSE: 0.0369 - MAPE: 0.011% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 6/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00106 - RMSE: 0.0231 - MAPE: 0.0018% } - val: { MAE: 0.00347 - RMSE: 0.0359 - MAPE: 0.0103% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 7/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.000996 - RMSE: 0.0197 - MAPE: 0.00172% } - val: { MAE: 0.00453 - RMSE: 0.0485 - MAPE: 0.0138% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 8/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.000933 - RMSE: 0.0172 - MAPE: 0.00157% } - val: { MAE: 0.00237 - RMSE: 0.0255 - MAPE: 0.00695% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 9/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.0011 - RMSE: 0.0216 - MAPE: 0.00187% } - val: { MAE: 0.00224 - RMSE: 0.0385 - MAPE: 0.00654% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 10/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00123 - RMSE: 0.0219 - MAPE: 0.00206% } - val: { MAE: 0.00342 - RMSE: 0.0417 - MAPE: 0.0106% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 11/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00124 - RMSE: 0.0256 - MAPE: 0.00213% } - val: { MAE: 0.00387 - RMSE: 0.044 - MAPE: 0.0121% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 12/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.000996 - RMSE: 0.0196 - MAPE: 0.00171% } - val: { MAE: 0.00411 - RMSE: 0.0472 - MAPE: 0.0133% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 13/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.000996 - RMSE: 0.0195 - MAPE: 0.00169% } - val: { MAE: 0.00257 - RMSE: 0.0266 - MAPE: 0.00783% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 14/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00121 - RMSE: 0.0242 - MAPE: 0.00205% } - val: { MAE: 0.00388 - RMSE: 0.0343 - MAPE: 0.0122% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 15/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00114 - RMSE: 0.022 - MAPE: 0.00193% } - val: { MAE: 0.00402 - RMSE: 0.0428 - MAPE: 0.0122% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 16/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00126 - RMSE: 0.0246 - MAPE: 0.00214% } - val: { MAE: 0.00262 - RMSE: 0.0302 - MAPE: 0.00755% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 17/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00111 - RMSE: 0.0222 - MAPE: 0.00192% } - val: { MAE: 0.00279 - RMSE: 0.0339 - MAPE: 0.00827% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 18/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00134 - RMSE: 0.0263 - MAPE: 0.0023% } - val: { MAE: 0.00486 - RMSE: 0.0627 - MAPE: 0.0152% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 19/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.0011 - RMSE: 0.0239 - MAPE: 0.00188% } - val: { MAE: 0.00213 - RMSE: 0.0181 - MAPE: 0.00623% } - lr: 0.001 - weight decay: 2e-06\n",
      "Epoch 20/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00105 - RMSE: 0.0199 - MAPE: 0.00179% } - val: { MAE: 0.00315 - RMSE: 0.0339 - MAPE: 0.00951% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 21/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00137 - RMSE: 0.0281 - MAPE: 0.00233% } - val: { MAE: 0.00323 - RMSE: 0.0328 - MAPE: 0.00987% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 22/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00108 - RMSE: 0.0214 - MAPE: 0.00184% } - val: { MAE: 0.00384 - RMSE: 0.0418 - MAPE: 0.0119% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 23/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00136 - RMSE: 0.0289 - MAPE: 0.00231% } - val: { MAE: 0.00361 - RMSE: 0.0388 - MAPE: 0.011% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 24/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00109 - RMSE: 0.0235 - MAPE: 0.00184% } - val: { MAE: 0.00307 - RMSE: 0.0452 - MAPE: 0.009% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 25/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00118 - RMSE: 0.0259 - MAPE: 0.00201% } - val: { MAE: 0.00285 - RMSE: 0.0272 - MAPE: 0.00874% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 26/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00116 - RMSE: 0.023 - MAPE: 0.00196% } - val: { MAE: 0.00454 - RMSE: 0.0454 - MAPE: 0.014% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 27/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00125 - RMSE: 0.0268 - MAPE: 0.00216% } - val: { MAE: 0.00236 - RMSE: 0.0251 - MAPE: 0.00754% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 28/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.000916 - RMSE: 0.0189 - MAPE: 0.00155% } - val: { MAE: 0.00297 - RMSE: 0.033 - MAPE: 0.00966% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 29/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00112 - RMSE: 0.0228 - MAPE: 0.0019% } - val: { MAE: 0.00279 - RMSE: 0.0273 - MAPE: 0.00854% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 30/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.000913 - RMSE: 0.0194 - MAPE: 0.00154% } - val: { MAE: 0.00298 - RMSE: 0.029 - MAPE: 0.00905% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 31/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00126 - RMSE: 0.0258 - MAPE: 0.00218% } - val: { MAE: 0.00298 - RMSE: 0.027 - MAPE: 0.00893% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 32/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00109 - RMSE: 0.0206 - MAPE: 0.00187% } - val: { MAE: 0.00441 - RMSE: 0.0394 - MAPE: 0.0136% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 33/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00104 - RMSE: 0.0216 - MAPE: 0.00178% } - val: { MAE: 0.00297 - RMSE: 0.0284 - MAPE: 0.00936% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 34/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00116 - RMSE: 0.0212 - MAPE: 0.00197% } - val: { MAE: 0.00296 - RMSE: 0.0301 - MAPE: 0.00919% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 35/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00125 - RMSE: 0.0279 - MAPE: 0.0022% } - val: { MAE: 0.00331 - RMSE: 0.0473 - MAPE: 0.0103% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 36/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00128 - RMSE: 0.0267 - MAPE: 0.00218% } - val: { MAE: 0.00433 - RMSE: 0.0541 - MAPE: 0.013% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 37/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00116 - RMSE: 0.0222 - MAPE: 0.00199% } - val: { MAE: 0.00244 - RMSE: 0.0253 - MAPE: 0.00748% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 38/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00151 - RMSE: 0.0277 - MAPE: 0.00257% } - val: { MAE: 0.00298 - RMSE: 0.0433 - MAPE: 0.00917% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 39/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00118 - RMSE: 0.0237 - MAPE: 0.00201% } - val: { MAE: 0.00261 - RMSE: 0.0432 - MAPE: 0.00798% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 40/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00113 - RMSE: 0.0197 - MAPE: 0.00191% } - val: { MAE: 0.00379 - RMSE: 0.0347 - MAPE: 0.0119% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 41/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00104 - RMSE: 0.0211 - MAPE: 0.00175% } - val: { MAE: 0.00421 - RMSE: 0.0403 - MAPE: 0.0127% } - lr: 0.0001 - weight decay: 2e-06\n",
      "Epoch 42/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00104 - RMSE: 0.0215 - MAPE: 0.00181% } - val: { MAE: 0.00358 - RMSE: 0.0371 - MAPE: 0.0111% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 43/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.000981 - RMSE: 0.0192 - MAPE: 0.00166% } - val: { MAE: 0.00323 - RMSE: 0.0384 - MAPE: 0.00981% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 44/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00116 - RMSE: 0.0222 - MAPE: 0.00199% } - val: { MAE: 0.00189 - RMSE: 0.0294 - MAPE: 0.00569% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 45/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00123 - RMSE: 0.0259 - MAPE: 0.00209% } - val: { MAE: 0.00366 - RMSE: 0.0322 - MAPE: 0.0113% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 46/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00124 - RMSE: 0.029 - MAPE: 0.00212% } - val: { MAE: 0.00223 - RMSE: 0.0301 - MAPE: 0.00707% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 47/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.0011 - RMSE: 0.0277 - MAPE: 0.00189% } - val: { MAE: 0.00373 - RMSE: 0.0411 - MAPE: 0.0119% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 48/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00123 - RMSE: 0.0212 - MAPE: 0.00205% } - val: { MAE: 0.00403 - RMSE: 0.0395 - MAPE: 0.0133% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 49/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00128 - RMSE: 0.0299 - MAPE: 0.0022% } - val: { MAE: 0.00398 - RMSE: 0.0483 - MAPE: 0.0127% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 50/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00137 - RMSE: 0.0262 - MAPE: 0.00234% } - val: { MAE: 0.00389 - RMSE: 0.059 - MAPE: 0.0119% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 51/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00109 - RMSE: 0.0207 - MAPE: 0.00184% } - val: { MAE: 0.00258 - RMSE: 0.0263 - MAPE: 0.00809% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 52/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00101 - RMSE: 0.0195 - MAPE: 0.00172% } - val: { MAE: 0.00199 - RMSE: 0.0192 - MAPE: 0.00607% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 53/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00112 - RMSE: 0.021 - MAPE: 0.00189% } - val: { MAE: 0.00375 - RMSE: 0.0452 - MAPE: 0.0111% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 54/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.0014 - RMSE: 0.0234 - MAPE: 0.00244% } - val: { MAE: 0.00406 - RMSE: 0.0433 - MAPE: 0.012% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 55/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00123 - RMSE: 0.0231 - MAPE: 0.00209% } - val: { MAE: 0.0023 - RMSE: 0.0213 - MAPE: 0.00667% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 56/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00101 - RMSE: 0.0205 - MAPE: 0.00173% } - val: { MAE: 0.00243 - RMSE: 0.0212 - MAPE: 0.00728% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 57/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00119 - RMSE: 0.0242 - MAPE: 0.00203% } - val: { MAE: 0.00321 - RMSE: 0.032 - MAPE: 0.0101% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 58/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00112 - RMSE: 0.0203 - MAPE: 0.00189% } - val: { MAE: 0.00278 - RMSE: 0.0301 - MAPE: 0.00863% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 59/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00116 - RMSE: 0.0214 - MAPE: 0.00197% } - val: { MAE: 0.00355 - RMSE: 0.0432 - MAPE: 0.0117% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 60/1000\n",
      "[17/17] - 37s - train: { MAE (loss): 0.00124 - RMSE: 0.0238 - MAPE: 0.00208% } - val: { MAE: 0.00447 - RMSE: 0.0441 - MAPE: 0.014% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 61/1000\n",
      "[17/17] - 36s - train: { MAE (loss): 0.00157 - RMSE: 0.0235 - MAPE: 0.00262% } - val: { MAE: 0.00216 - RMSE: 0.0206 - MAPE: 0.00644% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 62/1000\n",
      "[17/17] - 35s - train: { MAE (loss): 0.00116 - RMSE: 0.0221 - MAPE: 0.00197% } - val: { MAE: 0.00207 - RMSE: 0.022 - MAPE: 0.00595% } - lr: 1e-05 - weight decay: 2e-06\n",
      "Epoch 63/1000\n",
      "[4/17] - 9s 2361ms/step - train { MAE (loss): 0.000991 - RMSE: 0.0189 - MAPE: 0.00166% } - lr: 1e-05 - weight decay: 2e-06              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 30\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model, optimizer, train_loader, val_loader, spatial_temporal_gnn, scaler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     EPOCHS, checkpoint, lr_scheduler, reload_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 30\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m running_train_mape \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m start_time \u001b[39m=\u001b[39m time()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (x, y, event) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# Increment the number of batch steps.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     batch_steps \u001b[39m=\u001b[39m batch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m# Get the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 30\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m selected_events_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(target_events)), target_size, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m selected_events \u001b[39m=\u001b[39m target_events[selected_events_idx]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m y \u001b[39m=\u001b[39m remove_features_by_events(y, selected_events\u001b[39m.\u001b[39;49mtolist())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m#print(y)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x, y, torch\u001b[39m.\u001b[39mTensor(event)\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\Explanations of the predictions on the Metr-LA dataset.ipynb Cell 30\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39m0\u001b[39m, time_step, node) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m speed_events:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         filtered_data[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, time_step, node, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m'''if n_features > 1 and len(kind_of_day_events):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m        if 1 in data[..., -7:-2]:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m            filtered_data[..., time_step, node, -7:-2] = 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m        else:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m            filtered_data[..., time_step, node, -2:] = 1'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m (\u001b[39m1\u001b[39m, time_step, \u001b[39mNone\u001b[39;00m) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m time_of_day_events:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/Explanations%20of%20the%20predictions%20on%20the%20Metr-LA%20dataset.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     filtered_data[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, time_step, :, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train(\n",
    "    model, optimizer, train_loader, val_loader, spatial_temporal_gnn, scaler,\n",
    "    EPOCHS, checkpoint, lr_scheduler, reload_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
